<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Edgar M." />

<meta name="date" content="2023-10-11" />

<title>MNIST Classification-Neural Network from Scratch</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">MNIST Classification-Neural Network from
Scratch</h1>
<h4 class="author">Edgar M.</h4>
<h4 class="date">10/11/23</h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The purpose of this notebook is to build a feedforward
classification<br> neural networks from scratch. What does from scratch
mean? It means<br> that I will not be using any machine learning
libraries (e.g. keras,<br> tensorflow, pytorch, etc.). I will only be
using the R libraries that<br> are pasted below. Neural network is made
up of input layer, either one or<br> two hidden layers, and output
layer. Feedforward neural network is for<br> MNIST data classification.
The first type of neural network is one with<br> no mappings. Continuing
the derivation. The second type of neural network<br> is with Fourier
feature mapping. Idea for Fourier feature mapping came<br> from the
paper, “Fourier Features Let Networks Learn High Frequency<br> Functions
in Low Dimensional Domains.” As I progress through this<br> notebook. I
will expand on the idea of neural network with no mapping and<br> neural
networks with Fourier feature mapping.<br></p>
<p>My original plan was to include all the derived math within this
notebook.<br> But my full derivation was more than 20 pages long, hence
I will only<br> include the most important parts from my derivation. My
Derivation is<br> longer than other standard derivation lenghts that
I’ve seen, because it<br> is my first classification neural network from
scratch and I did not skip<br> over most of the trivial easy to see
stuff. Mostly for my own sanity. As<br> later in the future. I want to
come back and find it very easy to see<br> what I did, without breaking
my brain. Basic math topics reader should<br> be familiar with are
matrix algebra and multivariable calculus. With all<br> that being said.
Let’s begin :)<br></p>
<p>Load necessary libraries and data.<br></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co">#load nessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co">#suppressMessages() is used to suppress messages from being printed in console</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(magrittr)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">library</span>(dplyr) <span class="sc">%&gt;%</span> <span class="fu">suppressMessages</span>()</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="fu">library</span>(readr) </span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="fu">library</span>(ggplot2)  </span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co">#this code is for loading MNIST data within kaggle</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co">#Data &lt;-read_csv(&quot;/kaggle/input/digit-recognizer/train.csv&quot;) %&gt;% suppressMessages()</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co">#load data from local machine </span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>Data <span class="ot">&lt;-</span><span class="fu">read_csv</span>(<span class="st">&quot;C:/Users/Eva-02/Downloads/Code/Personal Projects/MNIST Classification-Neural Network From Scratch/Data/train.csv&quot;</span>) </span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="co">#Convert Data into a matrix</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>Data <span class="ot">&lt;-</span><span class="fu">as.matrix</span>(Data) </span></code></pre></div>
</div>
<div id="parameter-setup-and-data-preparation-for-image-classification" class="section level2">
<h2>Parameter Setup and Data Preparation for Image Classification</h2>
<div id="image-matrix" class="section level3">
<h3>Image Matrix</h3>
<p>Counting the zeroth pixel as one and the 783th pixel as 784, then
there’s<br> 784 pixels in total. Each pixel is a number between 0 and
255, which<br> represents the intensity of the pixel. “0 means
background (white), 255”<br> “means foreground (black)” (Yann LeCun,
Corinna Cortes, Christopher J.C.<br> Burges, 1998).<br></p>
<p>According to THE MNIST DATABASE of handwritten digits. Each 0 to 9
digit<br> is from a 28 by 28 image matrix.<br></p>
<p><span class="math display">\[ \text{Image Matrix}\ = \begin{bmatrix}
pixel0 &amp; pixel1 &amp; pixel2 &amp;...pixel27 \\
pixel28 &amp; pixel29 &amp; pixel30 &amp;...pixel55\\
pixel56 &amp; pixel57 &amp; pixel58 &amp;...pixel83\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
pixel756 &amp; pixel757 &amp; pixel758 &amp;...pixel783\\
\end{bmatrix} \]</span></p>
<p>You can mathematically convert an image matrix into a row vector
by:<br></p>
<p><span class="math display">\[ Row Image Vector = [Vec((\text{Image
Matrix})^T)]^T \]</span></p>
<p><span class="math display">\[ Row Image Vector = \begin{bmatrix}
pixel0 &amp; pixel1 &amp; pixel2 &amp; pixel3 &amp; ...pixel783 \\
\end{bmatrix} \]</span></p>
<p>The first 5 columns and first 5 rows of the training data are shown
below.</p>
<pre><code>##      label pixel0 pixel1 pixel2 pixel3
## [1,]     1      0      0      0      0
## [2,]     0      0      0      0      0
## [3,]     1      0      0      0      0
## [4,]     4      0      0      0      0
## [5,]     0      0      0      0      0</code></pre>
<p>The last 5 columns and first 5 rows of the training data are shown
below.</p>
<pre><code>##      pixel779 pixel780 pixel781 pixel782 pixel783
## [1,]        0        0        0        0        0
## [2,]        0        0        0        0        0
## [3,]        0        0        0        0        0
## [4,]        0        0        0        0        0
## [5,]        0        0        0        0        0</code></pre>
<p>Each row of the MNIST dataset is as follows:<br> <span class="math display">\[\begin{bmatrix}
label &amp; pixel0 &amp; pixel1 &amp; pixel2 &amp; pixel3 &amp;
...pixel783 \\
\end{bmatrix}\]</span></p>
<p>Label column is the <strong>y</strong> dependent variable, and pixels
are the <strong>x</strong><br> independent variables. For each row we
have:<br></p>
<p><span class="math display">\[\begin{bmatrix}
[y] &amp; [Row Image Vector] \\
\end{bmatrix}\]</span><br></p>
</div>
<div id="parameter-configuration-and-data-splitting-for-training-and-testing" class="section level3">
<h3>Parameter Configuration and Data Splitting for Training and
Testing</h3>
<p>Let Q = number of classification categories, which in this case Q =
10.<br></p>
<p>Let N = Total number of observations, which in this case N =
42000.<br> <span class="math display">\[N = N_{training} + N_{testing}\
= training + testing\]</span> Good rule of thumb for the total number of
observations for training-testing<br> can be set as (%80-%20), (%90-%10)
or some other percentage.<br></p>
<p><span class="math display">\[N_{training} =N*percent\]</span> Total
number of observations for testing.<br> <span class="math display">\[N_{testing} =N-N_{training}\]</span></p>
<p>Let: <br> <span class="math display">\[n \in N_{training} \]</span>
Stochastic gradient descent (1 observation)<br> <span class="math display">\[n = 1 \]</span> Mini-batch gradient descent (more
than one, but less than all training observations).<br> <span class="math display">\[1 &lt; n &lt; N_{training} \]</span> Batch
gradient descent (all training observations).<br> <span class="math display">\[n = N_{training} \]</span></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co">#Number of classification categories</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>Q <span class="ot">&lt;-</span><span class="dv">10</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">#Total number of observations</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>N <span class="ot">&lt;-</span><span class="fu">nrow</span>(Data)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co">#Percent of training observations</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>Percent <span class="ot">&lt;-</span><span class="fl">0.9</span>  </span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co">#Training observations</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>N_training <span class="ot">&lt;-</span>N<span class="sc">*</span>Percent</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="co">#Testing observations</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>N_testing <span class="ot">&lt;-</span>N<span class="sc">-</span>N_training</span></code></pre></div>
<p>Separate training and testing data.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Randomly select N_training observations from the total N observations.</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="co"># Randomly select N_training rows</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># replace = TRUE means that the same row can be selected more than once.</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># replace = FALSE means that the same row cannot be selected more than once.</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>Randomly_selected_rows <span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">nrow</span>(Data), N_training, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co"># Create a new matrix with the selected rows</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>Data_training <span class="ot">&lt;-</span>Data[Randomly_selected_rows, ]</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="co"># Get the remaining rows</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>remaining_rows <span class="ot">&lt;-</span><span class="fu">setdiff</span>(<span class="fu">seq_len</span>(<span class="fu">nrow</span>(Data)), Randomly_selected_rows)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="co"># Create a new matrix with the remaining rows</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>Data_testing <span class="ot">&lt;-</span>Data[remaining_rows, ]</span></code></pre></div>
<p>Let’s separate the <strong>y</strong> column and the <strong>image
vector columns</strong>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co">#Training data</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>Y_Labels_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">1</span>]  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_training)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="co">#Testing data</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>Y_Labels_testing <span class="ot">&lt;-</span>Data_testing[, <span class="dv">1</span>]  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>Row_Image_Matrix_testing <span class="ot">&lt;-</span>Data_testing[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_testing)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span></code></pre></div>
</div>
<div id="scale-pixel-input-data" class="section level3">
<h3>Scale Pixel Input Data</h3>
<p>Why does the pixel input data need to be scaled? One reason is to
prevent<br> numerical overflow for softmax function. The softmax
function is&lt; used to<br> convert the output of the neural network
into a probability distribution.<br> The softmax function becomes
numerically unstable with very large values.<br> Thousands of inputs for
softmax quickly explodes softmax output value.<br> Scaled input values
helps to prevent numerical overflow for softmax.<br> Another reason is
to redude the number of iterations needed for the<br> neural network
convergence. The neural network converges faster with<br> scaled input
data.</p>
<p>After splitting the data into training and testing data. We need to
scale<br> the data. Calculate statistical values from training data
(e.g. mean,<br> standard deviation, min-max values, etc.). Then use
these values to scale<br> training and testing data with the same
statistical values. This is to<br> prevent data leakage from testing
data to training data. In other words.<br> We obtain consistency and
reliable performance evaluations for the model.<br></p>
<div id="generalized-min-max-normalization." class="section level4">
<h4>Generalized min-max normalization.</h4>
<p>We can scale between any two numbers with the general min-max<br>
normalization formula. Input data is between lower and upper bounds.
0<br> and 1 are the most common used lower and upper bounds. The
generalized<br> min-max normalization formula is defined as
follows:<br></p>
<p><span class="math display">\[ Rescale = lowerbound + \frac{[(i -
i_{min})*(upperbound - lowerbound)]}{i_{range}}  \]</span></p>
<p>Pixels values <span class="math inline">\(i\)</span> are between 0
and 255.<br> Rescaled pixel values can be between any number (e.g. 0 and
1).<br></p>
</div>
<div id="standardized-z-score-normalization." class="section level4">
<h4>Standardized z-score normalization.</h4>
<p>Another way to scale the pixel input data is to use the
standardized<br> z-score normalization formula. The standardized z-score
normalization<br> formula is defined as follows:<br></p>
<p><span class="math display">\[ Rescale = \frac{i - \mu}{\sigma}
\]</span></p>
<p>Where:<br></p>
<p><span class="math display">\[ i = \text{pixel value}
\]</span><br></p>
<p><span class="math display">\[ \mu = \text{mean of pixel values}
\]</span><br></p>
<p><span class="math display">\[ \sigma = \text{standard deviation of
pixel values} \]</span><br></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Set the scale_var to 0 for generalized min-max normalization.</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># Set the scale_var to 1 for standardized z-score normalization.</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>scale_var <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="cf">if</span> (scale_var <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># Generalized min-max normalization.</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>i_min <span class="ot">&lt;-</span> <span class="fu">min</span>(Row_Image_Matrix_training)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>i_range <span class="ot">&lt;-</span> <span class="fu">max</span>(Row_Image_Matrix_training) <span class="sc">-</span> i_min</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>lower_bound <span class="ot">&lt;-</span> <span class="dv">0</span>  <span class="co"># set your desired lower bound</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>upper_bound <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># set your desired upper bound</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a><span class="co">#Rescale training matrix.</span></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span>lower_bound <span class="sc">+</span> (((Row_Image_Matrix_training <span class="sc">-</span> i_min) <span class="sc">*</span> (upper_bound <span class="sc">-</span> lower_bound)) <span class="sc">/</span> i_range)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a><span class="co">#Rescale test matrix.</span></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a><span class="co"># Use the same statistical values from training data to scale testing data.</span></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>Row_Image_Matrix_testing <span class="ot">&lt;-</span>lower_bound <span class="sc">+</span> (((Row_Image_Matrix_testing <span class="sc">-</span> i_min) <span class="sc">*</span> (upper_bound <span class="sc">-</span> lower_bound)) <span class="sc">/</span> i_range)</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a><span class="co"># Standardized z-score normalization.</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a><span class="co"># Rescale training matrix.</span></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a><span class="co"># mean of training data</span></span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>mean_training <span class="ot">&lt;-</span><span class="fu">mean</span>(Row_Image_Matrix_training)</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a><span class="co"># Calculate the sample standard deviation </span></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>sd_training <span class="ot">&lt;-</span> <span class="fu">sd</span>(Row_Image_Matrix_training)</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a><span class="co">#Rescale training matrix.</span></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a><span class="co"># Standardize training data using the sample standard deviation and mean</span></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span> (Row_Image_Matrix_training <span class="sc">-</span> mean_training) <span class="sc">/</span> sd_training</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a><span class="co"># Rescale testing matrix.</span></span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a><span class="co"># Rescale test matrix.</span></span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a><span class="co"># Use the same statistical values from training data to scale testing data.</span></span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>Row_Image_Matrix_testing <span class="ot">&lt;-</span> (Row_Image_Matrix_testing <span class="sc">-</span> mean_training) <span class="sc">/</span> sd_training</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>}</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a><span class="co">#Scaled Training/Testing data.</span></span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a><span class="co"># Bind Y_Labels_training and Row_Image_Matrix_training </span></span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a>Data_training_scaled <span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="fu">t</span>(Y_Labels_training), Row_Image_Matrix_training)  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a><span class="co"># Bind Y_Labels_testing and Row_Image_Matrix_testing</span></span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>Data_testing_scaled <span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="fu">t</span>(Y_Labels_testing), Row_Image_Matrix_testing)  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span></code></pre></div>
</div>
</div>
</div>
<div id="feedforward" class="section level2">
<h2>Feedforward</h2>
<div id="no-mapping-in-neural-networks" class="section level3">
<h3>No Mapping in Neural Networks</h3>
<p>The M weight matrix is a matrix of random numbers. The multiplication
of<br> the M and X matrix creates planes in <strong>d</strong>
dimensions. A neural network is<br> nothing more than the optimization
of planes in <strong>d</strong> dimensions. The<br> planes in
<strong>d</strong> dimensions cross through some point
<strong>b</strong> on the <strong>y-axis</strong>.<br> What does no
mapping mean? It means that input data is not transformed by<br> any
function. In other words, the input X matrix is simply.<br></p>
<p><span class="math display">\[X_{i} = X_{d} \]</span><br></p>
<p>Whereas neural networks with Fourier Feature mapping, the input X
matrix<br> is transformed by some function. In other words, the input X
matrix is.<br></p>
<p><span class="math display">\[X_{i} = \gamma(X_{d})\]</span><br></p>
<p>Note that <span class="math inline">\(X_{i}\)</span> is shorthand for
<span class="math inline">\(X_{input}\)</span> and <span class="math inline">\(X_{d}\)</span> is shorthand for <span class="math inline">\(X_{data}\)</span>.<br></p>
</div>
<div id="fourier-feature-mapping-in-neural-networks" class="section level3">
<h3>Fourier Feature Mapping in Neural Networks</h3>
<p>Fourier feature mapping means that the neural network is trained on
the<br> scaled pixel values that are first passed through a Fourier
transform.<br> The Fourier series is defined as:<br></p>
<p><span class="math display">\[f(x) = a_{0} + \sum_{n=1}^{\infty}
\left[ a_{n}\cos\left(\frac{2\pi nx}{f}\right) +
b_{n}\sin\left(\frac{2\pi nx}{f}\right) \right] \]</span></p>
<p>where:<br></p>
<p><span class="math display">\[f = \text{frequency} \]</span></p>
<p><span class="math display">\[H = 1\ 2\ 3\ 4\ \ldots\ n =
\text{harmonic numbers}\]</span></p>
<p>I will build up to the Fourier series. Not the most
mathematically<br> rigorous explanation, but it will do. First define
the input vector <span class="math inline">\((X_{d})_{v}\)</span><br> as
a single observation. The input vector is size 1 x n. For Mnist<br>
dataset, input vector is size 1 x 784. Next, pass the input vector
through<br> an alternating sine cos function <span class="math inline">\(\gamma(X_{d})_{v}\)</span>.<br></p>
<p><span class="math display">\[\gamma(X_{d})_{v} = \cos\left(\frac{\pi
x_{1d}}{f_{1}}\right) + \sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  +
\cdots + \cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right) \]</span></p>
<p>Pass <span class="math inline">\(\gamma(X_{d})_{v}\)</span> matrix
through the bias operator.<br></p>
<p><span class="math display">\[\gamma(X_{d})_{v}
\underset{bias}{\rightarrow} X1_{v} = 1 + \cos\left(\frac{\pi
x_{1d}}{f_{1}}\right) + \sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  +
\cdots + \cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right) \]</span></p>
<p>Cos and sine map onto the x values for the X matrix. Note that <span class="math inline">\(x_{b} = 1\)</span>,<br> which represents the bias
node.<br></p>
<p><span class="math display">\[X1_{v}=1 + \cos\left(\frac{\pi
x_{1d}}{f_{1}}\right) + \sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  +
\cdots + \cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right) \]</span></p>
<p><span class="math display">\[X1_{v}= 1 + x_{1} + x_{2} + \cdots +
x_{n} \]</span></p>
<p>Multiply M weight vector by X1 vector.<br></p>
<p><span class="math display">\[M_{v}X1_{v} =w_{b}(1) +
w_{1}\cos\left(\frac{\pi x_{1d}}{f_{1}}\right) +
w_{2}\sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  + \cdots +
w_{n}\cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
w_{n}\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right)  \]</span></p>
<p><span class="math display">\[M_{v}X1_{v} = w_{b}(1) + w_{1}x_{1} +
w_{2}x_{2} + \cdots + w_{n}x_{n} \]</span></p>
<p><span class="math inline">\(M_{v}X1_{v}\)</span> can be seen as a
Fourier series. <span class="math inline">\(w_{cos}\)</span> are weights
that<br> correspond to the cosine terms. <span class="math inline">\(w_{sin}\)</span> are weights that correspond
to<br> the sine terms. <span class="math inline">\(w_{b}\)</span> is the
bias weight. The Fourier series can be<br> written as:<br></p>
<p><span class="math display">\[M_{v}X1_{v} = w_{b}(1) +
\sum_{n=1}^{\infty} \left[ w_{cos}\cos\left(\frac{2\pi nx}{f}\right) +
w_{sin}\sin\left(\frac{2\pi nx}{f}\right) \right] \]</span></p>
<p>In the papaer, “Fourier Features Let Networks Learn High
Frequency<br> Functions in Low Dimensional Domains” by Tancik et al. The
authors<br> recommend setting the frequency as ramdom points from a
Gaussian<br> distribution <span class="math inline">\(\mathcal{N}(0,\sigma^{2})\)</span>. What does
ramdomly sampling points from<br> a Gaussian distribution mean? Let’s
say that <span class="math inline">\(\sigma = 1\)</span>, which is about
68%<br> of the area under the curve. Then at <span class="math inline">\(\sigma = 1\)</span> means that points are<br>
randomly sampled from about 68% area under the curve. Let’s say that
<span class="math inline">\(\sigma = 2\)</span>,<br> which means that
points are randomly sampled from about 95% of the area<br> under the
curve. So on and so forth. The higher the sigma value, then<br> there’s
more area under the curve that can be randomly sampled for the<br> B
vector.<br></p>
<p><span class="math display">\[f = B = \text{randomly sampled points
from a Gaussian distribution} \]</span></p>
<p>Thus applying H (harmonic numbers) and B to <span class="math inline">\(\gamma(X_{d})_{v}\)</span>, you get<br> fourier
feature mapping.<br></p>
<p><span class="math display">\[X_{i} = \gamma(X_{d})_{v} =  \left[
\cos\left({H\pi B x}\right), \sin\left({H\pi B x}\right) \right]^T
\]</span></p>
</div>
<div id="x-input-matrix-preparation" class="section level3">
<h3>X Input Matrix Preparation</h3>
<p>For no mapping use regular pixel values. For Fourier feature mapping
use<br> Fourier series scaled pixel values. X training matrix for batch
gradient<br> descent is simply the transpose of the image
matrix.<br></p>
<p><span class="math display">\[ X1_{training} = Transpose(Image Matrix)
= \begin{bmatrix}
pixel0 &amp; pixel28 &amp; pixel56 &amp; \cdots &amp; pixel756 \\
pixel1 &amp; pixel29 &amp; pixel57 &amp; \cdots &amp; pixel757 \\
pixel2 &amp; pixel30 &amp; pixel58 &amp; \cdots &amp; pixel758 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
pixel27 &amp; pixel55 &amp; pixel83 &amp; \cdots &amp; pixel783 \\
\end{bmatrix} \]</span></p>
<p>Note that observation is used as a synonym for training example.
For<br> stochastic gradient descent. We randomly select 1 observation
from<br> training data. Then transpose to make each column an
observation. For<br> mini-batch gradient descent, we randomly select
some n random observations<br> from training data. Then transpose the
resulting matrix to make each<br> column an observation.<br></p>
<p>For example, let’s say we randomly select 3 observations (n=3)
from<br> training data. Then transpose to make each column as an
observation.<br></p>
<p><span class="math display">\[ X1_{training} = \begin{bmatrix}
observation24147 &amp; observation587 &amp; observation13371\\
\end{bmatrix}  =\]</span></p>
<p><span class="math display">\[ Transpose(Image Matrix) =
\begin{bmatrix}
pixel0 &amp; pixel28 &amp; pixel56 \\
pixel1 &amp; pixel29 &amp; pixel57 \\
pixel2 &amp; pixel30 &amp; pixel58 \\
\vdots &amp; \vdots &amp; \vdots \\
pixel27 &amp; pixel55 &amp; pixel83 \\
\end{bmatrix} \]</span></p>
<p>Note that I set the columns of the X1 matrix to be observations. One
can<br> likewise set the rows of the X1 matrix to be observations. It
doesn’t<br> matter if the rows or columns are set as observations. The
important<br> thing is to be consistent throughout the the mathematical
derivation and<br> the implemented code.<br></p>
</div>
<div id="simplification-of-weight-and-input-matrice" class="section level3">
<h3>Simplification of Weight and Input Matrice</h3>
<p>With every layer with an activation function, the none bias input
X<br> matrix is multiplied by the none bias weight matrix. Then the bias
input<br> X matrix is multiplied by the bias weight matrix. Then the two
resulting<br> matrices are added together. Here’s a question, can we
simplify the<br> multiplication of the weight and input matrices? The
answer is yes.<br> Before passing through the LeakyReLU activation
function. The weight<br> matrix and input matrices can be simplified as
follows:<br></p>
<p><span class="math display">\[m_{m \; x \; n} * x_{n \; x \; p} +
m_{bias, m \; x \; 1 } * x_{bias, 1 \; x \; p} = MX_{m \; x \; p}
\]</span></p>
<p>For example: <span class="math display">\[ \begin{bmatrix}
w1 &amp; w2 \\
w3 &amp; w4\\
\end{bmatrix}_{m \; x \; n} *  \begin{bmatrix}
x1 &amp; x3 \\
x2 &amp; x4\\
\end{bmatrix}_{n \; x \; p} + \begin{bmatrix}
wb1 \\
wb2 \\
\end{bmatrix}_{m \; x \; 1} *  \begin{bmatrix}
xb1 &amp; xb2 \\
\end{bmatrix}_{1 \; x \; p} = \]</span></p>
<p><span class="math display">\[\left[\begin{array}{cc}
w1 &amp; w2 \\
w3 &amp; w4 \\
\end{array}\right.
\left.\begin{array}{c}
\begin{bmatrix}
wb1 \\
wb2 \\
\end{bmatrix}
\end{array}\right]_{m \; x \; (n+1)}
*  
\begin{bmatrix}
x1 &amp; x3\\
x2 &amp; x4\\
[xb1 &amp; xb2]\\
\end{bmatrix}_{(n+1) \; x \; p} \]</span></p>
<p>Where: <span class="math display">\[\begin{bmatrix}xb1 &amp; xb2 = 1
&amp; 1 \end{bmatrix} \]</span> Thus:</p>
<p><span class="math display">\[ \left[\begin{array}{cc}
w1 &amp; w2 \\
w3 &amp; w4 \\
\end{array}\right.
\left.\begin{array}{c}
\begin{bmatrix}
wb1 \\
wb2 \\
\end{bmatrix}
\end{array}\right]_{m \; x \; (n+1)}
*  
\begin{bmatrix}
x1 &amp; x3\\
x2 &amp; x4\\
[1 &amp; 1]\\
\end{bmatrix}_{(n+1) \; x \; p} = MX_{m \; x \; p} \]</span></p>
<p>Define the bias operator as adding a row of ones to the input
matrix.<br></p>
<p><span class="math display">\[x_{n \; x \; p}
\underset{bias}{\rightarrow} X_{(n + 1) \; x \; p} \]</span></p>
<p>As the previous example shows: <span class="math display">\[\begin{bmatrix}
x1 &amp; x3 \\
x2 &amp; x4\\
\end{bmatrix}_{n \; x \; p} \underset{bias}{\rightarrow} \begin{bmatrix}
x1 &amp; x3\\
x2 &amp; x4\\
[1 &amp; 1]\\
\end{bmatrix}_{(n+1) \; x \; p} \]</span></p>
<p>Therefore, instead of doing two operations. That is to say.
Multiplying<br> bias matrix and non-bias matrix by their corresponding
input matrices,<br> then adding the multiplied matrices. Both weight
matrix and input matrix<br> can be combined into one weight and one
input matrix. Thus, simplifying<br> the computation into one
operation.<br></p>
</div>
<div id="leakyrelu-activation-function" class="section level3">
<h3>LeakyReLU Activation Function</h3>
<p>Activation functions in the hidden layers are used to introduce
non-linearity<br> into the neural network. LeakyReLU was chosen because
of it’s simplicity<br> and it’s ability to prevent the dying ReLU
problem. The dying ReLU problem<br> is when the ReLU activation function
outputs 0 for all negative inputs.<br> Thus, the gradient becomes 0 and
the network cannot perform backpropagation.<br> LeakyReLU solves this
problem by having a small positive slope for negative<br> inputs. Note
that setting alpha to 0 makes LeakyReLU the same as ReLU.<br> Hence
LeakyReLU is the more general form of ReLU. The LeakyReLU activation<br>
function is simply a piecewise function which is defined as
follows:<br></p>
<p><span class="math display">\[LeakyReLU(x) =
\begin{cases}
\alpha*x &amp; \text{if } x &lt; 0  \\
x &amp; \text{if } x \geq 0
\end{cases} \]</span><br></p>
<p>Plot for LeakyReLU.<br> <img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="softmax-activation-function" class="section level3">
<h3>Softmax Activation Function</h3>
<p>The softmax activation function is used to convert the output of
the<br> neural network into a probability distribution. Softmax
essentially<br> predicts the probability that an observation belongs to
a particular<br> class. The softmax function is defined as
follows:<br></p>
<p><span class="math display">\[Softmax(z) =
\frac{e^{z_{i}}}{\sum_{j=1}^{Q} e^{z_{j}}} \]</span></p>
<p>As previously stated:<br> <span class="math display">\[NumberClasses  = Q = 10 \]</span></p>
<p>For better numerical stability, the softmax function is modified as
follows:<br></p>
<p><span class="math display">\[Softmax(z)_{Max}  = \frac{e^{z_{i} -
max(z)}}{\sum_{j=1}^{Q} e^{z_{j} - max(z)}} \]</span></p>
<p>Taking log of both sides:<br></p>
<p><span class="math display">\[ln[Softmax(z)_{Max}] =
ln\left(\frac{e^{z_{i} - max(z)}}{\sum_{j=1}^{Q} e^{z_{j} -
max(z)}}\right) \]</span></p>
<p>Apply the log identity to the softmax function:<br></p>
<p><span class="math display">\[ln(\frac{a}{b}) =ln(a) -
ln(b)  \]</span></p>
<p><span class="math display">\[ln(Softmax(z)_{Max}) =  ln\left[e^{z_{i}
- max(z)}\right] - ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right]
\]</span></p>
<p><span class="math display">\[ln(Softmax(z)_{Max}) =  z_{i} - max(z) -
ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right] \]</span></p>
<p><span class="math display">\[e^{ln(Softmax(z)_{Max})}  = e^{z_{i} -
\max(z) - ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right]}
\]</span></p>
<p>After some algebraic manipulation, the final version of the softmax
function is redefined<br> as follows:<br></p>
<p><span class="math display">\[Softmax(z)_{Max}  = e^{z_{i} - \max(z) -
ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right]} \]</span></p>
</div>
<div id="one-hot-encoding-matrix" class="section level3">
<h3>One Hot Encoding Matrix</h3>
<p>The output of the NN is a vector of 10 probabilities, each
probability<br> represents the probability of the image being a digit
from 0 to 9. But<br> Y_Labels is not a vector of probabilities, it’s a
vector of digits. Hence,<br> Y_Labels needs to be converted into a
vector of probabilities. This is<br> done by using one-hot
encoding.<br></p>
<p>Since I set the columns of the X1 matrix to be observations. Then
each<br> column of the one hot encoding matrix is also an observation.
The one hot<br> encoding matrix is a matrix of zeros with only a single
1 in each column.<br> Mathematically you can think of each column as a
basis vector. In other<br> words, each column is a vector of zeros with
only a single 1. The position<br> of the 1 represents the class. Thus
each column represents an observation.<br> Neural network is trained to
classify images of digits from 0 to 9. Thus<br> row 1 represents digit
0, row 2 represents digit 1, row 3 represents<br> digit 2, so on and so
forth. Example of one hot encoding matrix for Q=10<br> classes and n=5
observations is as follows:<br></p>
<p><span class="math display">\[Y_{Q \; x \; n} = \text{One Hot Encoding
Matrix} \]</span><br></p>
<p><span class="math display">\[Y_{Q \; x \; n} = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix} \]</span><br></p>
<p>One Hot Encoding Training Data<br></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Initialize a list to store the basis vectors</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>basis_vectors_list_training <span class="ot">&lt;-</span><span class="fu">list</span>()</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># Loop over each column of the matrix</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_training) {</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>  <span class="co"># Get the label for the q-th column</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>  label <span class="ot">&lt;-</span> Y_Labels_training[<span class="dv">1</span>, q] <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  </span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>  <span class="co"># Create a vector of zeros with length Q</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>  basis_vectors_training <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, Q)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>  </span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>  <span class="co"># Set the (label+1)-th element to 1</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>  basis_vectors_training[label <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>  </span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>  <span class="co"># Add the basis vector to the list</span></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>  basis_vectors_list_training[[q]] <span class="ot">&lt;-</span>basis_vectors_training <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>}</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a><span class="co"># Convert the list of basis vectors to a matrix</span></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>Y_One_Hot_Encoding_training <span class="ot">&lt;-</span> <span class="fu">do.call</span>(cbind, basis_vectors_list_training)</span></code></pre></div>
<p>One Hot Encoding Testing Data<br></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Initialize a list to store the basis vectors</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>basis_vectors_list_testing <span class="ot">&lt;-</span><span class="fu">list</span>()</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co"># Loop over each column of the matrix</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_testing) {</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>  <span class="co"># Get the label for the q-th column</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>  label <span class="ot">&lt;-</span>Y_Labels_testing[<span class="dv">1</span>, q] <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>  </span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>  <span class="co"># Create a vector of zeros with length Q</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>  basis_vectors_testing <span class="ot">&lt;-</span><span class="fu">rep</span>(<span class="dv">0</span>, Q)</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>  </span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>  <span class="co"># Set the (label+1)-th element to 1</span></span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>  basis_vectors_testing[label <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>  </span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>  <span class="co"># Add the basis vector to the list</span></span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>  basis_vectors_list_testing[[q]] <span class="ot">&lt;-</span>basis_vectors_testing <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>}</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a><span class="co"># Convert the list of basis vectors to a matrix</span></span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>Y_One_Hot_Encoding_testing <span class="ot">&lt;-</span> <span class="fu">do.call</span>(cbind, basis_vectors_list_testing)</span></code></pre></div>
</div>
<div id="categorical-cross-entropy-loss-function" class="section level3">
<h3>Categorical Cross Entropy Loss Function</h3>
<p>The output from the softmax function is passed to the categorical
cross<br> entropy loss function. The purpose of the categorical cross
entropy loss<br> function is to measure the difference between the
predicted probabilities<br> and the actual probabilities. Note that the
hollow circle means hadamard<br> product (element-wise multiplication).
After the hadamard product, the<br> categorical cross entropy loss
function is summed across <strong>Q</strong> classes<br> and then summed
over <strong>n</strong> observations. Finally, the sum is divided by<br>
the number of observations. The mean categorical cross entropy loss<br>
function is defined as follows:<br></p>
<p><span class="math display">\[C_{mean} = -\frac{1}{n} \sum^{n}
\sum^{Q} [Y_{Q \; x \; n} \circ \ln(X_{out})_{Q \; x \; n}]
\]</span><br></p>
<p><span class="math display">\[C_{mean} = -mean\left[ \sum^{Q} [Y_{Q \;
x \; n} \circ \ln(X_{out})_{Q \; x \; n}] \right] \]</span><br></p>
<p>Cost function for each observation is a vector.<br> <span class="math display">\[C_{vector} = C_{v} = -[y_{v} \circ
\ln(x_{out})_{v}] \]</span><br></p>
<p>All cost function vectors make up the cost function matrix.<br> <span class="math display">\[C_{Matrix} = C_{M} = -Y_{Q \; x \; n} \circ
\ln(X_{out})_{Q \; x \; n} \]</span></p>
</div>
<div id="feedforward-summary" class="section level3">
<h3>Feedforward Summary</h3>
<p><span class="math display">\[NumberPixels  = 784 \]</span><br> <span class="math display">\[NumberObservations  = n \]</span><br> <span class="math display">\[NumberClasses  = Q = 10 \]</span> <br></p>
<div id="when-using-one-hidden-layer" class="section level4">
<h4>When Using One Hidden Layer</h4>
<p><span class="math display">\[\text{Number Hidden Neurons}= nu
\]</span> <br></p>
<p><span class="math display">\[Xi_{784 \; x \;
n}  \underset{bias}{\rightarrow}  (X1)_{(784+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M1)_{nu \; x \; 785} *
(X1)_{785 \; x \; n}  = (Z2)_{nu \; x \; n} \]</span><br> <span class="math display">\[LR[(Z2)]_{nu \; x \;
n}  \underset{bias}{\rightarrow}  (X2)_{(nu+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M2)_{Q \; x \; (nu+1)}
* (X2)_{(nu+1) \; x \; n}  = (Z3)_{Q \; x \; n} \]</span><br> <span class="math display">\[(Z3)_{Q \; x \; n}  = (Z_{out})_{Q \; x \;
n}\]</span><br></p>
</div>
<div id="when-using-two-hidden-layers" class="section level4">
<h4>When Using Two Hidden Layers</h4>
<p><span class="math display">\[\text{Number Hidden Neurons 1st layer }=
nu \]</span> <br> <span class="math display">\[\text{Number Hidden
Neurons 2nd layer }= nu2 \]</span> <br></p>
<p><span class="math display">\[Xi_{784 \; x \;
n}  \underset{bias}{\rightarrow}  (X1)_{(784+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M1)_{nu \; x \; 785} *
(X1)_{785 \; x \; n}  = (Z2)_{nu \; x \; n} \]</span><br> <span class="math display">\[LR[(Z2)]_{nu \; x \;
n}  \underset{bias}{\rightarrow}  (X2)_{(nu+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M2)_{nu2 \; x \;
(nu+1)} * (X2)_{(nu+1) \; x \; n}  = (Z3)_{nu2 \; x \; n} \]</span><br>
<span class="math display">\[LR[(Z3)]_{nu2 \; x \;
n}  \underset{bias}{\rightarrow}  (X3)_{(nu2+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M3)_{Q \; x \; (nu2+1)}
* (X3)_{(nu2+1) \; x \; n}  = (Z4)_{Q \; x \; n} \]</span><br> <span class="math display">\[(Z4)_{Q \; x \; n}  = (Z_{out})_{Q \; x \;
n}\]</span><br></p>
</div>
<div id="output-layer" class="section level4">
<h4>Output Layer</h4>
<p><span class="math display">\[[Softmax(Z_{out})_{Max}]_{Q \; x \;
n}  = (X_{out})_{Q \; x \; n} \]</span><br></p>
<p><span class="math display">\[C_{Matrix} = C_{M} = -Y_{Q \; x \; n}
\circ \ln(X_{out})_{Q \; x \; n} \]</span><br> <span class="math display">\[C_{mean} = -mean\left[ \sum^{Q} [Y_{Q \; x \; n}
\circ \ln(X_{out})_{Q \; x \; n}] \right] \]</span><br></p>
</div>
</div>
</div>
<div id="backpropagation" class="section level2">
<h2>Backpropagation</h2>
<p>This is where the magic happens. Backpropagation is where the
neural<br> network learns. The neural network learns by minimizing the
cost function.<br> The cost function is minimized by calculating the
gradient of the cost<br> function with respect to the M weight matrices.
The gradient of the cost<br> function is calculated by using the chain
rule. The chain rule is used to<br> calculate the partial derivatives of
the cost function with respect to<br> the M weight matrices.
Backpropagation is nothing more than finding the<br> partial derivative
of the cost function with respect to the M weight<br> matrices.I did
this via the partial derivative tree diagram. The partial<br> derivative
tree diagram is a visual representation of the chain rule.<br></p>
<div id="one-hidden-layer" class="section level3">
<h3>One Hidden Layer</h3>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m2-weight-matrix" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M2 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M2} = \left(
\frac{\partial C^T}{\partial X3} * \frac{\partial X3^T}{\partial Z3}
\right)^T * \frac{\partial Z3}{\partial M2} \]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\left(\frac{\partial C^T}{\partial X3} *
\frac{\partial X3^T}{\partial Z3}\right)^T = (X3 - Y)_{Q \; x \; n}
\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial M2} =
(X2^T)_{n \; x \; (nu +1)} \]</span><br></p>
<p>thus:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M2} = (X3 -
Y)_{Q \; x \; n} * (X2^T)_{n \; x \; (nu+1)} \]</span><br></p>
<p>The matrix (X3 - Y) is obtained by the matrix multiplication of the
softmax<br> Jabobian matrix with the derivative of the C cost vector
with respect to<br> the output softmax vector.<br></p>
<p>For sanity check, the size of the M2 weight matrix must equal the
size of<br> the derivative of the C cost matrix with respect to the M2
weight matrix.<br></p>
<p>Size of M2 matrix is:<br></p>
<p><span class="math display">\[(M2)_{size}= Q \; x \;
(nu+1)\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M2}_{size} =
(Q \; x \; n) * (n \; x \; (nu+1))\]</span><br> <span class="math display">\[\frac{\partial C}{\partial M2}_{size} = Q \; x \;
(nu+1)\]</span><br></p>
<p><span class="math display">\[(M2)_{size} = \frac{\partial C}{\partial
M2}_{size}\]</span><br></p>
</div>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m1-weight-matrix" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M1 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M1} = \left[
\left( \left( \frac{\partial C^T}{\partial X3} * \frac{\partial
X3^T}{\partial Z3} \right) * \frac{\partial Z3}{\partial X2} \right)^T
\circ \frac{\partial X2}{\partial Z2} \right] * \frac{\partial
Z2}{\partial M1}\]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\left( \frac{\partial C^T}{\partial X3}
* \frac{\partial X3^T}{\partial Z3} \right) = (X3 - Y)^T_{n \; x \; Q}
\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial X2}=
(M2_{-b})_{Q \; x \; nu} \]</span> <br></p>
<p>The subscript -b means that the bias column is removed from the M2
weight<br> matrix. Note that leaky ReLU LR(z) = x2. The bias weights are
multiplied<br> by 1, hence there is no LR(z) activation function in the
bias node. Thus<br> when you take the derivative of the Z3 matrix with
respect to the X2<br> matrix, the bias column is removed because the
bias node has no x2<br> activation function. The derivative dZ3/dX2= 0
for the bias column.<br></p>
<p>You also have:<br></p>
<p><span class="math display">\[\frac{\partial X2}{\partial Z2} =
{[\partial LR(X2_{-b})}]_{nu \; x \; n}\]</span><br></p>
<p>Similarly, the subscript -b means that the bias row is removed from
the<br> X2 matrix. The bias node has only ones, which are not dependent
on previous<br> weights. Thus, when you take the derivative of the X2
matrix with respect<br> to the Z2 matrix, the bias row is removed
because the bias node is not<br> connected to previous nodes. The
derivative dX2/dZ2= 0 for the bias row.<br></p>
<p>You finally have:<br></p>
<p><span class="math display">\[\frac{\partial Z2}{\partial M1}= X1^T_{n
\; x \; 785} \]</span><br></p>
<p>Thus you have:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1} = [[(X3 -
Y)^T * M2_{-b}]^T \circ \partial LR(X2_{-b})] * X1^T \]</span><br></p>
<p>Again for sanity check, the size of the M1 weight matrix must equal
the<br> size of the derivative of the C cost matrix with respect to the
M1 weight<br></p>
<p>Size of M1 matrix is:<br></p>
<p><span class="math display">\[(M1)_{size}= nu \; x \;
785\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1}_{size} =
[[(n \; x \; Q)*(Q \; x \; nu)]^T \circ (nu \; x \; n)] * (n \; x \;
785)\]</span> <span class="math display">\[                               = [(n \; x \;
nu)^T \circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = [(nu \; x \; n)
\circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = (nu \; x \; n) *
(n \; x \; 785)\]</span> <span class="math display">\[\frac{\partial
C}{\partial M1}_{size} = nu \; x \; 785\]</span></p>
<p><span class="math display">\[(M1)_{size} = \frac{\partial C}{\partial
M1}_{size}\]</span><br></p>
</div>
</div>
<div id="two-hidden-layers" class="section level3">
<h3>Two Hidden Layers</h3>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m3-weight-matrix" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M3 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M3} = \left(
\frac{\partial C^T}{\partial X4} * \frac{\partial X4^T}{\partial Z4}
\right)^T * \frac{\partial Z4}{\partial M3} \]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M3} = (X4 -
Y)_{Q \; x \; n} * (X3)_{n \; x \; (nu2+1)}^T \]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\left(\frac{\partial C}{\partial
Z4}\right)^T = (X4 - Y)_{Q \; x \; n} \]</span><br></p>
<p>For sanity check, the size of the M3 weight matrix must equal the
size of<br> the derivative of the C cost matrix with respect to the M3
weight matrix.<br></p>
<p>Size of M3 matrix is:<br></p>
<p><span class="math display">\[(M3)_{size}= Q \; x \;
(nu2+1)\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M3}_{size} =
(Q \; x \; n) * (n \; x \; (nu2+1))\]</span><br> <span class="math display">\[\frac{\partial C}{\partial M3}_{size} = Q \; x \;
(nu2+1)\]</span><br></p>
<p><span class="math display">\[(M3)_{size} = \frac{\partial C}{\partial
M2}_{size}\]</span><br></p>
</div>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m2-weight-matrix-1" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M2 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M2} = \left[
\left( \left( \frac{\partial C^T}{\partial X4} * \frac{\partial
X4^T}{\partial Z4} \right) * \frac{\partial Z4}{\partial X3} \right)^T
\circ \frac{\partial X3}{\partial Z3} \right] * \frac{\partial
Z3}{\partial M2}\]</span><br> <span class="math display">\[\frac{\partial C}{\partial M2} = \left[ \left(
\left(\frac{\partial C}{\partial Z4}\right) * \frac{\partial
Z4}{\partial X3} \right)^T \circ \frac{\partial X3}{\partial Z3} \right]
* \frac{\partial Z3}{\partial M2}\]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial Z3} = \left(
\left(\frac{\partial C}{\partial Z4}\right) * \frac{\partial
Z4}{\partial X3} \right)^T  \circ \frac{\partial X3}{\partial Z3}
\]</span><br></p>
<p>Then:<br></p>
<p><span class="math display">\[\frac{\partial Z4}{\partial X3}=
(M3_{-b})_{Q \; x \; nu2} \]</span> <br></p>
<p><span class="math display">\[\frac{\partial X3}{\partial Z3}=
{[\partial LR(X3_{-b})}]_{nu2 \; x \; n}\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial M2}= X2^T_{n
\; x \; (nu+1)} \]</span><br></p>
<p>Thus you have:<br> <span class="math display">\[\frac{\partial
C}{\partial M2} = \left[\left(\frac{\partial C}{\partial Z4} *
M3_{-b}\right)^T \circ \partial LR(X3_{-b})\right] * X2^T \]</span><br>
<span class="math display">\[\frac{\partial C}{\partial M2} = [[(X4 -
Y)^T * M3_{-b}]^T \circ \partial LR(X3_{-b})] * X2^T \]</span><br></p>
<p>The size of the M2 weight matrix must equal the size of the
derivative of<br> the C cost matrix with respect to the M2
weight<br></p>
<p>Size of M2 matrix is:<br></p>
<p><span class="math display">\[(M2)_{size}= nu2 \; x \; (nu +
1)\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M2}_{size} =
[[(n \; x \; Q)*(Q \; x \; nu2)]^T \circ (nu2 \; x \; n)] * (n \; x \;
(nu+1))\]</span> <span class="math display">\[                               = [(n \; x \;
nu2)^T \circ (nu2 \; x \; n)] * (n \; x \; (nu+1))\]</span> <span class="math display">\[                               = [(nu2 \; x \; n)
\circ (nu2 \; x \; n)] * (n \; x \; (nu +1))\]</span> <span class="math display">\[                               = (nu2 \; x \; n)
* (n \; x \; (nu +1))\]</span> <span class="math display">\[\frac{\partial C}{\partial M2}_{size} = nu2 \; x
\; (nu +1)\]</span></p>
<p><span class="math display">\[(M2)_{size} = \frac{\partial C}{\partial
M2}_{size}\]</span><br></p>
</div>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m1-weight-matrix-1" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M1 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M1} =
\left[\left[\left[ \left( \left( \frac{\partial C^T}{\partial X4} *
\frac{\partial X4^T}{\partial Z4} \right) * \frac{\partial Z4}{\partial
X3} \right)^T \circ \frac{\partial X3}{\partial Z3} \right]^T *
\frac{\partial Z3}{\partial X2}\right]^T \circ \frac{\partial
X2}{\partial Z2}\right] * \frac{\partial Z2}{\partial
M1}\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1} =
\left[\left[ \left(\frac{\partial C}{\partial Z3}\right)^T *
\frac{\partial Z3}{\partial X2}\right]^T \circ \frac{\partial
X2}{\partial Z2}\right] * \frac{\partial Z2}{\partial
M1}\]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial X2}=
(M2_{-b})_{nu2 \; x \; nu} \]</span> <br></p>
<p><span class="math display">\[\frac{\partial X2}{\partial Z2}=
{[\partial LR(X2_{-b})}]_{nu \; x \; n}\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z2}{\partial M1}= X1^T_{n
\; x \; 785} \]</span><br></p>
<p>Thus:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1} =
\left[\left[ \left(\frac{\partial C}{\partial Z3}\right)^T *
M2_{-b}\right]^T \circ \partial LR(X2_{-b})\right] * X1^T
\]</span><br></p>
<p>The size of the M1 weight matrix must equal the size of the
derivative of<br> the C cost matrix with respect to the M1
weight<br></p>
<p>Size of M1 matrix is:<br></p>
<p><span class="math display">\[(M1)_{size}= nu \; x \;
785\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1}_{size} =
[[(n \; x \; nu2)*(nu2 \; x \; nu)]^T \circ (nu \; x \; n)] * (n \; x \;
785)\]</span> <span class="math display">\[                               = [(n \; x \;
nu)^T \circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = [(nu \; x \; n)
\circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = (nu \; x \; n) *
(n \; x \; 785)\]</span> <span class="math display">\[\frac{\partial
C}{\partial M1}_{size} = nu \; x \; 785\]</span></p>
<p><span class="math display">\[(M1)_{size} = \frac{\partial C}{\partial
M1}_{size}\]</span><br></p>
</div>
</div>
</div>
<div id="gradient-descent" class="section level2">
<h2>Gradient Descent</h2>
<p>Gradient descent is applied at the end of backpropagation.
Gradient<br> descent is an algorithm that minimizes the cost function by
iteratively<br> updating the M weight matrices. The M weight matrices
are updated by<br> subtracting the M weight matrix by the gradient of
the cost function with<br> respect to the M weight matrices multiplied
by the learning rate. The<br> learning rate is a hyperparameter that
determines the size of the step<br> taken in the direction of the
gradient. The learning rate is a<br> hyperparameter that needs to be
tuned. If the learning rate is too small,<br> then the algorithm will
take a long time to converge. If the learning<br> rate is too large,
then the algorithm will diverge.<br></p>
<div id="basic-gradient-descent-algorithm" class="section level3">
<h3>Basic Gradient Descent Algorithm</h3>
<p><span class="math display">\[M_{t+1} = M_{t} - \alpha \nabla f(M_{t})
\]</span></p>
<p><span class="math display">\[\alpha = \text{learning rate set through
trial and error} \]</span></p>
<div id="one-hidden-layer-1" class="section level4">
<h4>One Hidden Layer</h4>
<p>For the M2 weight matrix:<br> <span class="math display">\[(M2)_{t+1}
= (M2)_{t} - \alpha \frac{\partial C}{\partial M2} \]</span></p>
<p>For the M1 weight matrix:<br> <span class="math display">\[(M1)_{t+1}
= (M1)_{t} - \alpha \frac{\partial C}{\partial M1} \]</span></p>
</div>
<div id="two-hidden-layers-1" class="section level4">
<h4>Two Hidden Layers</h4>
<p>For the M3 weight matrix:<br> <span class="math display">\[(M3)_{t+1}
= (M3)_{t} - \alpha \frac{\partial C}{\partial M3} \]</span></p>
<p>For the M2 weight matrix:<br> <span class="math display">\[(M2)_{t+1}
= (M2)_{t} - \alpha \frac{\partial C}{\partial M2} \]</span></p>
<p>For the M1 weight matrix:<br> <span class="math display">\[(M1)_{n+1}
= (M1)_{t} - \alpha \frac{\partial C}{\partial M1} \]</span></p>
</div>
</div>
<div id="gradient-descent-with-decaying-learning-rate" class="section level3">
<h3>Gradient Descent With Decaying Learning Rate</h3>
<p>Basically the same as the basic gradient descent algorithm, except
the<br> learning rate is decaying with each iteration/epoch. Variable t
stands<br> for the iteration/epoch number. Where t can refer to the
iteration if<br> using SGD or Mini-batch GD. Variable t can refer to the
epoch if using<br> Batch GD. The learning rate is decaying by a factor
of 1/t.<br></p>
<p><span class="math display">\[\alpha = \alpha_{1}*(1/t) \]</span></p>
<p><span class="math display">\[\alpha_{1} = \text{learning rate alpha1
set through trial and error} \]</span></p>
<p><span class="math display">\[M_{t+1} = M_{t} - \alpha \nabla f(M_{t})
\]</span></p>
</div>
<div id="gradient-descent-with-adam-optimizer" class="section level3">
<h3>Gradient Descent With Adam Optimizer</h3>
<p>Adam optimizer is a combination of RMSprop and momentum. Adam
optimizer<br> is a more advanced gradient descent algorithm. Adam
optimizer is more<br> stable and converges faster than basic gradient
descent. Adam optimizer<br> is the recommended gradient descent
algorithm for neural networks. Beta1 is<br> typically set to 0.9. Beta2
is typically set to 0.999. Epsilon is typically<br> set to 10^-8. Adam
optimzer is defined as follows:<br></p>
<p><span class="math display">\[M_{t} = M_{t-1} - \alpha
\frac{\hat{v1}_{t}}{\sqrt{\hat{v2}_{t}} + \epsilon} \]</span></p>
<p>Initialize:<br></p>
<p><span class="math display">\[v1_{0} = 0 \leftarrow \text{Initialize
1st Momentum} \]</span> <span class="math display">\[v2_{0} = 0
\leftarrow \text{Initialize 2nd Momentum} \]</span> <span class="math display">\[t_{0} = 0 \leftarrow \text{Initialize timestep}
\]</span> <span class="math display">\[\beta_{1} = 0.9 \]</span> <span class="math display">\[\beta_{2} = 0.999 \]</span> <span class="math display">\[\epsilon = 10^{-8} \]</span> <span class="math display">\[\alpha = \text{learning rate set through trial
and error} \]</span></p>
<p>Note v at (t-1) is the momentum from the previous iteration/epoch.
1st and<br> 2nd momentum at any time step t is defined as
follows.<br></p>
<p><span class="math display">\[v1_{t} = \beta_{1}v2_{t-1} +
\left[(1-\beta_{1})\nabla f(M_{t-1})\right] \]</span></p>
<p><span class="math display">\[v2_{t} = \beta_{2}v2_{t-1} +
\left[(1-\beta_{2}) \left[\nabla f(M_{t-1})\right]^2\right]
\]</span></p>
<p><span class="math display">\[\left[\nabla f(M_{t-1})\right]^2 =
\nabla f(M_{t-1}) \circ \nabla f(M_{t-1})\]</span></p>
<p>Note that Beta1 and Beta2 are raised to the power of t. Hence if t=3,
then<br> (Beta1)^3 and (Beta2)^3. Bias-corrected 1st and 2nd momentum at
any time<br> step t is defined as follows:<br></p>
<p><span class="math display">\[\hat{v1}_{t} =
\frac{v1_{t}}{1-\beta_{1}^{t}} \]</span> <span class="math display">\[\hat{v2}_{t} = \frac{v2_{t}}{1-\beta_{2}^{t}}
\]</span></p>
<div id="one-hidden-layer-adam-optimizer-for-m2-weight-matrix" class="section level4">
<h4>One Hidden Layer Adam Optimizer for M2 Weight Matrix</h4>
<p>For the M2 weight matrix Initialize v1, v2, and t to 0. Then 1st
and<br> 2nd momentum at any time step t for M2 matrix.<br></p>
<p><span class="math display">\[v1_{t} = \beta_{1}v_{t-1} +
\left[(1-\beta_{1})\frac{\partial C}{\partial M2}\right] \]</span></p>
<p><span class="math display">\[v2_{t} = \beta_{2}v_{t-1} +
\left[(1-\beta_{2}) \left[\frac{\partial C}{\partial M2}\right]^2\right]
\]</span></p>
<p><span class="math display">\[\left[\frac{\partial C}{\partial
M2}\right]^2 = \frac{\partial C}{\partial M2} \circ \frac{\partial
C}{\partial M2}\]</span></p>
<p>Then calculate Bias-corrected 1st and 2nd momentum at any time step t
for M2<br> matrix.<br></p>
<p><span class="math display">\[\hat{v1}_{t} =
\frac{v1_{t}}{1-\beta_{1}^{t}} \]</span> <span class="math display">\[\hat{v2}_{t} = \frac{v2_{t}}{1-\beta_{2}^{t}}
\]</span></p>
<p>Finally, the M2 weight matrix is updated.<br></p>
<p><span class="math display">\[M2_{t} = M2_{t-1} - \alpha
\frac{\hat{v1}_{t}}{\sqrt{\hat{v2}_{t}} + \epsilon} \]</span></p>
<p>Note that <span class="math inline">\(\hat{v1}_{t}\)</span> and <span class="math inline">\(\hat{v2}_{t}\)</span> are matrices with same
dimension as M2. Hence in the formula, <span class="math inline">\(\frac{\hat{v1}_{t}}{\sqrt{\hat{v2}_{t}} +
\epsilon}\)</span><br> represents element-wise division, where each
element in <span class="math inline">\(\hat{v1}_{t}\)</span> is divided
by the corresponding element in <span class="math inline">\(\sqrt{\hat{v2}_{t}} + \epsilon\)</span>.<br></p>
</div>
</div>
</div>
<div id="training-and-accuracy-of-neural-network" class="section level2">
<h2>Training and Accuracy of Neural Network</h2>
<div id="training-phase" class="section level3">
<h3>Training Phase</h3>
<p>What is the training phase for NN? The training phase is composed
of<br> feedforward, backpropagation, then gradient descent. Then start
over<br> again. Feedforward, backpropagation, and finally gradient
descent. So on<br> and so forth. One iteration of feedforward,
backpropagation, and gradient<br> descent through every single
observation is called an epoch.So then, after<br> how many epochs do you
need to stop training? Training stoppage is<br> determined by the most
optimal solution from the testing dataset. Once the<br> minima for the
testing data is reached, training is optimized. The minima<br> for the
testing data is determined by the mean categorical cross entropy<br>
loss function.<br></p>
</div>
<div id="accuracy-of-neural-network" class="section level3">
<h3>Accuracy of Neural Network</h3>
<p><span class="math inline">\(X_{out}\)</span> is the output
probability. Each column is an observation. Assuming<br> that the
highest predicted class from each column represents the model’s<br>
final classification decision. Then extracting the highest
probability<br> from each column, <span class="math inline">\((X_{out})_{highest}\)</span> is a vector with the
highest<br> probability for each column.<br></p>
<p><span class="math display">\[X_{out}
\underset{collapse\;vector}{\rightarrow} [(X_{out})_{highest}]_{1 \; x
\;n} \]</span></p>
<p><span class="math inline">\(Y\)</span> is the one hot encoding matrix
which contains all one hot encoding<br> vectors. Matrix <span class="math inline">\(Y\)</span> contains all the correct probabilities
for each<br> observation. In theory, if 100% accuracy was ever reached,
then element<br> wise multiplication of <span class="math inline">\(X_{out} \circ Y= X_{out}\)</span>. But if 100%
accuracy<br> is not reached, then <span class="math inline">\(X_{out}
\circ Y \neq X_{out}\)</span>. The difference between<br> <span class="math inline">\(X_{out} \circ Y\)</span> and <span class="math inline">\(X_{out}\)</span> is the error. The error is the
difference between<br> the predicted probability and the correct
probability. <span class="math inline">\(X_{out} \circ Y\)</span> has
mostly<br> zeros. Each column only contains one non zero value. Hence if
you collapse<br> <span class="math inline">\(X_{out} \circ Y\)</span>
into a vector. The collapsed vector contains the correct<br> predictions
and the incorrect predictions for all observations.<br></p>
<p><span class="math display">\[(X_{out})_{Q \; x \; n} \circ Y_{Q \; x
\; n} \underset{collapse\;vector}{\rightarrow} [(X_{out} \circ
Y)_{v}]_{1 \; x \;n} \]</span></p>
<p>We can find out the correct prediction for each observation by
subtracting<br> <span class="math inline">\([(X_{out} \circ Y)_{v}]_{1
\; x \;n}\)</span> by <span class="math inline">\([(X_{out})_{highest}]_{1 \; x \;n}\)</span>. The
zeros in the vector<br> means the NN made the correct prediction for
that observation. The non<br> zero values in the vector means the NN
made the incorrect prediction for<br> that observation.<br></p>
<p><span class="math display">\[[[(X_{out})_{highest}]_{1 \; x \;n} -
(X_{out} \circ Y)_{v}]_{1 \; x \;n}\]</span></p>
<p>Count number of zeros to obtain the number of correct
predictions.<br></p>
<p><span class="math display">\[count([[(X_{out})_{highest}]_{1 \; x
\;n} - (X_{out} \circ Y)_{v}]_{1 \; x \;n})\]</span></p>
<p>Thus accuracy = (count)(100%)/n, where n=total number of
observations.<br></p>
<p><span class="math display">\[Accuracy \; \% = \frac{count}{n} * 100\%
\]</span></p>
</div>
</div>
<div id="setting-up-model-parameters-and-data-structures" class="section level2">
<h2>Setting Up Model Parameters and Data Structures</h2>
<div id="tunenable-hyperparameters" class="section level3">
<h3>Tunenable Hyperparameters</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co">#12</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co"># &quot;No mapping&quot; = No mapping</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co"># &quot;Feature Mapping&quot; = Fourier Feature Mapping</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>NoMapping_vs_FourierFeatureMapping <span class="ot">&lt;-</span><span class="st">&quot;No mapping&quot;</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co"># Sigma value for Gaussian distribution for Fourier Feature Mapping</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>sigma <span class="ot">&lt;-</span><span class="dv">60</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co">#hyperparameter for learning rate</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>Alpha_ONE <span class="ot">&lt;-</span><span class="dv">10</span><span class="sc">^-</span><span class="dv">3</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="co">#Beta1</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>Beta1 <span class="ot">&lt;-</span><span class="fl">0.9</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a><span class="co">#Beta2</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>Beta2 <span class="ot">&lt;-</span><span class="fl">0.999</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a><span class="co">#Epsilon</span></span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>Epsilon <span class="ot">&lt;-</span><span class="dv">10</span><span class="sc">^-</span><span class="dv">8</span></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a><span class="co"># Initialize 1st moment at t=1</span></span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>v1 <span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a><span class="co"># Initialize 2nd moment at t=1</span></span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>v2 <span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a><span class="co"># GRADIENT DESCENT ALGORITHMS </span></span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a><span class="co"># &quot;constant&quot; = constant learning rate</span></span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a><span class="co"># &quot;decaying&quot; = decaying learning rate (1/t)</span></span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a><span class="co">#     &quot;adam&quot; = adam optimizer</span></span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a>gradient_descent_algorithm <span class="ot">&lt;-</span><span class="st">&quot;adam&quot;</span></span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a><span class="co">#Number of training observations used in each batch.</span></span>
<span id="cb10-34"><a href="#cb10-34" tabindex="-1"></a><span class="co"># Set n=1 for SGD (1 observation)</span></span>
<span id="cb10-35"><a href="#cb10-35" tabindex="-1"></a><span class="co"># Set 1 &lt;n&lt; N_training for Mini-batch GD (more than 1, but less than all observations)</span></span>
<span id="cb10-36"><a href="#cb10-36" tabindex="-1"></a><span class="co"># Set n= N_training, Batch GD (All observation)</span></span>
<span id="cb10-37"><a href="#cb10-37" tabindex="-1"></a>n <span class="ot">&lt;-</span><span class="dv">350</span></span>
<span id="cb10-38"><a href="#cb10-38" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" tabindex="-1"></a><span class="co"># Setting LeakyReLU_alpha = 0 is the same as using ReLU.</span></span>
<span id="cb10-40"><a href="#cb10-40" tabindex="-1"></a>LeakyReLU_alpha <span class="ot">&lt;-</span><span class="fl">0.1</span></span>
<span id="cb10-41"><a href="#cb10-41" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" tabindex="-1"></a><span class="co"># 1 for one hidden layer and 2 for two hidden layers.</span></span>
<span id="cb10-43"><a href="#cb10-43" tabindex="-1"></a><span class="co"># Code is only set up for one or two hidden layers.</span></span>
<span id="cb10-44"><a href="#cb10-44" tabindex="-1"></a><span class="co"># Thus 1 and 2 are the only allowed values for num_hidden_layers.</span></span>
<span id="cb10-45"><a href="#cb10-45" tabindex="-1"></a>num_hidden_layers <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb10-46"><a href="#cb10-46" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" tabindex="-1"></a><span class="co"># Code not set up for one neuron in hidden layer, thus n=1 is not allowed.</span></span>
<span id="cb10-48"><a href="#cb10-48" tabindex="-1"></a><span class="co"># Apart from that. Any other positive integer is allowed.</span></span>
<span id="cb10-49"><a href="#cb10-49" tabindex="-1"></a><span class="co"># Number of hidden neurons in 1st hidden layer.</span></span>
<span id="cb10-50"><a href="#cb10-50" tabindex="-1"></a>nu <span class="ot">&lt;-</span><span class="dv">100</span></span>
<span id="cb10-51"><a href="#cb10-51" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" tabindex="-1"></a><span class="co"># Same as other hidden layer, n=1 is not allowed.</span></span>
<span id="cb10-53"><a href="#cb10-53" tabindex="-1"></a><span class="co"># Number of hidden neurons in 2nd hidden layer.</span></span>
<span id="cb10-54"><a href="#cb10-54" tabindex="-1"></a>nu2 <span class="ot">&lt;-</span><span class="dv">20</span></span>
<span id="cb10-55"><a href="#cb10-55" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb10-57"><a href="#cb10-57" tabindex="-1"></a><span class="do">##############################################</span></span>
<span id="cb10-58"><a href="#cb10-58" tabindex="-1"></a><span class="co"># SGD AND MINI BATCH GRADIENT DESCENT</span></span>
<span id="cb10-59"><a href="#cb10-59" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb10-60"><a href="#cb10-60" tabindex="-1"></a><span class="co"># Total number of observations for training</span></span>
<span id="cb10-61"><a href="#cb10-61" tabindex="-1"></a>Tb <span class="ot">&lt;-</span>N_training</span>
<span id="cb10-62"><a href="#cb10-62" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" tabindex="-1"></a><span class="co"># Number of observations per iteration</span></span>
<span id="cb10-64"><a href="#cb10-64" tabindex="-1"></a>n_iteration <span class="ot">&lt;-</span>n</span>
<span id="cb10-65"><a href="#cb10-65" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" tabindex="-1"></a><span class="co"># Number of batches = number of iterations per epoch</span></span>
<span id="cb10-67"><a href="#cb10-67" tabindex="-1"></a>number_batches <span class="ot">&lt;-</span>Tb<span class="sc">/</span>n_iteration</span>
<span id="cb10-68"><a href="#cb10-68" tabindex="-1"></a>number_iterations_epoch <span class="ot">&lt;-</span>number_batches</span>
<span id="cb10-69"><a href="#cb10-69" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" tabindex="-1"></a><span class="co"># Total Number of epochs</span></span>
<span id="cb10-71"><a href="#cb10-71" tabindex="-1"></a>Epoch <span class="ot">&lt;-</span><span class="dv">4</span></span>
<span id="cb10-72"><a href="#cb10-72" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" tabindex="-1"></a><span class="co"># (Epoch*number_iterations_epoch) = total number of iterations per epoch</span></span>
<span id="cb10-74"><a href="#cb10-74" tabindex="-1"></a>Epochs <span class="ot">&lt;-</span>Epoch<span class="sc">*</span>number_iterations_epoch</span>
<span id="cb10-75"><a href="#cb10-75" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb10-77"><a href="#cb10-77" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb10-78"><a href="#cb10-78" tabindex="-1"></a><span class="co">#BATCH GRADIENT DESCENT</span></span>
<span id="cb10-79"><a href="#cb10-79" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb10-80"><a href="#cb10-80" tabindex="-1"></a><span class="co"># Total Number of epochs</span></span>
<span id="cb10-81"><a href="#cb10-81" tabindex="-1"></a>Epochs <span class="ot">&lt;-</span><span class="dv">3</span></span>
<span id="cb10-82"><a href="#cb10-82" tabindex="-1"></a></span>
<span id="cb10-83"><a href="#cb10-83" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="weight-matrices" class="section level3">
<h3>Weight Matrices</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="do">##############################################</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co"># ONE HIDDEN LAYER</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co"># Size of matrix M1.</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>M1_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>M1_n_col <span class="ot">&lt;-</span><span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a><span class="co"># Size of matrix X1.</span></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>X1_m_rows <span class="ot">&lt;-</span><span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>X1_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a><span class="co"># Size of matrix Z2.</span></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>Z2_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>Z2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a><span class="co"># Size of LeakyReLU.</span></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>LeakyReLU_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>LeakyReLU_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a><span class="co"># Size of matrix M2.</span></span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>M2_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>M2_n_col <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a><span class="co"># Size of matrix X2</span></span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a><span class="co"># Add 1 to the number of rows of LeakyReLU to account for the bias.</span></span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a>X2_m_rows <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a>X2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a><span class="co"># Size of matrix Z3.</span></span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a>Z3_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>Z3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-33"><a href="#cb11-33" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" tabindex="-1"></a><span class="co"># Size of Softmax. Where softmax = X3 =X_out</span></span>
<span id="cb11-35"><a href="#cb11-35" tabindex="-1"></a>X3_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-36"><a href="#cb11-36" tabindex="-1"></a>X3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-37"><a href="#cb11-37" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb11-38"><a href="#cb11-38" tabindex="-1"></a><span class="do">##############################################</span></span>
<span id="cb11-39"><a href="#cb11-39" tabindex="-1"></a><span class="co"># TWO HIDDEN LAYERS</span></span>
<span id="cb11-40"><a href="#cb11-40" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb11-41"><a href="#cb11-41" tabindex="-1"></a><span class="co"># Size of matrix M1.</span></span>
<span id="cb11-42"><a href="#cb11-42" tabindex="-1"></a>M1_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-43"><a href="#cb11-43" tabindex="-1"></a>M1_n_col <span class="ot">&lt;-</span> <span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-44"><a href="#cb11-44" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" tabindex="-1"></a><span class="co"># Size of matrix X1.</span></span>
<span id="cb11-46"><a href="#cb11-46" tabindex="-1"></a>X1_m_rows <span class="ot">&lt;-</span> <span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-47"><a href="#cb11-47" tabindex="-1"></a>X1_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-48"><a href="#cb11-48" tabindex="-1"></a></span>
<span id="cb11-49"><a href="#cb11-49" tabindex="-1"></a><span class="co"># Size of matrix Z2.</span></span>
<span id="cb11-50"><a href="#cb11-50" tabindex="-1"></a>Z2_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-51"><a href="#cb11-51" tabindex="-1"></a>Z2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-52"><a href="#cb11-52" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" tabindex="-1"></a><span class="co"># Size of LeakyReLU.</span></span>
<span id="cb11-54"><a href="#cb11-54" tabindex="-1"></a>LeakyReLU_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-55"><a href="#cb11-55" tabindex="-1"></a>LeakyReLU_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-56"><a href="#cb11-56" tabindex="-1"></a></span>
<span id="cb11-57"><a href="#cb11-57" tabindex="-1"></a><span class="co"># Size of matrix M2.</span></span>
<span id="cb11-58"><a href="#cb11-58" tabindex="-1"></a>M2_m_rows <span class="ot">&lt;-</span>nu2</span>
<span id="cb11-59"><a href="#cb11-59" tabindex="-1"></a>M2_n_col <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-60"><a href="#cb11-60" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" tabindex="-1"></a><span class="co"># Size of matrix X2</span></span>
<span id="cb11-62"><a href="#cb11-62" tabindex="-1"></a><span class="co"># Add 1 to the number of rows of LeakyReLU to account for the bias.</span></span>
<span id="cb11-63"><a href="#cb11-63" tabindex="-1"></a>X2_m_rows <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-64"><a href="#cb11-64" tabindex="-1"></a>X2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-65"><a href="#cb11-65" tabindex="-1"></a></span>
<span id="cb11-66"><a href="#cb11-66" tabindex="-1"></a><span class="co"># Size of matrix Z3.</span></span>
<span id="cb11-67"><a href="#cb11-67" tabindex="-1"></a>Z3_m_rows <span class="ot">&lt;-</span>nu2</span>
<span id="cb11-68"><a href="#cb11-68" tabindex="-1"></a>Z3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-69"><a href="#cb11-69" tabindex="-1"></a></span>
<span id="cb11-70"><a href="#cb11-70" tabindex="-1"></a><span class="co"># Size of LeakyReLU2.</span></span>
<span id="cb11-71"><a href="#cb11-71" tabindex="-1"></a>LeakyReLU_2_m_rows <span class="ot">&lt;-</span>nu2</span>
<span id="cb11-72"><a href="#cb11-72" tabindex="-1"></a>LeakyReLU_2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-73"><a href="#cb11-73" tabindex="-1"></a></span>
<span id="cb11-74"><a href="#cb11-74" tabindex="-1"></a><span class="co"># Size of matrix M3.</span></span>
<span id="cb11-75"><a href="#cb11-75" tabindex="-1"></a>M3_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-76"><a href="#cb11-76" tabindex="-1"></a>M3_n_col <span class="ot">&lt;-</span>nu2 <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-77"><a href="#cb11-77" tabindex="-1"></a></span>
<span id="cb11-78"><a href="#cb11-78" tabindex="-1"></a><span class="co"># Size of matrix X3</span></span>
<span id="cb11-79"><a href="#cb11-79" tabindex="-1"></a>X3_m_rows <span class="ot">&lt;-</span>nu2 <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-80"><a href="#cb11-80" tabindex="-1"></a>X3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-81"><a href="#cb11-81" tabindex="-1"></a></span>
<span id="cb11-82"><a href="#cb11-82" tabindex="-1"></a><span class="co"># Size of matrix Z4.</span></span>
<span id="cb11-83"><a href="#cb11-83" tabindex="-1"></a>Z4_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-84"><a href="#cb11-84" tabindex="-1"></a>Z4_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-85"><a href="#cb11-85" tabindex="-1"></a></span>
<span id="cb11-86"><a href="#cb11-86" tabindex="-1"></a><span class="co"># Size of Softmax. Where softmax = X4 =X_out</span></span>
<span id="cb11-87"><a href="#cb11-87" tabindex="-1"></a>X4_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-88"><a href="#cb11-88" tabindex="-1"></a>X4_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-89"><a href="#cb11-89" tabindex="-1"></a>}</span></code></pre></div>
<p>Set seed is used to make sure that the same random numbers are
generated<br> each time the code is run.<br></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span></code></pre></div>
<p>Weight matrices.<br></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Define the standard deviation for weight initialization (e.g., 1e-1 for small values)</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>weight_sd <span class="ot">&lt;-</span> <span class="fl">1e-1</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>  <span class="co"># When using One Hidden Layer</span></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>  M1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M1_m_rows <span class="sc">*</span> M1_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M1_m_rows, <span class="at">ncol =</span> M1_n_col)</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>  M2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M2_m_rows <span class="sc">*</span> M2_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M2_m_rows, <span class="at">ncol =</span> M2_n_col)</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>  <span class="co"># When using Two Hidden Layers</span></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>  M1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M1_m_rows <span class="sc">*</span> M1_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M1_m_rows, <span class="at">ncol =</span> M1_n_col)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>  M2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M2_m_rows <span class="sc">*</span> M2_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M2_m_rows, <span class="at">ncol =</span> M2_n_col)</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>  M3 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M3_m_rows <span class="sc">*</span> M3_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M3_m_rows, <span class="at">ncol =</span> M3_n_col)</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="organizing-neural-network-metrics-with-lists-and-dataframes" class="section level3">
<h3>Organizing Neural Network Metrics with Lists and Dataframes</h3>
<p>Create a list to store M matrix for each epoch.<br></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>M1_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>M2_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>M1_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>M2_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>M3_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>}</span></code></pre></div>
<p>Create an empty dataframe to store the mean categorical cross entropy
loss<br> for each epoch.<br></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>CCEntropy_Loss <span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">iteration =</span> <span class="fu">numeric</span>(Epochs), <span class="at">epoch =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_x_out =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_testing_x_out =</span> <span class="fu">numeric</span>(Epochs))</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>CCEntropy_Loss <span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">epoch =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_x_out =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_testing_x_out =</span> <span class="fu">numeric</span>(Epochs))</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>}</span></code></pre></div>
<p>Create an empty dataframe to store training and testing accuracy for
each<br> epoch.<br></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>Accuracy_Percent <span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">Training_percent =</span> <span class="fu">numeric</span>(Epochs), <span class="at">Testing_percent =</span> <span class="fu">numeric</span>(Epochs))</span></code></pre></div>
</div>
</div>
<div id="fourier-feature-mapping-for-gradient-descent" class="section level2">
<h2>Fourier Feature Mapping for Gradient Descent</h2>
<div id="sampling-from-a-gaussian-distribution-with-mean-0-and-variance-sigma2" class="section level3">
<h3>Sampling From a Gaussian Distribution With Mean 0 and Variance
(sigma)^2</h3>
<p>Ramdom sampled points are stored in B matrix.<br></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Row size matrix X(data)</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>Xd_m_rows <span class="ot">&lt;-</span><span class="fu">ncol</span>(Row_Image_Matrix_training)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="co"># B Matrix = Sample from the Gaussian distribution with mean 0 and variance sigma^2</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(Xd_m_rows <span class="sc">*</span> <span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="co"># Hf is harmonic frequencies (1 2 3 4...#number pixels)</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>Hf <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">1</span>, Xd_m_rows), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a><span class="co"># B martrix for training data</span></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a><span class="co"># bind column n_iteration times</span></span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>B_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>Hf_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a><span class="co"># bind column N_training times</span></span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a>B_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb17-22"><a href="#cb17-22" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" tabindex="-1"></a>Hf_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-24"><a href="#cb17-24" tabindex="-1"></a>}</span>
<span id="cb17-25"><a href="#cb17-25" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-27"><a href="#cb17-27" tabindex="-1"></a><span class="co"># B matrix for testing data</span></span>
<span id="cb17-28"><a href="#cb17-28" tabindex="-1"></a>B_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-29"><a href="#cb17-29" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" tabindex="-1"></a><span class="co"># Hf matrix for testing data</span></span>
<span id="cb17-31"><a href="#cb17-31" tabindex="-1"></a>Hf_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-32"><a href="#cb17-32" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-33"><a href="#cb17-33" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-35"><a href="#cb17-35" tabindex="-1"></a><span class="co"># B matrix for data prediction</span></span>
<span id="cb17-36"><a href="#cb17-36" tabindex="-1"></a>B_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-37"><a href="#cb17-37" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" tabindex="-1"></a><span class="co"># Hf matrix for data prediction</span></span>
<span id="cb17-39"><a href="#cb17-39" tabindex="-1"></a>Hf_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-40"><a href="#cb17-40" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-41"><a href="#cb17-41" tabindex="-1"></a></span>
<span id="cb17-42"><a href="#cb17-42" tabindex="-1"></a><span class="co"># Hf*pi*B </span></span>
<span id="cb17-43"><a href="#cb17-43" tabindex="-1"></a>Hf_pi_B_training <span class="ot">&lt;-</span>Hf_training<span class="sc">*</span>pi<span class="sc">*</span>B_training</span>
<span id="cb17-44"><a href="#cb17-44" tabindex="-1"></a>Hf_pi_B_testing <span class="ot">&lt;-</span>Hf_testing<span class="sc">*</span>pi<span class="sc">*</span>B_testing</span>
<span id="cb17-45"><a href="#cb17-45" tabindex="-1"></a>Hf_pi_B_pred <span class="ot">&lt;-</span>Hf_pred<span class="sc">*</span>pi<span class="sc">*</span>B_pred</span></code></pre></div>
</div>
<div id="fourier-feature-mapping-for-batch-gradient-descent-training-data" class="section level3">
<h3>Fourier Feature Mapping for Batch Gradient Descent (Training
Data)</h3>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> N_training) {</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>      X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>      X1_map_vs_no_map <span class="ot">&lt;-</span> X1_training</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>    },</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>    <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>      <span class="co"># Xi = X1_training</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>      <span class="co"># Batch Gradient Descent</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>      X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>      Hf_pi_B_Xi <span class="ot">&lt;-</span> Hf_pi_B_training <span class="sc">*</span> X1_training</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>      <span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>      cos_element_wise <span class="ot">&lt;-</span> <span class="fu">cos</span>(Hf_pi_B_Xi)</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>      <span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>      sin_element_wise <span class="ot">&lt;-</span> <span class="fu">sin</span>(Hf_pi_B_Xi)</span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>      <span class="do">###############################################</span></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>      <span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>      one_zero_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a>      <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a>      one_zero_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>      cos_matrix <span class="ot">&lt;-</span> one_zero_M <span class="sc">*</span> cos_element_wise</span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a>      <span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a>      zero_one_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a>      <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a>      zero_one_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>      sin_matrix <span class="ot">&lt;-</span> zero_one_M <span class="sc">*</span> sin_element_wise</span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a></span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a>      <span class="co"># Fourier Feature Mapping</span></span>
<span id="cb18-39"><a href="#cb18-39" tabindex="-1"></a>      gamma_Xi <span class="ot">&lt;-</span> cos_matrix <span class="sc">+</span> sin_matrix</span>
<span id="cb18-40"><a href="#cb18-40" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" tabindex="-1"></a>      X1_map_vs_no_map <span class="ot">&lt;-</span> gamma_Xi</span>
<span id="cb18-42"><a href="#cb18-42" tabindex="-1"></a>         })</span>
<span id="cb18-43"><a href="#cb18-43" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="fourier-feature-mapping-for-sgd-mini-batch-and-batch-gradient-descent-testing-data" class="section level3">
<h3>Fourier Feature Mapping for SGD, Mini-Batch and Batch Gradient
Descent (Testing Data)</h3>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="co"># Transpose to make each column an observation.</span></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>    X1_testing <span class="ot">&lt;-</span> Row_Image_Matrix_testing <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>    X1_map_vs_no_map_testing <span class="ot">&lt;-</span>X1_testing</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a>       },</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb19-12"><a href="#cb19-12" tabindex="-1"></a>    <span class="co">#Xi = X1_testing</span></span>
<span id="cb19-13"><a href="#cb19-13" tabindex="-1"></a>    X1_testing <span class="ot">&lt;-</span>Row_Image_Matrix_testing <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb19-14"><a href="#cb19-14" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" tabindex="-1"></a>    Hf_pi_B_Xi_testing <span class="ot">&lt;-</span>Hf_pi_B_testing <span class="sc">*</span> X1_testing</span>
<span id="cb19-16"><a href="#cb19-16" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" tabindex="-1"></a>    <span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb19-18"><a href="#cb19-18" tabindex="-1"></a>    cos_element_wise_testing <span class="ot">&lt;-</span><span class="fu">cos</span>(Hf_pi_B_Xi_testing)</span>
<span id="cb19-19"><a href="#cb19-19" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" tabindex="-1"></a>    <span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb19-21"><a href="#cb19-21" tabindex="-1"></a>    sin_element_wise_testing <span class="ot">&lt;-</span><span class="fu">sin</span>(Hf_pi_B_Xi_testing)</span>
<span id="cb19-22"><a href="#cb19-22" tabindex="-1"></a>    <span class="do">###############################################</span></span>
<span id="cb19-23"><a href="#cb19-23" tabindex="-1"></a>    <span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb19-24"><a href="#cb19-24" tabindex="-1"></a>    one_zero_V_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb19-25"><a href="#cb19-25" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" tabindex="-1"></a>    <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb19-27"><a href="#cb19-27" tabindex="-1"></a>    one_zero_M_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V_testing, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb19-28"><a href="#cb19-28" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" tabindex="-1"></a>    cos_matrix_testing <span class="ot">&lt;-</span>one_zero_M_testing <span class="sc">*</span> cos_element_wise_testing</span>
<span id="cb19-30"><a href="#cb19-30" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" tabindex="-1"></a>    <span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb19-32"><a href="#cb19-32" tabindex="-1"></a>    zero_one_V_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb19-33"><a href="#cb19-33" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" tabindex="-1"></a>    <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb19-35"><a href="#cb19-35" tabindex="-1"></a>    zero_one_M_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V_testing, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb19-36"><a href="#cb19-36" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" tabindex="-1"></a>    sin_matrix_testing <span class="ot">&lt;-</span>zero_one_M_testing <span class="sc">*</span> sin_element_wise_testing</span>
<span id="cb19-38"><a href="#cb19-38" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" tabindex="-1"></a>    <span class="co"># Fourier Feature Mapping</span></span>
<span id="cb19-40"><a href="#cb19-40" tabindex="-1"></a>    gamma_Xi_testing <span class="ot">&lt;-</span> cos_matrix_testing <span class="sc">+</span> sin_matrix_testing</span>
<span id="cb19-41"><a href="#cb19-41" tabindex="-1"></a></span>
<span id="cb19-42"><a href="#cb19-42" tabindex="-1"></a>    X1_map_vs_no_map_testing <span class="ot">&lt;-</span>gamma_Xi_testing</span>
<span id="cb19-43"><a href="#cb19-43" tabindex="-1"></a>       })</span></code></pre></div>
</div>
</div>
<div id="putting-it-all-together-neural-network-code" class="section level2">
<h2>Putting It All Together: Neural Network Code</h2>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># if n &lt; N_training, then SGD and mini batch gradient descent</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="co"># if n = N_training, then batch gradient descent</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="co">#12</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="co"># SGD AND MINI BATCH GRADIENT DESCENT</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a><span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs) {</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a><span class="co"># Data Preprocessing for Training Data</span></span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a><span class="co"># Separate training</span></span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a><span class="co"># Randomly select n_iteration rows</span></span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a><span class="co"># replace = TRUE means that the same row can be selected more than once.</span></span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a><span class="co"># replace = FALSE means that the same row cannot be selected more than once.</span></span>
<span id="cb20-19"><a href="#cb20-19" tabindex="-1"></a>Randomly_selected_rows <span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">nrow</span>(Data_training_scaled), n_iteration, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-20"><a href="#cb20-20" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" tabindex="-1"></a><span class="co"># Create a new matrix with the selected rows</span></span>
<span id="cb20-23"><a href="#cb20-23" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-24"><a href="#cb20-24" tabindex="-1"></a><span class="co">#SGD </span></span>
<span id="cb20-25"><a href="#cb20-25" tabindex="-1"></a>Data_training <span class="ot">&lt;-</span>Data_training_scaled[Randomly_selected_rows, ] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-26"><a href="#cb20-26" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-27"><a href="#cb20-27" tabindex="-1"></a><span class="co">#Mini Batch Gradient Descent</span></span>
<span id="cb20-28"><a href="#cb20-28" tabindex="-1"></a>Data_training <span class="ot">&lt;-</span>Data_training_scaled[Randomly_selected_rows, ] </span>
<span id="cb20-29"><a href="#cb20-29" tabindex="-1"></a>}</span>
<span id="cb20-30"><a href="#cb20-30" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-32"><a href="#cb20-32" tabindex="-1"></a><span class="co"># Let&#39;s separate the **y** column and the **image vector columns**.</span></span>
<span id="cb20-33"><a href="#cb20-33" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-34"><a href="#cb20-34" tabindex="-1"></a><span class="co">#Training data</span></span>
<span id="cb20-35"><a href="#cb20-35" tabindex="-1"></a>Y_Labels_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">1</span>]  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-36"><a href="#cb20-36" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_training)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-38"><a href="#cb20-38" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-40"><a href="#cb20-40" tabindex="-1"></a><span class="co">#One Hot Encoding Training Data</span></span>
<span id="cb20-41"><a href="#cb20-41" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-42"><a href="#cb20-42" tabindex="-1"></a><span class="co"># Initialize a list to store the basis vectors</span></span>
<span id="cb20-43"><a href="#cb20-43" tabindex="-1"></a>basis_vectors_list_training <span class="ot">&lt;-</span><span class="fu">list</span>()</span>
<span id="cb20-44"><a href="#cb20-44" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" tabindex="-1"></a><span class="co"># Loop over each column of the matrix</span></span>
<span id="cb20-46"><a href="#cb20-46" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iteration) {</span>
<span id="cb20-47"><a href="#cb20-47" tabindex="-1"></a>  <span class="co"># Get the label for the q-th column</span></span>
<span id="cb20-48"><a href="#cb20-48" tabindex="-1"></a>  label <span class="ot">&lt;-</span> Y_Labels_training[<span class="dv">1</span>, q] <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb20-49"><a href="#cb20-49" tabindex="-1"></a>  </span>
<span id="cb20-50"><a href="#cb20-50" tabindex="-1"></a>  <span class="co"># Create a vector of zeros with length Q</span></span>
<span id="cb20-51"><a href="#cb20-51" tabindex="-1"></a>  basis_vectors_training <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, Q)</span>
<span id="cb20-52"><a href="#cb20-52" tabindex="-1"></a>  </span>
<span id="cb20-53"><a href="#cb20-53" tabindex="-1"></a>  <span class="co"># Set the (label+1)-th element to 1</span></span>
<span id="cb20-54"><a href="#cb20-54" tabindex="-1"></a>  basis_vectors_training[label <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb20-55"><a href="#cb20-55" tabindex="-1"></a>  </span>
<span id="cb20-56"><a href="#cb20-56" tabindex="-1"></a>  <span class="co"># Add the basis vector to the list</span></span>
<span id="cb20-57"><a href="#cb20-57" tabindex="-1"></a>  basis_vectors_list_training[[q]] <span class="ot">&lt;-</span>basis_vectors_training <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-58"><a href="#cb20-58" tabindex="-1"></a>}</span>
<span id="cb20-59"><a href="#cb20-59" tabindex="-1"></a></span>
<span id="cb20-60"><a href="#cb20-60" tabindex="-1"></a><span class="co"># Convert the list of basis vectors to a matrix</span></span>
<span id="cb20-61"><a href="#cb20-61" tabindex="-1"></a>Y_One_Hot_Encoding_training <span class="ot">&lt;-</span> <span class="fu">do.call</span>(cbind, basis_vectors_list_training)</span>
<span id="cb20-62"><a href="#cb20-62" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-63"><a href="#cb20-63" tabindex="-1"></a>  <span class="co"># Code for Training forward forward pass</span></span>
<span id="cb20-64"><a href="#cb20-64" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-65"><a href="#cb20-65" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb20-66"><a href="#cb20-66" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb20-67"><a href="#cb20-67" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb20-68"><a href="#cb20-68" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-69"><a href="#cb20-69" tabindex="-1"></a>X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-70"><a href="#cb20-70" tabindex="-1"></a></span>
<span id="cb20-71"><a href="#cb20-71" tabindex="-1"></a>X1_map_vs_no_map <span class="ot">&lt;-</span>X1_training</span>
<span id="cb20-72"><a href="#cb20-72" tabindex="-1"></a>       },</span>
<span id="cb20-73"><a href="#cb20-73" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-74"><a href="#cb20-74" tabindex="-1"></a><span class="co">#Xi = X1_training</span></span>
<span id="cb20-75"><a href="#cb20-75" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-76"><a href="#cb20-76" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-77"><a href="#cb20-77" tabindex="-1"></a>  X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training</span>
<span id="cb20-78"><a href="#cb20-78" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-79"><a href="#cb20-79" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-80"><a href="#cb20-80" tabindex="-1"></a>  X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-81"><a href="#cb20-81" tabindex="-1"></a>}</span>
<span id="cb20-82"><a href="#cb20-82" tabindex="-1"></a></span>
<span id="cb20-83"><a href="#cb20-83" tabindex="-1"></a>Hf_pi_B_Xi <span class="ot">&lt;-</span>Hf_pi_B_training <span class="sc">*</span> X1_training</span>
<span id="cb20-84"><a href="#cb20-84" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" tabindex="-1"></a><span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb20-86"><a href="#cb20-86" tabindex="-1"></a>cos_element_wise <span class="ot">&lt;-</span><span class="fu">cos</span>(Hf_pi_B_Xi)</span>
<span id="cb20-87"><a href="#cb20-87" tabindex="-1"></a></span>
<span id="cb20-88"><a href="#cb20-88" tabindex="-1"></a><span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb20-89"><a href="#cb20-89" tabindex="-1"></a>sin_element_wise <span class="ot">&lt;-</span><span class="fu">sin</span>(Hf_pi_B_Xi)</span>
<span id="cb20-90"><a href="#cb20-90" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-91"><a href="#cb20-91" tabindex="-1"></a><span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb20-92"><a href="#cb20-92" tabindex="-1"></a>one_zero_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb20-93"><a href="#cb20-93" tabindex="-1"></a></span>
<span id="cb20-94"><a href="#cb20-94" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb20-95"><a href="#cb20-95" tabindex="-1"></a>one_zero_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb20-96"><a href="#cb20-96" tabindex="-1"></a></span>
<span id="cb20-97"><a href="#cb20-97" tabindex="-1"></a>cos_matrix <span class="ot">&lt;-</span>one_zero_M <span class="sc">*</span> cos_element_wise</span>
<span id="cb20-98"><a href="#cb20-98" tabindex="-1"></a></span>
<span id="cb20-99"><a href="#cb20-99" tabindex="-1"></a><span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb20-100"><a href="#cb20-100" tabindex="-1"></a>zero_one_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-101"><a href="#cb20-101" tabindex="-1"></a></span>
<span id="cb20-102"><a href="#cb20-102" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb20-103"><a href="#cb20-103" tabindex="-1"></a>zero_one_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-104"><a href="#cb20-104" tabindex="-1"></a></span>
<span id="cb20-105"><a href="#cb20-105" tabindex="-1"></a>sin_matrix <span class="ot">&lt;-</span>zero_one_M <span class="sc">*</span> sin_element_wise</span>
<span id="cb20-106"><a href="#cb20-106" tabindex="-1"></a></span>
<span id="cb20-107"><a href="#cb20-107" tabindex="-1"></a><span class="co"># Fourier Feature Mapping</span></span>
<span id="cb20-108"><a href="#cb20-108" tabindex="-1"></a>gamma_Xi <span class="ot">&lt;-</span> cos_matrix <span class="sc">+</span> sin_matrix</span>
<span id="cb20-109"><a href="#cb20-109" tabindex="-1"></a></span>
<span id="cb20-110"><a href="#cb20-110" tabindex="-1"></a>X1_map_vs_no_map <span class="ot">&lt;-</span>gamma_Xi</span>
<span id="cb20-111"><a href="#cb20-111" tabindex="-1"></a>       })</span>
<span id="cb20-112"><a href="#cb20-112" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb20-113"><a href="#cb20-113" tabindex="-1"></a><span class="co"># Add bias row. Row of 1&#39;s</span></span>
<span id="cb20-114"><a href="#cb20-114" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-115"><a href="#cb20-115" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-116"><a href="#cb20-116" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb20-117"><a href="#cb20-117" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-118"><a href="#cb20-118" tabindex="-1"></a>  X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(<span class="fu">t</span>(X1_map_vs_no_map), <span class="fu">c</span>(<span class="dv">1</span>))</span>
<span id="cb20-119"><a href="#cb20-119" tabindex="-1"></a>       },</span>
<span id="cb20-120"><a href="#cb20-120" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-121"><a href="#cb20-121" tabindex="-1"></a>  X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map)))</span>
<span id="cb20-122"><a href="#cb20-122" tabindex="-1"></a>      })</span>
<span id="cb20-123"><a href="#cb20-123" tabindex="-1"></a></span>
<span id="cb20-124"><a href="#cb20-124" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-125"><a href="#cb20-125" tabindex="-1"></a></span>
<span id="cb20-126"><a href="#cb20-126" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-127"><a href="#cb20-127" tabindex="-1"></a>  X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map)))</span>
<span id="cb20-128"><a href="#cb20-128" tabindex="-1"></a>}</span>
<span id="cb20-129"><a href="#cb20-129" tabindex="-1"></a></span>
<span id="cb20-130"><a href="#cb20-130" tabindex="-1"></a>Z2 <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1</span>
<span id="cb20-131"><a href="#cb20-131" tabindex="-1"></a></span>
<span id="cb20-132"><a href="#cb20-132" tabindex="-1"></a><span class="co"># LeakyReLU activation function</span></span>
<span id="cb20-133"><a href="#cb20-133" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-134"><a href="#cb20-134" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-135"><a href="#cb20-135" tabindex="-1"></a>  LeakyReLU <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x)) <span class="sc">%&gt;%</span> </span>
<span id="cb20-136"><a href="#cb20-136" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-137"><a href="#cb20-137" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-138"><a href="#cb20-138" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-139"><a href="#cb20-139" tabindex="-1"></a>  LeakyReLU <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-140"><a href="#cb20-140" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-141"><a href="#cb20-141" tabindex="-1"></a>}</span>
<span id="cb20-142"><a href="#cb20-142" tabindex="-1"></a></span>
<span id="cb20-143"><a href="#cb20-143" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-144"><a href="#cb20-144" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-145"><a href="#cb20-145" tabindex="-1"></a></span>
<span id="cb20-146"><a href="#cb20-146" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-147"><a href="#cb20-147" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-148"><a href="#cb20-148" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-149"><a href="#cb20-149" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-150"><a href="#cb20-150" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-151"><a href="#cb20-151" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-152"><a href="#cb20-152" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-153"><a href="#cb20-153" tabindex="-1"></a></span>
<span id="cb20-154"><a href="#cb20-154" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-155"><a href="#cb20-155" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-156"><a href="#cb20-156" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-157"><a href="#cb20-157" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-158"><a href="#cb20-158" tabindex="-1"></a>Z3 <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-159"><a href="#cb20-159" tabindex="-1"></a></span>
<span id="cb20-160"><a href="#cb20-160" tabindex="-1"></a><span class="co"># LeakyReLU activation function</span></span>
<span id="cb20-161"><a href="#cb20-161" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-162"><a href="#cb20-162" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-163"><a href="#cb20-163" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x)) <span class="sc">%&gt;%</span> </span>
<span id="cb20-164"><a href="#cb20-164" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-165"><a href="#cb20-165" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-166"><a href="#cb20-166" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-167"><a href="#cb20-167" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-168"><a href="#cb20-168" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-169"><a href="#cb20-169" tabindex="-1"></a>}</span>
<span id="cb20-170"><a href="#cb20-170" tabindex="-1"></a></span>
<span id="cb20-171"><a href="#cb20-171" tabindex="-1"></a>X3 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-172"><a href="#cb20-172" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-173"><a href="#cb20-173" tabindex="-1"></a></span>
<span id="cb20-174"><a href="#cb20-174" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3</span>
<span id="cb20-175"><a href="#cb20-175" tabindex="-1"></a>}</span>
<span id="cb20-176"><a href="#cb20-176" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-177"><a href="#cb20-177" tabindex="-1"></a><span class="co"># Define Softmax(Max)</span></span>
<span id="cb20-178"><a href="#cb20-178" tabindex="-1"></a>softmax_MAX <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb20-179"><a href="#cb20-179" tabindex="-1"></a>  <span class="co"># zi-max(z)</span></span>
<span id="cb20-180"><a href="#cb20-180" tabindex="-1"></a>  zi_max <span class="ot">&lt;-</span>z <span class="sc">-</span> <span class="fu">max</span>(z)</span>
<span id="cb20-181"><a href="#cb20-181" tabindex="-1"></a>  <span class="co"># e^(zi-max(z))</span></span>
<span id="cb20-182"><a href="#cb20-182" tabindex="-1"></a>  exp_zi_max <span class="ot">&lt;-</span><span class="fu">exp</span>(zi_max)</span>
<span id="cb20-183"><a href="#cb20-183" tabindex="-1"></a>  <span class="co"># e^(zi-max(z)-log(sum(e^(zj-max(z)))</span></span>
<span id="cb20-184"><a href="#cb20-184" tabindex="-1"></a>  <span class="fu">exp</span>(zi_max <span class="sc">-</span> <span class="fu">log</span>(<span class="fu">sum</span>(exp_zi_max)))</span>
<span id="cb20-185"><a href="#cb20-185" tabindex="-1"></a>}</span>
<span id="cb20-186"><a href="#cb20-186" tabindex="-1"></a></span>
<span id="cb20-187"><a href="#cb20-187" tabindex="-1"></a><span class="co"># note that:</span></span>
<span id="cb20-188"><a href="#cb20-188" tabindex="-1"></a><span class="co"># apply(Z_out, 1 ----means its row wise</span></span>
<span id="cb20-189"><a href="#cb20-189" tabindex="-1"></a><span class="co"># apply(Z_out, 2 ----means its column wise</span></span>
<span id="cb20-190"><a href="#cb20-190" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-191"><a href="#cb20-191" tabindex="-1"></a>X_out <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-192"><a href="#cb20-192" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-193"><a href="#cb20-193" tabindex="-1"></a><span class="co"># Code for Testing forward forward pass</span></span>
<span id="cb20-194"><a href="#cb20-194" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-195"><a href="#cb20-195" tabindex="-1"></a><span class="co"># Add bias row. Row of 1&#39;s</span></span>
<span id="cb20-196"><a href="#cb20-196" tabindex="-1"></a>X1_test <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map_testing, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map_testing)))</span>
<span id="cb20-197"><a href="#cb20-197" tabindex="-1"></a></span>
<span id="cb20-198"><a href="#cb20-198" tabindex="-1"></a>Z2_testing <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1_test</span>
<span id="cb20-199"><a href="#cb20-199" tabindex="-1"></a></span>
<span id="cb20-200"><a href="#cb20-200" tabindex="-1"></a>LeakyReLU_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-201"><a href="#cb20-201" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-202"><a href="#cb20-202" tabindex="-1"></a></span>
<span id="cb20-203"><a href="#cb20-203" tabindex="-1"></a>X2_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-204"><a href="#cb20-204" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-205"><a href="#cb20-205" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-206"><a href="#cb20-206" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-207"><a href="#cb20-207" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-208"><a href="#cb20-208" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-209"><a href="#cb20-209" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-210"><a href="#cb20-210" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-211"><a href="#cb20-211" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-212"><a href="#cb20-212" tabindex="-1"></a></span>
<span id="cb20-213"><a href="#cb20-213" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-214"><a href="#cb20-214" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-215"><a href="#cb20-215" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-216"><a href="#cb20-216" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-217"><a href="#cb20-217" tabindex="-1"></a>Z3_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-218"><a href="#cb20-218" tabindex="-1"></a></span>
<span id="cb20-219"><a href="#cb20-219" tabindex="-1"></a>LeakyReLU2_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-220"><a href="#cb20-220" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-221"><a href="#cb20-221" tabindex="-1"></a></span>
<span id="cb20-222"><a href="#cb20-222" tabindex="-1"></a>X3_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-223"><a href="#cb20-223" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-224"><a href="#cb20-224" tabindex="-1"></a></span>
<span id="cb20-225"><a href="#cb20-225" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3_testing</span>
<span id="cb20-226"><a href="#cb20-226" tabindex="-1"></a>}</span>
<span id="cb20-227"><a href="#cb20-227" tabindex="-1"></a></span>
<span id="cb20-228"><a href="#cb20-228" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-229"><a href="#cb20-229" tabindex="-1"></a>X_out_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out_testing, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-230"><a href="#cb20-230" tabindex="-1"></a></span>
<span id="cb20-231"><a href="#cb20-231" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-232"><a href="#cb20-232" tabindex="-1"></a>  <span class="co"># Code for backpropagation training</span></span>
<span id="cb20-233"><a href="#cb20-233" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-234"><a href="#cb20-234" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-235"><a href="#cb20-235" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-236"><a href="#cb20-236" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-237"><a href="#cb20-237" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-238"><a href="#cb20-238" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-239"><a href="#cb20-239" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-240"><a href="#cb20-240" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-241"><a href="#cb20-241" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t <span class="ot">&lt;-</span> (X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-242"><a href="#cb20-242" tabindex="-1"></a></span>
<span id="cb20-243"><a href="#cb20-243" tabindex="-1"></a>  dZ3_out_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-244"><a href="#cb20-244" tabindex="-1"></a></span>
<span id="cb20-245"><a href="#cb20-245" tabindex="-1"></a>  <span class="co"># dC/dM2 = [(dC/dX_out)^t *(dX_out/dZ3_out)^t]^t * dZ3_out/dM2</span></span>
<span id="cb20-246"><a href="#cb20-246" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span> dc_dX_out_t_times_dX_out_dZ3_out_t <span class="sc">%*%</span> dZ3_out_dM2 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-247"><a href="#cb20-247" tabindex="-1"></a></span>
<span id="cb20-248"><a href="#cb20-248" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-249"><a href="#cb20-249" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-250"><a href="#cb20-250" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-251"><a href="#cb20-251" tabindex="-1"></a>  <span class="co"># (dC/dX_out)^t *(dX_out/dZ3_out)^t</span></span>
<span id="cb20-252"><a href="#cb20-252" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t)</span>
<span id="cb20-253"><a href="#cb20-253" tabindex="-1"></a></span>
<span id="cb20-254"><a href="#cb20-254" tabindex="-1"></a>  dZ3_out_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-255"><a href="#cb20-255" tabindex="-1"></a></span>
<span id="cb20-256"><a href="#cb20-256" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-257"><a href="#cb20-257" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-258"><a href="#cb20-258" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-259"><a href="#cb20-259" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-260"><a href="#cb20-260" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-261"><a href="#cb20-261" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-262"><a href="#cb20-262" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-263"><a href="#cb20-263" tabindex="-1"></a>  }</span>
<span id="cb20-264"><a href="#cb20-264" tabindex="-1"></a></span>
<span id="cb20-265"><a href="#cb20-265" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-266"><a href="#cb20-266" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-267"><a href="#cb20-267" tabindex="-1"></a></span>
<span id="cb20-268"><a href="#cb20-268" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-269"><a href="#cb20-269" tabindex="-1"></a></span>
<span id="cb20-270"><a href="#cb20-270" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="sc">%*%</span> dZ3_out_dX2)) <span class="sc">*</span> dX2_dZ2) <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-271"><a href="#cb20-271" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-272"><a href="#cb20-272" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-273"><a href="#cb20-273" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-274"><a href="#cb20-274" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-275"><a href="#cb20-275" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-276"><a href="#cb20-276" tabindex="-1"></a>  <span class="co"># DC/DM3</span></span>
<span id="cb20-277"><a href="#cb20-277" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-278"><a href="#cb20-278" tabindex="-1"></a>  dc_dZ_out_t <span class="ot">&lt;-</span>(X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-279"><a href="#cb20-279" tabindex="-1"></a></span>
<span id="cb20-280"><a href="#cb20-280" tabindex="-1"></a>  dZ_out_dM3 <span class="ot">&lt;-</span><span class="fu">t</span>(X3)</span>
<span id="cb20-281"><a href="#cb20-281" tabindex="-1"></a></span>
<span id="cb20-282"><a href="#cb20-282" tabindex="-1"></a>  dC_dM3 <span class="ot">&lt;-</span>dc_dZ_out_t <span class="sc">%*%</span> dZ_out_dM3 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-283"><a href="#cb20-283" tabindex="-1"></a></span>
<span id="cb20-284"><a href="#cb20-284" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-285"><a href="#cb20-285" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-286"><a href="#cb20-286" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-287"><a href="#cb20-287" tabindex="-1"></a>  dc_dZ_out <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dZ_out_t)</span>
<span id="cb20-288"><a href="#cb20-288" tabindex="-1"></a></span>
<span id="cb20-289"><a href="#cb20-289" tabindex="-1"></a>  dZ4_dX3 <span class="ot">&lt;-</span> M3[, <span class="sc">-</span><span class="fu">ncol</span>(M3)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-290"><a href="#cb20-290" tabindex="-1"></a></span>
<span id="cb20-291"><a href="#cb20-291" tabindex="-1"></a>  <span class="co"># dX3/dZ3 (element wise)</span></span>
<span id="cb20-292"><a href="#cb20-292" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-293"><a href="#cb20-293" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-294"><a href="#cb20-294" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-295"><a href="#cb20-295" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-296"><a href="#cb20-296" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-297"><a href="#cb20-297" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-298"><a href="#cb20-298" tabindex="-1"></a>  }</span>
<span id="cb20-299"><a href="#cb20-299" tabindex="-1"></a></span>
<span id="cb20-300"><a href="#cb20-300" tabindex="-1"></a>  dX3_dZ3 <span class="ot">&lt;-</span> dX3_dZ3_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX3_dZ3_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-301"><a href="#cb20-301" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-302"><a href="#cb20-302" tabindex="-1"></a></span>
<span id="cb20-303"><a href="#cb20-303" tabindex="-1"></a>  dZ3_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-304"><a href="#cb20-304" tabindex="-1"></a></span>
<span id="cb20-305"><a href="#cb20-305" tabindex="-1"></a>  dC_dZ3 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dc_dZ_out <span class="sc">%*%</span> dZ4_dX3)) <span class="sc">*</span> dX3_dZ3</span>
<span id="cb20-306"><a href="#cb20-306" tabindex="-1"></a></span>
<span id="cb20-307"><a href="#cb20-307" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span>dC_dZ3 <span class="sc">%*%</span> dZ3_dM2</span>
<span id="cb20-308"><a href="#cb20-308" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-309"><a href="#cb20-309" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-310"><a href="#cb20-310" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-311"><a href="#cb20-311" tabindex="-1"></a>  dC_dZ3_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dC_dZ3)</span>
<span id="cb20-312"><a href="#cb20-312" tabindex="-1"></a></span>
<span id="cb20-313"><a href="#cb20-313" tabindex="-1"></a>  dZ3_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-314"><a href="#cb20-314" tabindex="-1"></a></span>
<span id="cb20-315"><a href="#cb20-315" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-316"><a href="#cb20-316" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-317"><a href="#cb20-317" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-318"><a href="#cb20-318" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-319"><a href="#cb20-319" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-320"><a href="#cb20-320" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-321"><a href="#cb20-321" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-322"><a href="#cb20-322" tabindex="-1"></a>  }</span>
<span id="cb20-323"><a href="#cb20-323" tabindex="-1"></a></span>
<span id="cb20-324"><a href="#cb20-324" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-325"><a href="#cb20-325" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-326"><a href="#cb20-326" tabindex="-1"></a></span>
<span id="cb20-327"><a href="#cb20-327" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-328"><a href="#cb20-328" tabindex="-1"></a></span>
<span id="cb20-329"><a href="#cb20-329" tabindex="-1"></a>  dC_dZ2 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dC_dZ3_t <span class="sc">%*%</span> dZ3_dX2)) <span class="sc">*</span> dX2_dZ2</span>
<span id="cb20-330"><a href="#cb20-330" tabindex="-1"></a></span>
<span id="cb20-331"><a href="#cb20-331" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span>dC_dZ2 <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-332"><a href="#cb20-332" tabindex="-1"></a>}</span>
<span id="cb20-333"><a href="#cb20-333" tabindex="-1"></a></span>
<span id="cb20-334"><a href="#cb20-334" tabindex="-1"></a><span class="do">##############################################################</span></span>
<span id="cb20-335"><a href="#cb20-335" tabindex="-1"></a>  <span class="co"># Gradient Descent</span></span>
<span id="cb20-336"><a href="#cb20-336" tabindex="-1"></a><span class="do">#############################################################</span></span>
<span id="cb20-337"><a href="#cb20-337" tabindex="-1"></a><span class="co"># note that in SGD and mini-batch, Epochs = Epoch*number_iterations_epoch</span></span>
<span id="cb20-338"><a href="#cb20-338" tabindex="-1"></a><span class="co"># Update weights and biases (based on backpropagation)</span></span>
<span id="cb20-339"><a href="#cb20-339" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-340"><a href="#cb20-340" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-341"><a href="#cb20-341" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-342"><a href="#cb20-342" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-343"><a href="#cb20-343" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-344"><a href="#cb20-344" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-345"><a href="#cb20-345" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-346"><a href="#cb20-346" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-347"><a href="#cb20-347" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-348"><a href="#cb20-348" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-349"><a href="#cb20-349" tabindex="-1"></a></span>
<span id="cb20-350"><a href="#cb20-350" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-351"><a href="#cb20-351" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-352"><a href="#cb20-352" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-353"><a href="#cb20-353" tabindex="-1"></a>}</span>
<span id="cb20-354"><a href="#cb20-354" tabindex="-1"></a>       },</span>
<span id="cb20-355"><a href="#cb20-355" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-356"><a href="#cb20-356" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-357"><a href="#cb20-357" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-358"><a href="#cb20-358" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-359"><a href="#cb20-359" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-360"><a href="#cb20-360" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-361"><a href="#cb20-361" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-362"><a href="#cb20-362" tabindex="-1"></a></span>
<span id="cb20-363"><a href="#cb20-363" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-364"><a href="#cb20-364" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-365"><a href="#cb20-365" tabindex="-1"></a>}</span>
<span id="cb20-366"><a href="#cb20-366" tabindex="-1"></a>       },</span>
<span id="cb20-367"><a href="#cb20-367" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-368"><a href="#cb20-368" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-369"><a href="#cb20-369" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-370"><a href="#cb20-370" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-371"><a href="#cb20-371" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-372"><a href="#cb20-372" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-373"><a href="#cb20-373" tabindex="-1"></a><span class="co"># M2 </span></span>
<span id="cb20-374"><a href="#cb20-374" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-375"><a href="#cb20-375" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-376"><a href="#cb20-376" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-377"><a href="#cb20-377" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-378"><a href="#cb20-378" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-379"><a href="#cb20-379" tabindex="-1"></a></span>
<span id="cb20-380"><a href="#cb20-380" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-381"><a href="#cb20-381" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-382"><a href="#cb20-382" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-383"><a href="#cb20-383" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-384"><a href="#cb20-384" tabindex="-1"></a></span>
<span id="cb20-385"><a href="#cb20-385" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-386"><a href="#cb20-386" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-387"><a href="#cb20-387" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-388"><a href="#cb20-388" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-389"><a href="#cb20-389" tabindex="-1"></a>}</span>
<span id="cb20-390"><a href="#cb20-390" tabindex="-1"></a></span>
<span id="cb20-391"><a href="#cb20-391" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-392"><a href="#cb20-392" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-393"><a href="#cb20-393" tabindex="-1"></a></span>
<span id="cb20-394"><a href="#cb20-394" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-395"><a href="#cb20-395" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-396"><a href="#cb20-396" tabindex="-1"></a></span>
<span id="cb20-397"><a href="#cb20-397" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-398"><a href="#cb20-398" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-399"><a href="#cb20-399" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-400"><a href="#cb20-400" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-401"><a href="#cb20-401" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-402"><a href="#cb20-402" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-403"><a href="#cb20-403" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-404"><a href="#cb20-404" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1 </span>
<span id="cb20-405"><a href="#cb20-405" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-406"><a href="#cb20-406" tabindex="-1"></a></span>
<span id="cb20-407"><a href="#cb20-407" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-408"><a href="#cb20-408" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-409"><a href="#cb20-409" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-410"><a href="#cb20-410" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-411"><a href="#cb20-411" tabindex="-1"></a></span>
<span id="cb20-412"><a href="#cb20-412" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-413"><a href="#cb20-413" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-414"><a href="#cb20-414" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-415"><a href="#cb20-415" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-416"><a href="#cb20-416" tabindex="-1"></a>}</span>
<span id="cb20-417"><a href="#cb20-417" tabindex="-1"></a></span>
<span id="cb20-418"><a href="#cb20-418" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-419"><a href="#cb20-419" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-420"><a href="#cb20-420" tabindex="-1"></a></span>
<span id="cb20-421"><a href="#cb20-421" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-422"><a href="#cb20-422" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-423"><a href="#cb20-423" tabindex="-1"></a></span>
<span id="cb20-424"><a href="#cb20-424" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-425"><a href="#cb20-425" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-426"><a href="#cb20-426" tabindex="-1"></a></span>
<span id="cb20-427"><a href="#cb20-427" tabindex="-1"></a>}</span>
<span id="cb20-428"><a href="#cb20-428" tabindex="-1"></a>       }</span>
<span id="cb20-429"><a href="#cb20-429" tabindex="-1"></a>)</span>
<span id="cb20-430"><a href="#cb20-430" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-431"><a href="#cb20-431" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-432"><a href="#cb20-432" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-433"><a href="#cb20-433" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-434"><a href="#cb20-434" tabindex="-1"></a></span>
<span id="cb20-435"><a href="#cb20-435" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-436"><a href="#cb20-436" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-437"><a href="#cb20-437" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-438"><a href="#cb20-438" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-439"><a href="#cb20-439" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-440"><a href="#cb20-440" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-441"><a href="#cb20-441" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-442"><a href="#cb20-442" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-443"><a href="#cb20-443" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-444"><a href="#cb20-444" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-445"><a href="#cb20-445" tabindex="-1"></a></span>
<span id="cb20-446"><a href="#cb20-446" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-447"><a href="#cb20-447" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-448"><a href="#cb20-448" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-449"><a href="#cb20-449" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-450"><a href="#cb20-450" tabindex="-1"></a>}</span>
<span id="cb20-451"><a href="#cb20-451" tabindex="-1"></a>       },</span>
<span id="cb20-452"><a href="#cb20-452" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-453"><a href="#cb20-453" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-454"><a href="#cb20-454" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-455"><a href="#cb20-455" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-456"><a href="#cb20-456" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-457"><a href="#cb20-457" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-458"><a href="#cb20-458" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-459"><a href="#cb20-459" tabindex="-1"></a></span>
<span id="cb20-460"><a href="#cb20-460" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-461"><a href="#cb20-461" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-462"><a href="#cb20-462" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-463"><a href="#cb20-463" tabindex="-1"></a>}</span>
<span id="cb20-464"><a href="#cb20-464" tabindex="-1"></a>       },</span>
<span id="cb20-465"><a href="#cb20-465" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-466"><a href="#cb20-466" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-467"><a href="#cb20-467" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-468"><a href="#cb20-468" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-469"><a href="#cb20-469" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-470"><a href="#cb20-470" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-471"><a href="#cb20-471" tabindex="-1"></a><span class="co"># M3</span></span>
<span id="cb20-472"><a href="#cb20-472" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-473"><a href="#cb20-473" tabindex="-1"></a></span>
<span id="cb20-474"><a href="#cb20-474" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-475"><a href="#cb20-475" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-476"><a href="#cb20-476" tabindex="-1"></a>v1_prev_M3 <span class="ot">&lt;-</span> v1_M3</span>
<span id="cb20-477"><a href="#cb20-477" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-478"><a href="#cb20-478" tabindex="-1"></a></span>
<span id="cb20-479"><a href="#cb20-479" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-480"><a href="#cb20-480" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-481"><a href="#cb20-481" tabindex="-1"></a>v2_prev_M3 <span class="ot">&lt;-</span> v2_M3</span>
<span id="cb20-482"><a href="#cb20-482" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM3<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-483"><a href="#cb20-483" tabindex="-1"></a></span>
<span id="cb20-484"><a href="#cb20-484" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-485"><a href="#cb20-485" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-486"><a href="#cb20-486" tabindex="-1"></a>  v1_M3 <span class="ot">&lt;-</span> v1_prev_M3</span>
<span id="cb20-487"><a href="#cb20-487" tabindex="-1"></a>  v2_M3 <span class="ot">&lt;-</span> v2_prev_M3</span>
<span id="cb20-488"><a href="#cb20-488" tabindex="-1"></a>}</span>
<span id="cb20-489"><a href="#cb20-489" tabindex="-1"></a></span>
<span id="cb20-490"><a href="#cb20-490" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-491"><a href="#cb20-491" tabindex="-1"></a>v1_hat_M3 <span class="ot">&lt;-</span> v1_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-492"><a href="#cb20-492" tabindex="-1"></a></span>
<span id="cb20-493"><a href="#cb20-493" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-494"><a href="#cb20-494" tabindex="-1"></a>v2_hat_M3 <span class="ot">&lt;-</span> v2_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-495"><a href="#cb20-495" tabindex="-1"></a></span>
<span id="cb20-496"><a href="#cb20-496" tabindex="-1"></a><span class="co"># Update M3 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-497"><a href="#cb20-497" tabindex="-1"></a>M3 <span class="ot">&lt;-</span> M3 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M3 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M3) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-498"><a href="#cb20-498" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-499"><a href="#cb20-499" tabindex="-1"></a><span class="co"># M2</span></span>
<span id="cb20-500"><a href="#cb20-500" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-501"><a href="#cb20-501" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-502"><a href="#cb20-502" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-503"><a href="#cb20-503" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-504"><a href="#cb20-504" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-505"><a href="#cb20-505" tabindex="-1"></a></span>
<span id="cb20-506"><a href="#cb20-506" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-507"><a href="#cb20-507" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-508"><a href="#cb20-508" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-509"><a href="#cb20-509" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-510"><a href="#cb20-510" tabindex="-1"></a></span>
<span id="cb20-511"><a href="#cb20-511" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-512"><a href="#cb20-512" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-513"><a href="#cb20-513" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-514"><a href="#cb20-514" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-515"><a href="#cb20-515" tabindex="-1"></a>}</span>
<span id="cb20-516"><a href="#cb20-516" tabindex="-1"></a></span>
<span id="cb20-517"><a href="#cb20-517" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-518"><a href="#cb20-518" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-519"><a href="#cb20-519" tabindex="-1"></a></span>
<span id="cb20-520"><a href="#cb20-520" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-521"><a href="#cb20-521" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-522"><a href="#cb20-522" tabindex="-1"></a></span>
<span id="cb20-523"><a href="#cb20-523" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-524"><a href="#cb20-524" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-525"><a href="#cb20-525" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-526"><a href="#cb20-526" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-527"><a href="#cb20-527" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-528"><a href="#cb20-528" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-529"><a href="#cb20-529" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-530"><a href="#cb20-530" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1</span>
<span id="cb20-531"><a href="#cb20-531" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-532"><a href="#cb20-532" tabindex="-1"></a></span>
<span id="cb20-533"><a href="#cb20-533" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-534"><a href="#cb20-534" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-535"><a href="#cb20-535" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-536"><a href="#cb20-536" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-537"><a href="#cb20-537" tabindex="-1"></a></span>
<span id="cb20-538"><a href="#cb20-538" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-539"><a href="#cb20-539" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-540"><a href="#cb20-540" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-541"><a href="#cb20-541" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-542"><a href="#cb20-542" tabindex="-1"></a>}</span>
<span id="cb20-543"><a href="#cb20-543" tabindex="-1"></a></span>
<span id="cb20-544"><a href="#cb20-544" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-545"><a href="#cb20-545" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-546"><a href="#cb20-546" tabindex="-1"></a></span>
<span id="cb20-547"><a href="#cb20-547" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-548"><a href="#cb20-548" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-549"><a href="#cb20-549" tabindex="-1"></a></span>
<span id="cb20-550"><a href="#cb20-550" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-551"><a href="#cb20-551" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-552"><a href="#cb20-552" tabindex="-1"></a></span>
<span id="cb20-553"><a href="#cb20-553" tabindex="-1"></a>}</span>
<span id="cb20-554"><a href="#cb20-554" tabindex="-1"></a>       }</span>
<span id="cb20-555"><a href="#cb20-555" tabindex="-1"></a>)</span>
<span id="cb20-556"><a href="#cb20-556" tabindex="-1"></a></span>
<span id="cb20-557"><a href="#cb20-557" tabindex="-1"></a>  <span class="co">#Store M3 for each epoch</span></span>
<span id="cb20-558"><a href="#cb20-558" tabindex="-1"></a>M3_list[[epoch]] <span class="ot">&lt;-</span>M3</span>
<span id="cb20-559"><a href="#cb20-559" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-560"><a href="#cb20-560" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-561"><a href="#cb20-561" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-562"><a href="#cb20-562" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-563"><a href="#cb20-563" tabindex="-1"></a></span>
<span id="cb20-564"><a href="#cb20-564" tabindex="-1"></a>}</span>
<span id="cb20-565"><a href="#cb20-565" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-566"><a href="#cb20-566" tabindex="-1"></a>  <span class="co"># Compute Mean Categorical Cross Entropy Loss and store it in the CCEntropy_Loss dataframe</span></span>
<span id="cb20-567"><a href="#cb20-567" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-568"><a href="#cb20-568" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-569"><a href="#cb20-569" tabindex="-1"></a>  <span class="co"># Mean Categorical Cross Entropy Loss Training</span></span>
<span id="cb20-570"><a href="#cb20-570" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-571"><a href="#cb20-571" tabindex="-1"></a>  <span class="co">#Categorical Cross Entropy Loss</span></span>
<span id="cb20-572"><a href="#cb20-572" tabindex="-1"></a>  Cost_training <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_training <span class="sc">*</span> <span class="fu">log</span>(X_out) </span>
<span id="cb20-573"><a href="#cb20-573" tabindex="-1"></a></span>
<span id="cb20-574"><a href="#cb20-574" tabindex="-1"></a>  <span class="co"># Sums up the CCE Loss for each observation</span></span>
<span id="cb20-575"><a href="#cb20-575" tabindex="-1"></a>  Cost_training <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_training)</span>
<span id="cb20-576"><a href="#cb20-576" tabindex="-1"></a></span>
<span id="cb20-577"><a href="#cb20-577" tabindex="-1"></a>  <span class="co">#Stores mean CCE Loss X_out for each epoch</span></span>
<span id="cb20-578"><a href="#cb20-578" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch] <span class="ot">&lt;-</span><span class="fu">round</span>(<span class="fu">mean</span>(Cost_training), <span class="dv">2</span>)</span>
<span id="cb20-579"><a href="#cb20-579" tabindex="-1"></a></span>
<span id="cb20-580"><a href="#cb20-580" tabindex="-1"></a>  <span class="co">#Stores iteration number(it says [epoch] but its actually iteration number)</span></span>
<span id="cb20-581"><a href="#cb20-581" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>iteration[epoch] <span class="ot">&lt;-</span>epoch</span>
<span id="cb20-582"><a href="#cb20-582" tabindex="-1"></a></span>
<span id="cb20-583"><a href="#cb20-583" tabindex="-1"></a>  <span class="co">#Stores epoch number</span></span>
<span id="cb20-584"><a href="#cb20-584" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>epoch[epoch] <span class="ot">&lt;-</span><span class="fu">round</span>(CCEntropy_Loss<span class="sc">$</span>iteration[epoch]<span class="sc">/</span>number_iterations_epoch, <span class="dv">2</span>)</span>
<span id="cb20-585"><a href="#cb20-585" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-586"><a href="#cb20-586" tabindex="-1"></a>  <span class="co"># Mean Categorical Cross Entropy Loss Testing</span></span>
<span id="cb20-587"><a href="#cb20-587" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-588"><a href="#cb20-588" tabindex="-1"></a>  <span class="co">#Categorical Cross Entropy Loss</span></span>
<span id="cb20-589"><a href="#cb20-589" tabindex="-1"></a>  Cost_testing <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_testing <span class="sc">*</span> <span class="fu">log</span>(X_out_testing)  </span>
<span id="cb20-590"><a href="#cb20-590" tabindex="-1"></a></span>
<span id="cb20-591"><a href="#cb20-591" tabindex="-1"></a>  <span class="co"># Sums up the CCE Loss for each observation  </span></span>
<span id="cb20-592"><a href="#cb20-592" tabindex="-1"></a>  Cost_testing <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_testing)</span>
<span id="cb20-593"><a href="#cb20-593" tabindex="-1"></a></span>
<span id="cb20-594"><a href="#cb20-594" tabindex="-1"></a>  <span class="co">#Stores mean CCE Loss X_out (testing) for each epoch</span></span>
<span id="cb20-595"><a href="#cb20-595" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch] <span class="ot">&lt;-</span><span class="fu">round</span>(<span class="fu">mean</span>(Cost_testing), <span class="dv">2</span>)</span>
<span id="cb20-596"><a href="#cb20-596" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-597"><a href="#cb20-597" tabindex="-1"></a>  <span class="co"># Compute Accuracy for each epoch</span></span>
<span id="cb20-598"><a href="#cb20-598" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-599"><a href="#cb20-599" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-600"><a href="#cb20-600" tabindex="-1"></a>  <span class="co">#Accuracy Training</span></span>
<span id="cb20-601"><a href="#cb20-601" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-602"><a href="#cb20-602" tabindex="-1"></a>  <span class="co"># Find the highest probability for each observation in X_out</span></span>
<span id="cb20-603"><a href="#cb20-603" tabindex="-1"></a>  X_out_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out, <span class="dv">2</span>, max)</span>
<span id="cb20-604"><a href="#cb20-604" tabindex="-1"></a></span>
<span id="cb20-605"><a href="#cb20-605" tabindex="-1"></a>  X_out_hadamard_Y_training <span class="ot">&lt;-</span>X_out <span class="sc">*</span> Y_One_Hot_Encoding_training</span>
<span id="cb20-606"><a href="#cb20-606" tabindex="-1"></a></span>
<span id="cb20-607"><a href="#cb20-607" tabindex="-1"></a>  <span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-608"><a href="#cb20-608" tabindex="-1"></a>  X_out_hadamard_Y_v_training <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_hadamard_Y_training, <span class="dv">2</span>, max)</span>
<span id="cb20-609"><a href="#cb20-609" tabindex="-1"></a></span>
<span id="cb20-610"><a href="#cb20-610" tabindex="-1"></a>  <span class="co"># Subtract the two vectors</span></span>
<span id="cb20-611"><a href="#cb20-611" tabindex="-1"></a>  difference_training <span class="ot">&lt;-</span> X_out_highest <span class="sc">-</span> X_out_hadamard_Y_v_training</span>
<span id="cb20-612"><a href="#cb20-612" tabindex="-1"></a></span>
<span id="cb20-613"><a href="#cb20-613" tabindex="-1"></a>  <span class="co"># Count the number of zeros</span></span>
<span id="cb20-614"><a href="#cb20-614" tabindex="-1"></a>  count_zeros_training <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_training <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-615"><a href="#cb20-615" tabindex="-1"></a></span>
<span id="cb20-616"><a href="#cb20-616" tabindex="-1"></a>  <span class="co"># Accuaracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-617"><a href="#cb20-617" tabindex="-1"></a>  accuracy_percent_training <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_training <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_training, <span class="dv">2</span>)</span>
<span id="cb20-618"><a href="#cb20-618" tabindex="-1"></a></span>
<span id="cb20-619"><a href="#cb20-619" tabindex="-1"></a>  <span class="co">#Stores training accuracy percentage for each epoch</span></span>
<span id="cb20-620"><a href="#cb20-620" tabindex="-1"></a>  Accuracy_Percent<span class="sc">$</span>Training_percent[epoch] <span class="ot">&lt;-</span>accuracy_percent_training</span>
<span id="cb20-621"><a href="#cb20-621" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-622"><a href="#cb20-622" tabindex="-1"></a><span class="co">#Accuracy Testing</span></span>
<span id="cb20-623"><a href="#cb20-623" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-624"><a href="#cb20-624" tabindex="-1"></a><span class="co"># Find the highest probability for each observation in X_out_testing</span></span>
<span id="cb20-625"><a href="#cb20-625" tabindex="-1"></a>X_out_testing_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-626"><a href="#cb20-626" tabindex="-1"></a></span>
<span id="cb20-627"><a href="#cb20-627" tabindex="-1"></a>X_out_testing_hadamard_Y_testing <span class="ot">&lt;-</span>X_out_testing <span class="sc">*</span> Y_One_Hot_Encoding_testing</span>
<span id="cb20-628"><a href="#cb20-628" tabindex="-1"></a></span>
<span id="cb20-629"><a href="#cb20-629" tabindex="-1"></a><span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-630"><a href="#cb20-630" tabindex="-1"></a>X_out_testing_hadamard_Y_v_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing_hadamard_Y_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-631"><a href="#cb20-631" tabindex="-1"></a></span>
<span id="cb20-632"><a href="#cb20-632" tabindex="-1"></a><span class="co"># Subtract the two vectors</span></span>
<span id="cb20-633"><a href="#cb20-633" tabindex="-1"></a>difference_testing <span class="ot">&lt;-</span> X_out_testing_highest <span class="sc">-</span> X_out_testing_hadamard_Y_v_testing</span>
<span id="cb20-634"><a href="#cb20-634" tabindex="-1"></a></span>
<span id="cb20-635"><a href="#cb20-635" tabindex="-1"></a><span class="co"># Count the number of zeros</span></span>
<span id="cb20-636"><a href="#cb20-636" tabindex="-1"></a>count_zeros_testing <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_testing <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-637"><a href="#cb20-637" tabindex="-1"></a></span>
<span id="cb20-638"><a href="#cb20-638" tabindex="-1"></a><span class="co"># Accuaracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-639"><a href="#cb20-639" tabindex="-1"></a>accuracy_percent_testing <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_testing <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_testing, <span class="dv">2</span>)</span>
<span id="cb20-640"><a href="#cb20-640" tabindex="-1"></a></span>
<span id="cb20-641"><a href="#cb20-641" tabindex="-1"></a><span class="co">#Stores testing accuracy percentage for each epoch</span></span>
<span id="cb20-642"><a href="#cb20-642" tabindex="-1"></a>Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch] <span class="ot">&lt;-</span>accuracy_percent_testing</span>
<span id="cb20-643"><a href="#cb20-643" tabindex="-1"></a> </span>
<span id="cb20-644"><a href="#cb20-644" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-645"><a href="#cb20-645" tabindex="-1"></a>  <span class="co"># Comment out #cat() to prevent results from being printed out on console</span></span>
<span id="cb20-646"><a href="#cb20-646" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;iteration: &quot;</span>, epoch, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-647"><a href="#cb20-647" tabindex="-1"></a>    <span class="st">&quot;epoch: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>epoch[epoch], <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-648"><a href="#cb20-648" tabindex="-1"></a>    <span class="st">&quot;Training Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-649"><a href="#cb20-649" tabindex="-1"></a>    <span class="st">&quot;Testing Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-650"><a href="#cb20-650" tabindex="-1"></a>    <span class="st">&quot;Training Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Training_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-651"><a href="#cb20-651" tabindex="-1"></a>    <span class="st">&quot;Testing Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb20-652"><a href="#cb20-652" tabindex="-1"></a></span>
<span id="cb20-653"><a href="#cb20-653" tabindex="-1"></a>}</span>
<span id="cb20-654"><a href="#cb20-654" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-655"><a href="#cb20-655" tabindex="-1"></a><span class="co">#12</span></span>
<span id="cb20-656"><a href="#cb20-656" tabindex="-1"></a><span class="co">#BATCH GRADIENT DESCENT</span></span>
<span id="cb20-657"><a href="#cb20-657" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-658"><a href="#cb20-658" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-659"><a href="#cb20-659" tabindex="-1"></a><span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs) {</span>
<span id="cb20-660"><a href="#cb20-660" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-661"><a href="#cb20-661" tabindex="-1"></a>  <span class="co"># Code for Training forward forward pass</span></span>
<span id="cb20-662"><a href="#cb20-662" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-663"><a href="#cb20-663" tabindex="-1"></a><span class="co"># Batch Gradient Descent</span></span>
<span id="cb20-664"><a href="#cb20-664" tabindex="-1"></a>X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map)))</span>
<span id="cb20-665"><a href="#cb20-665" tabindex="-1"></a></span>
<span id="cb20-666"><a href="#cb20-666" tabindex="-1"></a>Z2 <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1</span>
<span id="cb20-667"><a href="#cb20-667" tabindex="-1"></a></span>
<span id="cb20-668"><a href="#cb20-668" tabindex="-1"></a>LeakyReLU <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-669"><a href="#cb20-669" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-670"><a href="#cb20-670" tabindex="-1"></a></span>
<span id="cb20-671"><a href="#cb20-671" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-672"><a href="#cb20-672" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-673"><a href="#cb20-673" tabindex="-1"></a></span>
<span id="cb20-674"><a href="#cb20-674" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-675"><a href="#cb20-675" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-676"><a href="#cb20-676" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-677"><a href="#cb20-677" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-678"><a href="#cb20-678" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-679"><a href="#cb20-679" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-680"><a href="#cb20-680" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-681"><a href="#cb20-681" tabindex="-1"></a></span>
<span id="cb20-682"><a href="#cb20-682" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-683"><a href="#cb20-683" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-684"><a href="#cb20-684" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-685"><a href="#cb20-685" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-686"><a href="#cb20-686" tabindex="-1"></a>Z3 <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-687"><a href="#cb20-687" tabindex="-1"></a></span>
<span id="cb20-688"><a href="#cb20-688" tabindex="-1"></a><span class="co"># LeakyReLU activation function</span></span>
<span id="cb20-689"><a href="#cb20-689" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-690"><a href="#cb20-690" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-691"><a href="#cb20-691" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x)) <span class="sc">%&gt;%</span> </span>
<span id="cb20-692"><a href="#cb20-692" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-693"><a href="#cb20-693" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-694"><a href="#cb20-694" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-695"><a href="#cb20-695" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-696"><a href="#cb20-696" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-697"><a href="#cb20-697" tabindex="-1"></a>}</span>
<span id="cb20-698"><a href="#cb20-698" tabindex="-1"></a></span>
<span id="cb20-699"><a href="#cb20-699" tabindex="-1"></a>X3 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-700"><a href="#cb20-700" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-701"><a href="#cb20-701" tabindex="-1"></a></span>
<span id="cb20-702"><a href="#cb20-702" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3</span>
<span id="cb20-703"><a href="#cb20-703" tabindex="-1"></a>}</span>
<span id="cb20-704"><a href="#cb20-704" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-705"><a href="#cb20-705" tabindex="-1"></a></span>
<span id="cb20-706"><a href="#cb20-706" tabindex="-1"></a><span class="co"># Define Softmax(Max)</span></span>
<span id="cb20-707"><a href="#cb20-707" tabindex="-1"></a>softmax_MAX <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb20-708"><a href="#cb20-708" tabindex="-1"></a>  <span class="co"># zi-max(z)</span></span>
<span id="cb20-709"><a href="#cb20-709" tabindex="-1"></a>  zi_max <span class="ot">&lt;-</span> z <span class="sc">-</span> <span class="fu">max</span>(z)</span>
<span id="cb20-710"><a href="#cb20-710" tabindex="-1"></a>  <span class="co"># e^(zi-max(z))</span></span>
<span id="cb20-711"><a href="#cb20-711" tabindex="-1"></a>  exp_zi_max <span class="ot">&lt;-</span> <span class="fu">exp</span>(zi_max)</span>
<span id="cb20-712"><a href="#cb20-712" tabindex="-1"></a>  <span class="co"># e^(zi-max(z)-log(sum(e^(zj-max(z)))</span></span>
<span id="cb20-713"><a href="#cb20-713" tabindex="-1"></a>  <span class="fu">exp</span>(zi_max <span class="sc">-</span> <span class="fu">log</span>(<span class="fu">sum</span>(exp_zi_max)))</span>
<span id="cb20-714"><a href="#cb20-714" tabindex="-1"></a>}</span>
<span id="cb20-715"><a href="#cb20-715" tabindex="-1"></a></span>
<span id="cb20-716"><a href="#cb20-716" tabindex="-1"></a><span class="co"># note that:</span></span>
<span id="cb20-717"><a href="#cb20-717" tabindex="-1"></a><span class="co"># apply(Z_out, 1 ----means its row wise</span></span>
<span id="cb20-718"><a href="#cb20-718" tabindex="-1"></a><span class="co"># apply(Z_out, 2 ----means its column wise</span></span>
<span id="cb20-719"><a href="#cb20-719" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-720"><a href="#cb20-720" tabindex="-1"></a>X_out <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-721"><a href="#cb20-721" tabindex="-1"></a></span>
<span id="cb20-722"><a href="#cb20-722" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-723"><a href="#cb20-723" tabindex="-1"></a><span class="co"># Code for Testing forward forward pass</span></span>
<span id="cb20-724"><a href="#cb20-724" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-725"><a href="#cb20-725" tabindex="-1"></a><span class="co"># Add bias row. Row of 1&#39;s</span></span>
<span id="cb20-726"><a href="#cb20-726" tabindex="-1"></a>X1_test <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map_testing, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map_testing)))</span>
<span id="cb20-727"><a href="#cb20-727" tabindex="-1"></a></span>
<span id="cb20-728"><a href="#cb20-728" tabindex="-1"></a>Z2_testing <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1_test</span>
<span id="cb20-729"><a href="#cb20-729" tabindex="-1"></a></span>
<span id="cb20-730"><a href="#cb20-730" tabindex="-1"></a>LeakyReLU_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-731"><a href="#cb20-731" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-732"><a href="#cb20-732" tabindex="-1"></a></span>
<span id="cb20-733"><a href="#cb20-733" tabindex="-1"></a>X2_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-734"><a href="#cb20-734" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-735"><a href="#cb20-735" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-736"><a href="#cb20-736" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-737"><a href="#cb20-737" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-738"><a href="#cb20-738" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-739"><a href="#cb20-739" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-740"><a href="#cb20-740" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-741"><a href="#cb20-741" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-742"><a href="#cb20-742" tabindex="-1"></a></span>
<span id="cb20-743"><a href="#cb20-743" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-744"><a href="#cb20-744" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-745"><a href="#cb20-745" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-746"><a href="#cb20-746" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-747"><a href="#cb20-747" tabindex="-1"></a>Z3_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-748"><a href="#cb20-748" tabindex="-1"></a></span>
<span id="cb20-749"><a href="#cb20-749" tabindex="-1"></a>LeakyReLU2_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-750"><a href="#cb20-750" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-751"><a href="#cb20-751" tabindex="-1"></a></span>
<span id="cb20-752"><a href="#cb20-752" tabindex="-1"></a>X3_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-753"><a href="#cb20-753" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-754"><a href="#cb20-754" tabindex="-1"></a></span>
<span id="cb20-755"><a href="#cb20-755" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3_testing</span>
<span id="cb20-756"><a href="#cb20-756" tabindex="-1"></a>}</span>
<span id="cb20-757"><a href="#cb20-757" tabindex="-1"></a></span>
<span id="cb20-758"><a href="#cb20-758" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-759"><a href="#cb20-759" tabindex="-1"></a>X_out_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out_testing, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-760"><a href="#cb20-760" tabindex="-1"></a></span>
<span id="cb20-761"><a href="#cb20-761" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-762"><a href="#cb20-762" tabindex="-1"></a>  <span class="co"># Code for backpropagation training</span></span>
<span id="cb20-763"><a href="#cb20-763" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-764"><a href="#cb20-764" tabindex="-1"></a> <span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-765"><a href="#cb20-765" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-766"><a href="#cb20-766" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-767"><a href="#cb20-767" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-768"><a href="#cb20-768" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-769"><a href="#cb20-769" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-770"><a href="#cb20-770" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-771"><a href="#cb20-771" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t <span class="ot">&lt;-</span> (X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-772"><a href="#cb20-772" tabindex="-1"></a></span>
<span id="cb20-773"><a href="#cb20-773" tabindex="-1"></a>  dZ3_out_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-774"><a href="#cb20-774" tabindex="-1"></a></span>
<span id="cb20-775"><a href="#cb20-775" tabindex="-1"></a>  <span class="co"># dC/dM2 = [(dC/dX_out)^t *(dX_out/dZ3_out)^t]^t * dZ3_out/dM2</span></span>
<span id="cb20-776"><a href="#cb20-776" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span> dc_dX_out_t_times_dX_out_dZ3_out_t <span class="sc">%*%</span> dZ3_out_dM2 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-777"><a href="#cb20-777" tabindex="-1"></a></span>
<span id="cb20-778"><a href="#cb20-778" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-779"><a href="#cb20-779" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-780"><a href="#cb20-780" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-781"><a href="#cb20-781" tabindex="-1"></a>  <span class="co"># (dC/dX_out)^t *(dX_out/dZ3_out)^t</span></span>
<span id="cb20-782"><a href="#cb20-782" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t)</span>
<span id="cb20-783"><a href="#cb20-783" tabindex="-1"></a></span>
<span id="cb20-784"><a href="#cb20-784" tabindex="-1"></a>  dZ3_out_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-785"><a href="#cb20-785" tabindex="-1"></a></span>
<span id="cb20-786"><a href="#cb20-786" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-787"><a href="#cb20-787" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-788"><a href="#cb20-788" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-789"><a href="#cb20-789" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-790"><a href="#cb20-790" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-791"><a href="#cb20-791" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-792"><a href="#cb20-792" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-793"><a href="#cb20-793" tabindex="-1"></a>  }</span>
<span id="cb20-794"><a href="#cb20-794" tabindex="-1"></a></span>
<span id="cb20-795"><a href="#cb20-795" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-796"><a href="#cb20-796" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-797"><a href="#cb20-797" tabindex="-1"></a></span>
<span id="cb20-798"><a href="#cb20-798" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-799"><a href="#cb20-799" tabindex="-1"></a></span>
<span id="cb20-800"><a href="#cb20-800" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="sc">%*%</span> dZ3_out_dX2)) <span class="sc">*</span> dX2_dZ2) <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-801"><a href="#cb20-801" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-802"><a href="#cb20-802" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-803"><a href="#cb20-803" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-804"><a href="#cb20-804" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-805"><a href="#cb20-805" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-806"><a href="#cb20-806" tabindex="-1"></a>  <span class="co"># DC/DM3</span></span>
<span id="cb20-807"><a href="#cb20-807" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-808"><a href="#cb20-808" tabindex="-1"></a>  dc_dZ_out_t <span class="ot">&lt;-</span>(X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-809"><a href="#cb20-809" tabindex="-1"></a></span>
<span id="cb20-810"><a href="#cb20-810" tabindex="-1"></a>  dZ_out_dM3 <span class="ot">&lt;-</span><span class="fu">t</span>(X3)</span>
<span id="cb20-811"><a href="#cb20-811" tabindex="-1"></a></span>
<span id="cb20-812"><a href="#cb20-812" tabindex="-1"></a>  dC_dM3 <span class="ot">&lt;-</span>dc_dZ_out_t <span class="sc">%*%</span> dZ_out_dM3 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-813"><a href="#cb20-813" tabindex="-1"></a></span>
<span id="cb20-814"><a href="#cb20-814" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-815"><a href="#cb20-815" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-816"><a href="#cb20-816" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-817"><a href="#cb20-817" tabindex="-1"></a>  dc_dZ_out <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dZ_out_t)</span>
<span id="cb20-818"><a href="#cb20-818" tabindex="-1"></a></span>
<span id="cb20-819"><a href="#cb20-819" tabindex="-1"></a>  dZ4_dX3 <span class="ot">&lt;-</span> M3[, <span class="sc">-</span><span class="fu">ncol</span>(M3)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-820"><a href="#cb20-820" tabindex="-1"></a></span>
<span id="cb20-821"><a href="#cb20-821" tabindex="-1"></a>  <span class="co"># dX3/dZ3 (element wise)</span></span>
<span id="cb20-822"><a href="#cb20-822" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-823"><a href="#cb20-823" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-824"><a href="#cb20-824" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-825"><a href="#cb20-825" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-826"><a href="#cb20-826" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-827"><a href="#cb20-827" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-828"><a href="#cb20-828" tabindex="-1"></a>  }</span>
<span id="cb20-829"><a href="#cb20-829" tabindex="-1"></a></span>
<span id="cb20-830"><a href="#cb20-830" tabindex="-1"></a>  dX3_dZ3 <span class="ot">&lt;-</span> dX3_dZ3_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX3_dZ3_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-831"><a href="#cb20-831" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-832"><a href="#cb20-832" tabindex="-1"></a></span>
<span id="cb20-833"><a href="#cb20-833" tabindex="-1"></a>  dZ3_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-834"><a href="#cb20-834" tabindex="-1"></a></span>
<span id="cb20-835"><a href="#cb20-835" tabindex="-1"></a>  dC_dZ3 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dc_dZ_out <span class="sc">%*%</span> dZ4_dX3)) <span class="sc">*</span> dX3_dZ3</span>
<span id="cb20-836"><a href="#cb20-836" tabindex="-1"></a></span>
<span id="cb20-837"><a href="#cb20-837" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span>dC_dZ3 <span class="sc">%*%</span> dZ3_dM2</span>
<span id="cb20-838"><a href="#cb20-838" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-839"><a href="#cb20-839" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-840"><a href="#cb20-840" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-841"><a href="#cb20-841" tabindex="-1"></a>  dC_dZ3_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dC_dZ3)</span>
<span id="cb20-842"><a href="#cb20-842" tabindex="-1"></a></span>
<span id="cb20-843"><a href="#cb20-843" tabindex="-1"></a>  dZ3_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-844"><a href="#cb20-844" tabindex="-1"></a></span>
<span id="cb20-845"><a href="#cb20-845" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-846"><a href="#cb20-846" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-847"><a href="#cb20-847" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-848"><a href="#cb20-848" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-849"><a href="#cb20-849" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-850"><a href="#cb20-850" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-851"><a href="#cb20-851" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-852"><a href="#cb20-852" tabindex="-1"></a>  }</span>
<span id="cb20-853"><a href="#cb20-853" tabindex="-1"></a></span>
<span id="cb20-854"><a href="#cb20-854" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-855"><a href="#cb20-855" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-856"><a href="#cb20-856" tabindex="-1"></a></span>
<span id="cb20-857"><a href="#cb20-857" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-858"><a href="#cb20-858" tabindex="-1"></a></span>
<span id="cb20-859"><a href="#cb20-859" tabindex="-1"></a>  dC_dZ2 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dC_dZ3_t <span class="sc">%*%</span> dZ3_dX2)) <span class="sc">*</span> dX2_dZ2</span>
<span id="cb20-860"><a href="#cb20-860" tabindex="-1"></a></span>
<span id="cb20-861"><a href="#cb20-861" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span>dC_dZ2 <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-862"><a href="#cb20-862" tabindex="-1"></a>}</span>
<span id="cb20-863"><a href="#cb20-863" tabindex="-1"></a></span>
<span id="cb20-864"><a href="#cb20-864" tabindex="-1"></a><span class="do">##############################################################</span></span>
<span id="cb20-865"><a href="#cb20-865" tabindex="-1"></a>  <span class="co"># Gradient Descent</span></span>
<span id="cb20-866"><a href="#cb20-866" tabindex="-1"></a><span class="do">#############################################################</span></span>
<span id="cb20-867"><a href="#cb20-867" tabindex="-1"></a><span class="co"># note that in SGD and minibatch Epochs = Epoch*number_iterations_epoch</span></span>
<span id="cb20-868"><a href="#cb20-868" tabindex="-1"></a><span class="co"># Update weights and biases (based on backpropagation)</span></span>
<span id="cb20-869"><a href="#cb20-869" tabindex="-1"></a>  </span>
<span id="cb20-870"><a href="#cb20-870" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-871"><a href="#cb20-871" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-872"><a href="#cb20-872" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-873"><a href="#cb20-873" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-874"><a href="#cb20-874" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-875"><a href="#cb20-875" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-876"><a href="#cb20-876" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-877"><a href="#cb20-877" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-878"><a href="#cb20-878" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-879"><a href="#cb20-879" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-880"><a href="#cb20-880" tabindex="-1"></a></span>
<span id="cb20-881"><a href="#cb20-881" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-882"><a href="#cb20-882" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-883"><a href="#cb20-883" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-884"><a href="#cb20-884" tabindex="-1"></a>}</span>
<span id="cb20-885"><a href="#cb20-885" tabindex="-1"></a>       },</span>
<span id="cb20-886"><a href="#cb20-886" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-887"><a href="#cb20-887" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-888"><a href="#cb20-888" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-889"><a href="#cb20-889" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-890"><a href="#cb20-890" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-891"><a href="#cb20-891" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-892"><a href="#cb20-892" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-893"><a href="#cb20-893" tabindex="-1"></a></span>
<span id="cb20-894"><a href="#cb20-894" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-895"><a href="#cb20-895" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-896"><a href="#cb20-896" tabindex="-1"></a>}</span>
<span id="cb20-897"><a href="#cb20-897" tabindex="-1"></a>       },</span>
<span id="cb20-898"><a href="#cb20-898" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-899"><a href="#cb20-899" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-900"><a href="#cb20-900" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-901"><a href="#cb20-901" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-902"><a href="#cb20-902" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-903"><a href="#cb20-903" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-904"><a href="#cb20-904" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-905"><a href="#cb20-905" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-906"><a href="#cb20-906" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-907"><a href="#cb20-907" tabindex="-1"></a><span class="co"># M2 </span></span>
<span id="cb20-908"><a href="#cb20-908" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-909"><a href="#cb20-909" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-910"><a href="#cb20-910" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-911"><a href="#cb20-911" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-912"><a href="#cb20-912" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-913"><a href="#cb20-913" tabindex="-1"></a></span>
<span id="cb20-914"><a href="#cb20-914" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-915"><a href="#cb20-915" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-916"><a href="#cb20-916" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-917"><a href="#cb20-917" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-918"><a href="#cb20-918" tabindex="-1"></a></span>
<span id="cb20-919"><a href="#cb20-919" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-920"><a href="#cb20-920" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-921"><a href="#cb20-921" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-922"><a href="#cb20-922" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-923"><a href="#cb20-923" tabindex="-1"></a>}</span>
<span id="cb20-924"><a href="#cb20-924" tabindex="-1"></a></span>
<span id="cb20-925"><a href="#cb20-925" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-926"><a href="#cb20-926" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-927"><a href="#cb20-927" tabindex="-1"></a></span>
<span id="cb20-928"><a href="#cb20-928" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-929"><a href="#cb20-929" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-930"><a href="#cb20-930" tabindex="-1"></a></span>
<span id="cb20-931"><a href="#cb20-931" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-932"><a href="#cb20-932" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-933"><a href="#cb20-933" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-934"><a href="#cb20-934" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-935"><a href="#cb20-935" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-936"><a href="#cb20-936" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-937"><a href="#cb20-937" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-938"><a href="#cb20-938" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1 </span>
<span id="cb20-939"><a href="#cb20-939" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-940"><a href="#cb20-940" tabindex="-1"></a></span>
<span id="cb20-941"><a href="#cb20-941" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-942"><a href="#cb20-942" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-943"><a href="#cb20-943" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-944"><a href="#cb20-944" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-945"><a href="#cb20-945" tabindex="-1"></a></span>
<span id="cb20-946"><a href="#cb20-946" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-947"><a href="#cb20-947" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-948"><a href="#cb20-948" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-949"><a href="#cb20-949" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-950"><a href="#cb20-950" tabindex="-1"></a>}</span>
<span id="cb20-951"><a href="#cb20-951" tabindex="-1"></a></span>
<span id="cb20-952"><a href="#cb20-952" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-953"><a href="#cb20-953" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-954"><a href="#cb20-954" tabindex="-1"></a></span>
<span id="cb20-955"><a href="#cb20-955" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-956"><a href="#cb20-956" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-957"><a href="#cb20-957" tabindex="-1"></a></span>
<span id="cb20-958"><a href="#cb20-958" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-959"><a href="#cb20-959" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-960"><a href="#cb20-960" tabindex="-1"></a></span>
<span id="cb20-961"><a href="#cb20-961" tabindex="-1"></a>} </span>
<span id="cb20-962"><a href="#cb20-962" tabindex="-1"></a>       }</span>
<span id="cb20-963"><a href="#cb20-963" tabindex="-1"></a>)</span>
<span id="cb20-964"><a href="#cb20-964" tabindex="-1"></a></span>
<span id="cb20-965"><a href="#cb20-965" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-966"><a href="#cb20-966" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-967"><a href="#cb20-967" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-968"><a href="#cb20-968" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-969"><a href="#cb20-969" tabindex="-1"></a></span>
<span id="cb20-970"><a href="#cb20-970" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-971"><a href="#cb20-971" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-972"><a href="#cb20-972" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-973"><a href="#cb20-973" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-974"><a href="#cb20-974" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-975"><a href="#cb20-975" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-976"><a href="#cb20-976" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-977"><a href="#cb20-977" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-978"><a href="#cb20-978" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-979"><a href="#cb20-979" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-980"><a href="#cb20-980" tabindex="-1"></a></span>
<span id="cb20-981"><a href="#cb20-981" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-982"><a href="#cb20-982" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-983"><a href="#cb20-983" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-984"><a href="#cb20-984" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-985"><a href="#cb20-985" tabindex="-1"></a>}</span>
<span id="cb20-986"><a href="#cb20-986" tabindex="-1"></a>       },</span>
<span id="cb20-987"><a href="#cb20-987" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-988"><a href="#cb20-988" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-989"><a href="#cb20-989" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-990"><a href="#cb20-990" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-991"><a href="#cb20-991" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-992"><a href="#cb20-992" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-993"><a href="#cb20-993" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-994"><a href="#cb20-994" tabindex="-1"></a></span>
<span id="cb20-995"><a href="#cb20-995" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-996"><a href="#cb20-996" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-997"><a href="#cb20-997" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-998"><a href="#cb20-998" tabindex="-1"></a>}</span>
<span id="cb20-999"><a href="#cb20-999" tabindex="-1"></a>       },</span>
<span id="cb20-1000"><a href="#cb20-1000" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-1001"><a href="#cb20-1001" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1002"><a href="#cb20-1002" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-1003"><a href="#cb20-1003" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1004"><a href="#cb20-1004" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1005"><a href="#cb20-1005" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-1006"><a href="#cb20-1006" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1007"><a href="#cb20-1007" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-1008"><a href="#cb20-1008" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1009"><a href="#cb20-1009" tabindex="-1"></a><span class="co"># M3</span></span>
<span id="cb20-1010"><a href="#cb20-1010" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1011"><a href="#cb20-1011" tabindex="-1"></a></span>
<span id="cb20-1012"><a href="#cb20-1012" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-1013"><a href="#cb20-1013" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-1014"><a href="#cb20-1014" tabindex="-1"></a>v1_prev_M3 <span class="ot">&lt;-</span> v1_M3</span>
<span id="cb20-1015"><a href="#cb20-1015" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-1016"><a href="#cb20-1016" tabindex="-1"></a></span>
<span id="cb20-1017"><a href="#cb20-1017" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-1018"><a href="#cb20-1018" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-1019"><a href="#cb20-1019" tabindex="-1"></a>v2_prev_M3 <span class="ot">&lt;-</span> v2_M3</span>
<span id="cb20-1020"><a href="#cb20-1020" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM3<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-1021"><a href="#cb20-1021" tabindex="-1"></a></span>
<span id="cb20-1022"><a href="#cb20-1022" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-1023"><a href="#cb20-1023" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-1024"><a href="#cb20-1024" tabindex="-1"></a>  v1_M3 <span class="ot">&lt;-</span> v1_prev_M3</span>
<span id="cb20-1025"><a href="#cb20-1025" tabindex="-1"></a>  v2_M3 <span class="ot">&lt;-</span> v2_prev_M3</span>
<span id="cb20-1026"><a href="#cb20-1026" tabindex="-1"></a>}</span>
<span id="cb20-1027"><a href="#cb20-1027" tabindex="-1"></a></span>
<span id="cb20-1028"><a href="#cb20-1028" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-1029"><a href="#cb20-1029" tabindex="-1"></a>v1_hat_M3 <span class="ot">&lt;-</span> v1_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-1030"><a href="#cb20-1030" tabindex="-1"></a></span>
<span id="cb20-1031"><a href="#cb20-1031" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-1032"><a href="#cb20-1032" tabindex="-1"></a>v2_hat_M3 <span class="ot">&lt;-</span> v2_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-1033"><a href="#cb20-1033" tabindex="-1"></a></span>
<span id="cb20-1034"><a href="#cb20-1034" tabindex="-1"></a><span class="co"># Update M3 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-1035"><a href="#cb20-1035" tabindex="-1"></a>M3 <span class="ot">&lt;-</span> M3 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M3 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M3) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-1036"><a href="#cb20-1036" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1037"><a href="#cb20-1037" tabindex="-1"></a><span class="co"># M2</span></span>
<span id="cb20-1038"><a href="#cb20-1038" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1039"><a href="#cb20-1039" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-1040"><a href="#cb20-1040" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-1041"><a href="#cb20-1041" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-1042"><a href="#cb20-1042" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-1043"><a href="#cb20-1043" tabindex="-1"></a></span>
<span id="cb20-1044"><a href="#cb20-1044" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-1045"><a href="#cb20-1045" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-1046"><a href="#cb20-1046" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-1047"><a href="#cb20-1047" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-1048"><a href="#cb20-1048" tabindex="-1"></a></span>
<span id="cb20-1049"><a href="#cb20-1049" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-1050"><a href="#cb20-1050" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-1051"><a href="#cb20-1051" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-1052"><a href="#cb20-1052" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-1053"><a href="#cb20-1053" tabindex="-1"></a>}</span>
<span id="cb20-1054"><a href="#cb20-1054" tabindex="-1"></a></span>
<span id="cb20-1055"><a href="#cb20-1055" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-1056"><a href="#cb20-1056" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-1057"><a href="#cb20-1057" tabindex="-1"></a></span>
<span id="cb20-1058"><a href="#cb20-1058" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-1059"><a href="#cb20-1059" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-1060"><a href="#cb20-1060" tabindex="-1"></a></span>
<span id="cb20-1061"><a href="#cb20-1061" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-1062"><a href="#cb20-1062" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-1063"><a href="#cb20-1063" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1064"><a href="#cb20-1064" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-1065"><a href="#cb20-1065" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1066"><a href="#cb20-1066" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-1067"><a href="#cb20-1067" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-1068"><a href="#cb20-1068" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1</span>
<span id="cb20-1069"><a href="#cb20-1069" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-1070"><a href="#cb20-1070" tabindex="-1"></a></span>
<span id="cb20-1071"><a href="#cb20-1071" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-1072"><a href="#cb20-1072" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-1073"><a href="#cb20-1073" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-1074"><a href="#cb20-1074" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-1075"><a href="#cb20-1075" tabindex="-1"></a></span>
<span id="cb20-1076"><a href="#cb20-1076" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-1077"><a href="#cb20-1077" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-1078"><a href="#cb20-1078" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-1079"><a href="#cb20-1079" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-1080"><a href="#cb20-1080" tabindex="-1"></a>}</span>
<span id="cb20-1081"><a href="#cb20-1081" tabindex="-1"></a></span>
<span id="cb20-1082"><a href="#cb20-1082" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-1083"><a href="#cb20-1083" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-1084"><a href="#cb20-1084" tabindex="-1"></a></span>
<span id="cb20-1085"><a href="#cb20-1085" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-1086"><a href="#cb20-1086" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-1087"><a href="#cb20-1087" tabindex="-1"></a></span>
<span id="cb20-1088"><a href="#cb20-1088" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-1089"><a href="#cb20-1089" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-1090"><a href="#cb20-1090" tabindex="-1"></a>}</span>
<span id="cb20-1091"><a href="#cb20-1091" tabindex="-1"></a>       }</span>
<span id="cb20-1092"><a href="#cb20-1092" tabindex="-1"></a>)</span>
<span id="cb20-1093"><a href="#cb20-1093" tabindex="-1"></a></span>
<span id="cb20-1094"><a href="#cb20-1094" tabindex="-1"></a>  <span class="co">#Store M3 for each epoch</span></span>
<span id="cb20-1095"><a href="#cb20-1095" tabindex="-1"></a>M3_list[[epoch]] <span class="ot">&lt;-</span>M3</span>
<span id="cb20-1096"><a href="#cb20-1096" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-1097"><a href="#cb20-1097" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-1098"><a href="#cb20-1098" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-1099"><a href="#cb20-1099" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-1100"><a href="#cb20-1100" tabindex="-1"></a></span>
<span id="cb20-1101"><a href="#cb20-1101" tabindex="-1"></a>}</span>
<span id="cb20-1102"><a href="#cb20-1102" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1103"><a href="#cb20-1103" tabindex="-1"></a>  <span class="co"># Compute Mean Categorical Cross Entropy Loss and store it in the CCEntropy_Loss dataframe</span></span>
<span id="cb20-1104"><a href="#cb20-1104" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1105"><a href="#cb20-1105" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1106"><a href="#cb20-1106" tabindex="-1"></a><span class="co"># Mean Categorical Cross Entropy Loss Training</span></span>
<span id="cb20-1107"><a href="#cb20-1107" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1108"><a href="#cb20-1108" tabindex="-1"></a><span class="co"># Catagorical Cross Entropy Loss</span></span>
<span id="cb20-1109"><a href="#cb20-1109" tabindex="-1"></a>Cost_training <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_training <span class="sc">*</span> <span class="fu">log</span>(X_out)</span>
<span id="cb20-1110"><a href="#cb20-1110" tabindex="-1"></a></span>
<span id="cb20-1111"><a href="#cb20-1111" tabindex="-1"></a><span class="co"># Sums up the CCE Loss for each observation</span></span>
<span id="cb20-1112"><a href="#cb20-1112" tabindex="-1"></a>Cost_training <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_training)</span>
<span id="cb20-1113"><a href="#cb20-1113" tabindex="-1"></a></span>
<span id="cb20-1114"><a href="#cb20-1114" tabindex="-1"></a><span class="co"># Stores mean CCE Loss X_out for each epoch</span></span>
<span id="cb20-1115"><a href="#cb20-1115" tabindex="-1"></a>CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch] <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>(Cost_training), <span class="dv">2</span>)</span>
<span id="cb20-1116"><a href="#cb20-1116" tabindex="-1"></a></span>
<span id="cb20-1117"><a href="#cb20-1117" tabindex="-1"></a><span class="co"># Stores epoch number for each epoch</span></span>
<span id="cb20-1118"><a href="#cb20-1118" tabindex="-1"></a>CCEntropy_Loss<span class="sc">$</span>epoch[epoch] <span class="ot">&lt;-</span> epoch</span>
<span id="cb20-1119"><a href="#cb20-1119" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1120"><a href="#cb20-1120" tabindex="-1"></a><span class="co"># Mean Categorical Cross Entropy Loss Testing</span></span>
<span id="cb20-1121"><a href="#cb20-1121" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1122"><a href="#cb20-1122" tabindex="-1"></a><span class="co"># Catagorical Cross Entropy Loss</span></span>
<span id="cb20-1123"><a href="#cb20-1123" tabindex="-1"></a>Cost_testing <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_testing <span class="sc">*</span> <span class="fu">log</span>(X_out_testing) </span>
<span id="cb20-1124"><a href="#cb20-1124" tabindex="-1"></a></span>
<span id="cb20-1125"><a href="#cb20-1125" tabindex="-1"></a><span class="co"># Sums up the CCE Loss for each observation</span></span>
<span id="cb20-1126"><a href="#cb20-1126" tabindex="-1"></a>Cost_testing <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_testing)</span>
<span id="cb20-1127"><a href="#cb20-1127" tabindex="-1"></a></span>
<span id="cb20-1128"><a href="#cb20-1128" tabindex="-1"></a><span class="co"># Stores mean CCE Loss X_out (testing) for each epoch</span></span>
<span id="cb20-1129"><a href="#cb20-1129" tabindex="-1"></a>CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch] <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>(Cost_testing), <span class="dv">2</span>)</span>
<span id="cb20-1130"><a href="#cb20-1130" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1131"><a href="#cb20-1131" tabindex="-1"></a>  <span class="co"># Compute Accuracy for each epoch</span></span>
<span id="cb20-1132"><a href="#cb20-1132" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1133"><a href="#cb20-1133" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1134"><a href="#cb20-1134" tabindex="-1"></a><span class="co">#Accuracy Training</span></span>
<span id="cb20-1135"><a href="#cb20-1135" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1136"><a href="#cb20-1136" tabindex="-1"></a><span class="co"># Find the highest probability for each observation in X_out</span></span>
<span id="cb20-1137"><a href="#cb20-1137" tabindex="-1"></a>X_out_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out, <span class="dv">2</span>, max)</span>
<span id="cb20-1138"><a href="#cb20-1138" tabindex="-1"></a></span>
<span id="cb20-1139"><a href="#cb20-1139" tabindex="-1"></a>X_out_hadamard_Y_training <span class="ot">&lt;-</span> X_out <span class="sc">*</span> Y_One_Hot_Encoding_training</span>
<span id="cb20-1140"><a href="#cb20-1140" tabindex="-1"></a></span>
<span id="cb20-1141"><a href="#cb20-1141" tabindex="-1"></a><span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-1142"><a href="#cb20-1142" tabindex="-1"></a>X_out_hadamard_Y_v_training <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_hadamard_Y_training, <span class="dv">2</span>, max)</span>
<span id="cb20-1143"><a href="#cb20-1143" tabindex="-1"></a></span>
<span id="cb20-1144"><a href="#cb20-1144" tabindex="-1"></a><span class="co"># Subtract the two vectors</span></span>
<span id="cb20-1145"><a href="#cb20-1145" tabindex="-1"></a>difference_training <span class="ot">&lt;-</span> X_out_highest <span class="sc">-</span> X_out_hadamard_Y_v_training</span>
<span id="cb20-1146"><a href="#cb20-1146" tabindex="-1"></a></span>
<span id="cb20-1147"><a href="#cb20-1147" tabindex="-1"></a><span class="co"># Count the number of zeros</span></span>
<span id="cb20-1148"><a href="#cb20-1148" tabindex="-1"></a>count_zeros_training <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_training <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-1149"><a href="#cb20-1149" tabindex="-1"></a></span>
<span id="cb20-1150"><a href="#cb20-1150" tabindex="-1"></a><span class="co"># Accuracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-1151"><a href="#cb20-1151" tabindex="-1"></a>accuracy_percent_training <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_training <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_training, <span class="dv">2</span>)</span>
<span id="cb20-1152"><a href="#cb20-1152" tabindex="-1"></a></span>
<span id="cb20-1153"><a href="#cb20-1153" tabindex="-1"></a><span class="co"># Stores training accuracy percentage for each epoch</span></span>
<span id="cb20-1154"><a href="#cb20-1154" tabindex="-1"></a>Accuracy_Percent<span class="sc">$</span>Training_percent[epoch] <span class="ot">&lt;-</span> accuracy_percent_training</span>
<span id="cb20-1155"><a href="#cb20-1155" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1156"><a href="#cb20-1156" tabindex="-1"></a>  <span class="co">#Accuracy Testing</span></span>
<span id="cb20-1157"><a href="#cb20-1157" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1158"><a href="#cb20-1158" tabindex="-1"></a><span class="co"># Find the highest probability for each observation in X_out_testing</span></span>
<span id="cb20-1159"><a href="#cb20-1159" tabindex="-1"></a>X_out_testing_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-1160"><a href="#cb20-1160" tabindex="-1"></a></span>
<span id="cb20-1161"><a href="#cb20-1161" tabindex="-1"></a>X_out_testing_hadamard_Y_testing <span class="ot">&lt;-</span> X_out_testing <span class="sc">*</span> Y_One_Hot_Encoding_testing</span>
<span id="cb20-1162"><a href="#cb20-1162" tabindex="-1"></a></span>
<span id="cb20-1163"><a href="#cb20-1163" tabindex="-1"></a><span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-1164"><a href="#cb20-1164" tabindex="-1"></a>X_out_testing_hadamard_Y_v_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing_hadamard_Y_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-1165"><a href="#cb20-1165" tabindex="-1"></a></span>
<span id="cb20-1166"><a href="#cb20-1166" tabindex="-1"></a><span class="co"># Subtract the two vectors</span></span>
<span id="cb20-1167"><a href="#cb20-1167" tabindex="-1"></a>difference_testing <span class="ot">&lt;-</span> X_out_testing_highest <span class="sc">-</span> X_out_testing_hadamard_Y_v_testing</span>
<span id="cb20-1168"><a href="#cb20-1168" tabindex="-1"></a></span>
<span id="cb20-1169"><a href="#cb20-1169" tabindex="-1"></a><span class="co"># Count the number of zeros</span></span>
<span id="cb20-1170"><a href="#cb20-1170" tabindex="-1"></a>count_zeros_testing <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_testing <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-1171"><a href="#cb20-1171" tabindex="-1"></a></span>
<span id="cb20-1172"><a href="#cb20-1172" tabindex="-1"></a><span class="co"># Accuracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-1173"><a href="#cb20-1173" tabindex="-1"></a>accuracy_percent_testing <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_testing <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_testing, <span class="dv">2</span>)</span>
<span id="cb20-1174"><a href="#cb20-1174" tabindex="-1"></a></span>
<span id="cb20-1175"><a href="#cb20-1175" tabindex="-1"></a><span class="co"># Stores testing accuracy percentage for each epoch</span></span>
<span id="cb20-1176"><a href="#cb20-1176" tabindex="-1"></a>Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch] <span class="ot">&lt;-</span> accuracy_percent_testing</span>
<span id="cb20-1177"><a href="#cb20-1177" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-1178"><a href="#cb20-1178" tabindex="-1"></a>  <span class="co"># Comment out #cat() will make results not being printed out on console</span></span>
<span id="cb20-1179"><a href="#cb20-1179" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, epoch, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1180"><a href="#cb20-1180" tabindex="-1"></a>    <span class="st">&quot;Training Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1181"><a href="#cb20-1181" tabindex="-1"></a>    <span class="st">&quot;Testing Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1182"><a href="#cb20-1182" tabindex="-1"></a>    <span class="st">&quot;Training Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Training_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1183"><a href="#cb20-1183" tabindex="-1"></a>    <span class="st">&quot;Testing Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb20-1184"><a href="#cb20-1184" tabindex="-1"></a>}</span>
<span id="cb20-1185"><a href="#cb20-1185" tabindex="-1"></a></span>
<span id="cb20-1186"><a href="#cb20-1186" tabindex="-1"></a>}</span></code></pre></div>
<p>Minimum value for mean categorical cross entropy loss.<br>
Epoch/iteration with minimum mean categorical cross entropy
loss.<br></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="co"># SGD and Mini Batch Gradient Descent</span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>  <span class="co"># SGD and Mini Batch Gradient Descent</span></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>  Iteration_lowest_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">which.min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>  Epoch_lowest_CCEntropy_Loss <span class="ot">&lt;-</span> CCEntropy_Loss<span class="sc">$</span>epoch[Iteration_lowest_CCEntropy_Loss]</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>  <span class="co"># Min Testing CCE Loss</span></span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>  Min_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>  <span class="co"># Training CCE Loss based on the epoch with the lowest testing CCE Loss</span></span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>  Min_CCEntropy_Loss_training <span class="ot">&lt;-</span> CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[Iteration_lowest_CCEntropy_Loss]</span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb21-16"><a href="#cb21-16" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-17"><a href="#cb21-17" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb21-18"><a href="#cb21-18" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-19"><a href="#cb21-19" tabindex="-1"></a>  <span class="co"># Batch Gradient Descent</span></span>
<span id="cb21-20"><a href="#cb21-20" tabindex="-1"></a>  Epoch_lowest_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">which.min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-21"><a href="#cb21-21" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" tabindex="-1"></a>  <span class="co"># Min Testing CCE Loss</span></span>
<span id="cb21-23"><a href="#cb21-23" tabindex="-1"></a>  Min_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-24"><a href="#cb21-24" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" tabindex="-1"></a>  <span class="co"># Training CCE Loss based on the epoch with the lowest testing CCE Loss</span></span>
<span id="cb21-26"><a href="#cb21-26" tabindex="-1"></a>  Min_CCEntropy_Loss_training <span class="ot">&lt;-</span> CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[Epoch_lowest_CCEntropy_Loss]</span>
<span id="cb21-27"><a href="#cb21-27" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="plot-mean-cce-loss-and-accuracy" class="section level2">
<h2>Plot Mean CCE Loss and Accuracy</h2>
<div id="plot-categorical-cross-entropy-testing-loss" class="section level3">
<h3>Plot Categorical Cross Entropy Testing Loss</h3>
<p>Plot of the mean categorical cross entropy TESTING loss for
CCEL_testing_x3<br> for each epoch in log scale. The red point
“CCEL_testing” is the location<br> of the testing loss where the testing
loss is the lowest.<br></p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="plot-categorical-cross-entropy-training-loss" class="section level3">
<h3>Plot Categorical Cross Entropy Training Loss</h3>
<p>Plot of the mean categorical cross entropy TRAINING loss for CCEL_x3
for<br> each epoch in log scale. The red point “CCEL_training” is the
location of<br> the training loss where the testing loss is the
lowest.<br></p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="plot-tracking-model-performance-training-vs-testing-accuracy" class="section level3">
<h3>Plot Tracking Model Performance: Training vs Testing Accuracy</h3>
<p>Percent Accuracy Derived from Lowest Loss in Testing Data.<br></p>
<p>Black dot is the accuracy that corresponds to the lowest loss in
the<br> testing data.<br></p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
</div>
<div id="neural-network-performance-summary" class="section level2">
<h2>Neural Network Performance Summary</h2>
<div id="no-mapping-vs-mapping" class="section level3">
<h3>No Mapping vs Mapping</h3>
<pre><code>## [1] &quot;No mapping&quot;</code></pre>
</div>
<div id="neural-network-parameters" class="section level3">
<h3>Neural Network Parameters</h3>
<pre><code>## [1] &quot;Number of training observations used in each batch: 350&quot;</code></pre>
<pre><code>## [1] &quot;Number of hidden layers: 1&quot;</code></pre>
<pre><code>## [1] &quot;Number of neurons in hidden layer: 100&quot;</code></pre>
</div>
<div id="epoch-with-min-testing-cce-loss" class="section level3">
<h3>Epoch with Min Testing CCE Loss</h3>
<pre><code>## [1] &quot;Epoch with Minimum Testing CCE Loss: 3.53&quot;</code></pre>
</div>
<div id="categorical-cross-entropy-loss" class="section level3">
<h3>Categorical Cross Entropy Loss</h3>
<pre><code>## [1] &quot;Minimum Testing CCE Loss: 0.16&quot;</code></pre>
<pre><code>## [1] &quot;Training CCE Loss based on lowest testing CCE Loss: 0.15&quot;</code></pre>
</div>
<div id="optimal-accuracy" class="section level3">
<h3>Optimal Accuracy</h3>
<pre><code>## [1] &quot;Optimal Testing Accuracy %: 95.31&quot;</code></pre>
<pre><code>## [1] &quot;Optimal Training Accuracy %: 0.87&quot;</code></pre>
</div>
</div>
<div id="feedforward-prediction" class="section level2">
<h2>Feedforward Prediction</h2>
<div id="optimal-m-weight-matrices" class="section level3">
<h3>Optimal M Weight Matrices</h3>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="do">###########################</span></span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="co"># Optimal M Weight Matrices</span></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a><span class="do">###########################</span></span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-13"><a href="#cb31-13" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb31-14"><a href="#cb31-14" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb31-15"><a href="#cb31-15" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-16"><a href="#cb31-16" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-18"><a href="#cb31-18" tabindex="-1"></a>}</span>
<span id="cb31-19"><a href="#cb31-19" tabindex="-1"></a></span>
<span id="cb31-20"><a href="#cb31-20" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb31-21"><a href="#cb31-21" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-22"><a href="#cb31-22" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb31-23"><a href="#cb31-23" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-24"><a href="#cb31-24" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb31-25"><a href="#cb31-25" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb31-26"><a href="#cb31-26" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-27"><a href="#cb31-27" tabindex="-1"></a></span>
<span id="cb31-28"><a href="#cb31-28" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-29"><a href="#cb31-29" tabindex="-1"></a></span>
<span id="cb31-30"><a href="#cb31-30" tabindex="-1"></a>M3_min <span class="ot">&lt;-</span>M3_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-31"><a href="#cb31-31" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb31-32"><a href="#cb31-32" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb31-33"><a href="#cb31-33" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-34"><a href="#cb31-34" tabindex="-1"></a></span>
<span id="cb31-35"><a href="#cb31-35" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-36"><a href="#cb31-36" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" tabindex="-1"></a>M3_min <span class="ot">&lt;-</span>M3_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-38"><a href="#cb31-38" tabindex="-1"></a>}</span>
<span id="cb31-39"><a href="#cb31-39" tabindex="-1"></a></span>
<span id="cb31-40"><a href="#cb31-40" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="feedforward-code" class="section level3">
<h3>Feedforward Code</h3>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="co"># Randomly select 3 images from the testing data</span></span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>Randomly_selected_rows_pred <span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">nrow</span>(Data_testing_scaled), n_prediction, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>Data_pred <span class="ot">&lt;-</span>Data_testing_scaled[Randomly_selected_rows_pred, ] </span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a>Row_Image_Matrix_pred <span class="ot">&lt;-</span>Data_pred[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_pred)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a>  <span class="co"># Code for Training forward forward pass</span></span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb32-11"><a href="#cb32-11" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb32-12"><a href="#cb32-12" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb32-13"><a href="#cb32-13" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb32-14"><a href="#cb32-14" tabindex="-1"></a>X1_pred <span class="ot">&lt;-</span>Row_Image_Matrix_pred <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb32-15"><a href="#cb32-15" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" tabindex="-1"></a>X1_map_vs_no_map_pred <span class="ot">&lt;-</span>X1_pred</span>
<span id="cb32-17"><a href="#cb32-17" tabindex="-1"></a>       },</span>
<span id="cb32-18"><a href="#cb32-18" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb32-19"><a href="#cb32-19" tabindex="-1"></a><span class="co">#Xi = X1_pred</span></span>
<span id="cb32-20"><a href="#cb32-20" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" tabindex="-1"></a>X1_pred <span class="ot">&lt;-</span>Row_Image_Matrix_pred <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb32-22"><a href="#cb32-22" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" tabindex="-1"></a>Hf_pi_B_Xi_pred <span class="ot">&lt;-</span>Hf_pi_B_pred <span class="sc">*</span> X1_pred</span>
<span id="cb32-24"><a href="#cb32-24" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" tabindex="-1"></a><span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb32-26"><a href="#cb32-26" tabindex="-1"></a>cos_element_wise_pred <span class="ot">&lt;-</span><span class="fu">cos</span>(Hf_pi_B_Xi_pred)</span>
<span id="cb32-27"><a href="#cb32-27" tabindex="-1"></a></span>
<span id="cb32-28"><a href="#cb32-28" tabindex="-1"></a><span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb32-29"><a href="#cb32-29" tabindex="-1"></a>sin_element_wise_pred <span class="ot">&lt;-</span><span class="fu">sin</span>(Hf_pi_B_Xi_pred)</span>
<span id="cb32-30"><a href="#cb32-30" tabindex="-1"></a></span>
<span id="cb32-31"><a href="#cb32-31" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb32-32"><a href="#cb32-32" tabindex="-1"></a><span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb32-33"><a href="#cb32-33" tabindex="-1"></a>one_zero_V_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb32-34"><a href="#cb32-34" tabindex="-1"></a></span>
<span id="cb32-35"><a href="#cb32-35" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb32-36"><a href="#cb32-36" tabindex="-1"></a>one_zero_M_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V_pred, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb32-37"><a href="#cb32-37" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" tabindex="-1"></a>cos_matrix_pred <span class="ot">&lt;-</span>one_zero_M_pred <span class="sc">*</span> cos_element_wise_pred</span>
<span id="cb32-39"><a href="#cb32-39" tabindex="-1"></a></span>
<span id="cb32-40"><a href="#cb32-40" tabindex="-1"></a><span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb32-41"><a href="#cb32-41" tabindex="-1"></a>zero_one_V_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb32-42"><a href="#cb32-42" tabindex="-1"></a></span>
<span id="cb32-43"><a href="#cb32-43" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb32-44"><a href="#cb32-44" tabindex="-1"></a>zero_one_M_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V_pred, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb32-45"><a href="#cb32-45" tabindex="-1"></a></span>
<span id="cb32-46"><a href="#cb32-46" tabindex="-1"></a>sin_matrix_pred <span class="ot">&lt;-</span>zero_one_M_pred <span class="sc">*</span> sin_element_wise_pred</span>
<span id="cb32-47"><a href="#cb32-47" tabindex="-1"></a></span>
<span id="cb32-48"><a href="#cb32-48" tabindex="-1"></a><span class="co"># Fourier Feature Mapping</span></span>
<span id="cb32-49"><a href="#cb32-49" tabindex="-1"></a>gamma_Xi_pred <span class="ot">&lt;-</span> cos_matrix_pred <span class="sc">+</span> sin_matrix_pred</span>
<span id="cb32-50"><a href="#cb32-50" tabindex="-1"></a></span>
<span id="cb32-51"><a href="#cb32-51" tabindex="-1"></a>X1_map_vs_no_map_pred <span class="ot">&lt;-</span>gamma_Xi_pred</span>
<span id="cb32-52"><a href="#cb32-52" tabindex="-1"></a>       })</span>
<span id="cb32-53"><a href="#cb32-53" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb32-54"><a href="#cb32-54" tabindex="-1"></a>X1_pred <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map_pred, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map_pred)))</span>
<span id="cb32-55"><a href="#cb32-55" tabindex="-1"></a></span>
<span id="cb32-56"><a href="#cb32-56" tabindex="-1"></a>Z2_pred <span class="ot">&lt;-</span> M1_min <span class="sc">%*%</span> X1_pred</span>
<span id="cb32-57"><a href="#cb32-57" tabindex="-1"></a></span>
<span id="cb32-58"><a href="#cb32-58" tabindex="-1"></a>LeakyReLU_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2_pred, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb32-59"><a href="#cb32-59" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb32-60"><a href="#cb32-60" tabindex="-1"></a></span>
<span id="cb32-61"><a href="#cb32-61" tabindex="-1"></a>X2_pred <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU_pred, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb32-62"><a href="#cb32-62" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb32-63"><a href="#cb32-63" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb32-64"><a href="#cb32-64" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb32-65"><a href="#cb32-65" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb32-66"><a href="#cb32-66" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-67"><a href="#cb32-67" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb32-68"><a href="#cb32-68" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-69"><a href="#cb32-69" tabindex="-1"></a>Z_out_pred <span class="ot">&lt;-</span> M2_min <span class="sc">%*%</span> X2_pred</span>
<span id="cb32-70"><a href="#cb32-70" tabindex="-1"></a></span>
<span id="cb32-71"><a href="#cb32-71" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb32-72"><a href="#cb32-72" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-73"><a href="#cb32-73" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb32-74"><a href="#cb32-74" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-75"><a href="#cb32-75" tabindex="-1"></a>Z3_pred <span class="ot">&lt;-</span> M2_min <span class="sc">%*%</span> X2_pred</span>
<span id="cb32-76"><a href="#cb32-76" tabindex="-1"></a></span>
<span id="cb32-77"><a href="#cb32-77" tabindex="-1"></a>LeakyReLU2_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3_pred, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb32-78"><a href="#cb32-78" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb32-79"><a href="#cb32-79" tabindex="-1"></a></span>
<span id="cb32-80"><a href="#cb32-80" tabindex="-1"></a>X3_pred <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2_pred, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb32-81"><a href="#cb32-81" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb32-82"><a href="#cb32-82" tabindex="-1"></a></span>
<span id="cb32-83"><a href="#cb32-83" tabindex="-1"></a>Z_out_pred <span class="ot">&lt;-</span> M3_min <span class="sc">%*%</span> X3_pred</span>
<span id="cb32-84"><a href="#cb32-84" tabindex="-1"></a>}</span>
<span id="cb32-85"><a href="#cb32-85" tabindex="-1"></a></span>
<span id="cb32-86"><a href="#cb32-86" tabindex="-1"></a>X_out_pred <span class="ot">&lt;-</span><span class="fu">apply</span>(Z_out_pred, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb32-87"><a href="#cb32-87" tabindex="-1"></a></span>
<span id="cb32-88"><a href="#cb32-88" tabindex="-1"></a><span class="co"># Predicted Minist Digit</span></span>
<span id="cb32-89"><a href="#cb32-89" tabindex="-1"></a>Mnist_digit_pred <span class="ot">&lt;-</span><span class="fu">apply</span>(X_out_pred, <span class="dv">2</span>, which.max)<span class="sc">-</span><span class="dv">1</span></span></code></pre></div>
</div>
<div id="pick-3-random-images-from-testing-data-to-predict." class="section level3">
<h3>Pick 3 Random Images From Testing Data to Predict.</h3>
<pre><code>## [1] &quot;Predicted Number: 6&quot;</code></pre>
<pre><code>## [1] &quot;Image:&quot;</code></pre>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAATlBMVEUAAAAAADoAAGYAOjoAOpAAZrY6AAA6ADo6kNtNTU1mAABmtv+QOgCQZgCQ2/+urq62ZgC2///bkDrb///m5ub/tmb/25D//7b//9v///+L2pFaAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAExUlEQVR4nO3c63KaQABAYdImaZqmiaU1Ce//okXAjEThLJdFlpzzo1M6zqrfwMoKNiust+zaL2DtCQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQTNDPQzlYLft0CLAGXHrv2+g3MPggSCBIIEggSCNgj076TpowkECQQJBAkECQRFAtpl2d37U7nm+iXQpfLbv6XRXVG8PT4IdN77U8myv3luqBYDOqV5qZo+ZhSgt8fyyNp/+1Mc/xSonXsQ9TEHVVQCnXedT7F5p+e6TZ0HCQSlCJRf61NsrjE3clUjxvRct5FDTCAoOaDqI76sYwb68kB51pwf7rNlThRjTM918ZYaDdUyS43EgKrFat1Ci9XEgNyDqPy4Bos/B9VTcmpA5UFWf4p17D8CYbO9AYEggaBToHlPEesEggSCBIKSB4o3PdcJBAkECQQJBCX/pf3GgD6a7Q0IBAnUWf8y9eWkKc8iECQQJBC0KaDzf5k+hQskkEAChQL1N27Cjgr0et91D+cXBzpeNuy5AeZrAx2vOLsHdfb2eLjoHBeoC6LrMasCKordzbNAveXZg0C9vd5/F6i396fOn2rEAep/zPqA+hKoKeJPEbYB9HmUGa9qbBLoo0k0XcvU/seMey6BrgMU96cI6QNF/ilC8kCxbyTvBwrhuzJQ7J8iJA/kHkRF/ilC+kCRf4rwL7gpNFGBMIEEEihtoElPcJJAkECQQFCSQHWnHNMn464EggSCBIIS/tJ+mRLeg5ZJIEggSCBIIEggSCBIIEggSCBIICjJuzuWLMm7O5YsyWvzS5bk3R1L5h4EJXl3x5IleXfHknkeBAkEJfxThGXyqgbkIQZdCyhLpjhAuFgNGZEeMHmA6c8w8rEBi9WQEbcLFLDUCBlxu0ABi9WQEbcL5B5E8WI1ZMQNA/FiNWTELQPNMqJAEx+wdaCNJRAkECQQJBAkECQQJBAkECQQJBAkECQQNBvQPstuni9ujBmgbNf3pROPkGdZ9/9mOKC5gPblS9sfX15rY8wAh+3eb+VwhPywMYfQTED119W7u/ONMQMU1XeXA4E+vYS7wS/hcjMB1f+9a3NLQ2tjzACHv93+HgjUGmF9QD8Ou3ZzMai1MWaAanPoHNQeYW2HWH3sNzNAa2PMANXhMhTo07OO+Zy41DqBDpfdpgHtyj3p9b774lRwqzzEqo1Jh9iYafByq5yk8+YWlUFTSGuEMTvx5db6MT98D2qNUGsN24kvt9oTxeFn0q0R1jYHVYfF4cXVJyD5iI+Q1gDFmKVGa4RdeYjO4ONilRIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEgj6Dx2CCxsxOurUAAAAAElFTkSuQmCC" /><!-- --></p>
<pre><code>## [1] &quot;Predicted Number: 3&quot;</code></pre>
<pre><code>## [1] &quot;Image:&quot;</code></pre>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAATlBMVEUAAAAAADoAAGYAOjoAOpAAZrY6AAA6ADo6kNtNTU1mAABmtv+QOgCQZgCQ2/+urq62ZgC2///bkDrb///m5ub/tmb/25D//7b//9v///+L2pFaAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAE50lEQVR4nO3cbXOaSgBAYdI2aW9umlhuTeL//6MXRR1R4CzLEhc450OnzMSNPgPI8pJiZ70V934DuScQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQlBjon7kU/LkF+hKg4tS9P3dwrkGQQJBAkECQQJBAkECQQJBAkECQQNBEQJuiePx8qeZc/wrUVvnjv8rocbf7eP4l0G2fLxXL9uH1SCXQdR/P1Za1/fZnd/pXoGauQdR5H3SgEui2r/gWe7vo70VvHWUFhAkkUFjldN9iywC6HmXkVY1bjriyBTonkEC9Hb7iqzr2QKsHKovj8eG2SHSg2LIzHvqB41413VTjSJVoqrEsoMNktS7VZHVZQK5BVHmagyXbBzWdRhz45QFUbWT1t1jH+iMQFvfpBKIEmrBLmnBigSCBIIGghQN1nU4LH0EgSCBIIGiBQH87iju8nNlJ+5BmDXRuCpljAkECtdR1Om3MtLZOIEggSCBoZkBdEP0JJJBAAo0EeguhuTyln4ppUqD3p657OFcOdLps2HMDzLqBTlecXYM6+3jeX3SeCugSIuRlY5gm2wdtHl4F6q0sfgnU2/vTd4F6+3zpfFTjay8c3uKGv3YuB4qjyhlowkcRwssZ6HqUu/yBpRkBnZtOo7O4XbVA0MweRRhTTkDpH0VIUEZAE9xInqCMgCZ4FCFBGQG5BlGTPYowppyAJnsUYUweKEICQQJBSwUa+olayvWEmUACCZQ10O0Z98Ef7/K1cdICCSTQkoAGnbRvAYq7WBiHexegcwIJJNDdgepuP2kDq4sjHFQggQQSCOuEeOtoDIpAAgk0d6Ac7+6Ia0V3d8S1omvzca3o7o64XIOgFd3dEdeK7u6Iay7HQXdLIGgVjyKMKeerGlnkJgbdC6iYTdMA4WQ1ZET6gdEDjP8NkT8bMFkNGXG5QAFTjZARlwsUMFkNGXG5QK5BFE9WQ0ZcMBBPVkNGXDJQkhEFGvkDSwdaWAJBAkECQQJBAkECQQJBAkECQQJBAkECQcmAtkXx8Nq6EDNA1abvpBOPUBZF918zHFAqoG311rant9dYiBlgv9x7Vg5HKPcLKYQSAdWnqzePtwsxA+wO5y4HAl29hcfBb6G9RED1n3c93tLQWIgZYP+/H78HAjVGyA/o537VPl4MaizEDHBYHLoPao6Q2yZWb/vHPUBjIWaAw+YyFOjqt8Z8T7SVJ9D+sts4oE21Jr0/dV+cCi7LTeywMGoTi9kNtpflTro83qIyaBfSGCFmJW4v16/54WtQY4Raa9hK3F62B4rDj6QbI+S2DzpsFvs3Vx+AlBFfIY0BdjFTjcYIm2oTTeDjZJUSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBII+h9boSJ//0OKiQAAAABJRU5ErkJggg==" /><!-- --></p>
<pre><code>## [1] &quot;Predicted Number: 9&quot;</code></pre>
<pre><code>## [1] &quot;Image:&quot;</code></pre>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAATlBMVEUAAAAAADoAAGYAOjoAOpAAZrY6AAA6ADo6kNtNTU1mAABmtv+QOgCQZgCQ2/+urq62ZgC2///bkDrb///m5ub/tmb/25D//7b//9v///+L2pFaAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEzElEQVR4nO3d61KbQABAYWzVWms1pY2a93/REkgckcthl02ykHN+dMqMWZNvFsK1LXY2WnHpN5B7AkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkGJgX4upcmfW6CzABXHLv25J+cMggSCBIIEggSCBIIEggSCBIIEggSCTgS0KYq796fqmOuXQH2Vt38ro7vd7u3xQaBu708Vy/bm+UAl0NfeHqs1a/vtz+74p0DtnEHUxzaophKom99ic7v0556cQNCpgUq/xYKae1Xjpe7fQC+fWijQRwIJNFr9FV81sAW6eqCyOOwfbou0O4pDNONk2QF92n9OfKixEqD6YLUp8cHqSoCcQVR5PAZLtg0KlekpJ6BqJWu+xQbmj0CYQAKdC2hw3zD4BVFMAgkkkEBhQOMvDt11FEggga4cqH9bOxGoO0TmQBEn7a8L6COBBLogUOiHFEgggQRKCTTrxYEjCCSQQALFfMjBl+UE9Ho/dA/nlQMdLxuO3ABz3UDHK87OoMHeHvcXnZMAweft7viN02Szo7i5eRZotLJ4EGi01/vvAo32/jT4qEYw0MvgufvAcgIaS6BDqR5FWC3Q11FiH0W4FqCPQoFaTnOAQn+jQJcBusyjCMsBOtmjCCsBOt2N5N26BC+fyhTodI8irATIGUSlfxRhZUDpH0WILFsg7BQaPQkECQQJNFiXRqBWAkECQQJBAkECQQJBCwU63/+K0KUJPV3fdEUzSKBWAkECQfM3z00CQQJBAkECQQJBAkECQQu7u2N6ny8cxu0iNi3s7o7pZQ10zmvzQ2UNdM67O4bKGiiHGZSqxd/dcepWfnfH/Fa7H5QqgaClPIpwsVZ7VSNVrmLQpYCKxXQaIDxYnTIi/cDsAeb/hsifnXCwOmXE9QJNONSYMuJ6gSYcrE4Zcb1AziCKD1anjLhiID5YnTLimoGSjCjQzB9YO9DKEggSCBIIEggSCBIIEggSCBIIEggSCBIISga0LYqb596FmAGqNmMnnXiEsiiG/zXDgFIBbau3tj2+vdZCzAD75dGzcjhCuV9IIZQIqDldvbnrLsQMsKvPXQYCfXkLd8Fvob9EQM0/73q4paG1EDPA/m+3vwOBWiPkB/RjP7UPF4NaCzED1Iuh26D2CLmtYs26f9gCtBZiBqhXl1CgL7815nuirzyB9pfd5gFtqpn0ej98cWpyWa5i9cKsVSxmM9hflhvp8nCLStAmpDVCzCTuL9ev+fAZ1Bqh0QqbxP1lu6MYvifdGiG3bVC9WuzfXLMDUkZ8hbQG2MUcarRG2FSraAIfD1YpgSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBoP/1DECbEJCrBQAAAABJRU5ErkJggg==" /><!-- --></p>
<p>FIN.<br></p>
<p>References<br></p>
<p>B, U. (2021, November 15). Cost Function in Machine Learning.
Medium.<br> <a href="https://medium.com/@uma.bollikonda/cost-function-in-machine-learning-129de85120d5" class="uri">https://medium.com/@uma.bollikonda/cost-function-in-machine-learning-129de85120d5</a><br></p>
<p><span class="citation">@article</span>{Chadha2020DistilledNeuralNetworks,<br>
title = {Neural Networks},<br> author = {Chadha, Aman},<br> journal =
{Distilled Notes for Stanford CS229: Machine Learning},<br> year =
{2020},<br> note = {}<br> }<br></p>
<p>charleshsliao. (2017, February 25). Two Ways of Visualization of
MNIST with R. Charles’ Hodgepodge.<br> <a href="https://charleshsliao.wordpress.com/2017/02/25/two-ways-of-visualization-of-mnist-with-r/" class="uri">https://charleshsliao.wordpress.com/2017/02/25/two-ways-of-visualization-of-mnist-with-r/</a><br></p>
<p>Kingma, D., &amp; Lei Ba, J. (2017). ADAM: A METHOD FOR STOCHASTIC
OPTIMIZATION.<br> <a href="https://arxiv.org/pdf/1412.6980.pdf" class="uri">https://arxiv.org/pdf/1412.6980.pdf</a><br></p>
<p>LeCun, Y. (2009). MNIST handwritten digit database, Yann LeCun,
Corinna Cortes and Chris Burges. Lecun.com.<br> <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a><br></p>
<p>Mildenhall, B. (n.d.). Fourier Features Let Networks Learn High
Frequency Functions in Low Dimensional Domains (10min talk)<br> [Review
of Fourier Features Let Networks Learn High Frequency Functions in Low
Dimensional Domains (10min talk)].<br> <a href="https://www.youtube.com/watch?v=iKyIJ_EtSkw&amp;ab_channel=BENMILDENHALL" class="uri">https://www.youtube.com/watch?v=iKyIJ_EtSkw&amp;ab_channel=BENMILDENHALL</a><br></p>
<p>neuralthreads. (2021, December 6). Softmax function — It is
frustrating that everyone talks about it but very few talk about
its….<br> Medium. <a href="https://neuralthreads.medium.com/softmax-function-it-is-frustrating-that-everyone-talks-about-it-but-very-few-talk-about-its-54c90b9d0acd" class="uri">https://neuralthreads.medium.com/softmax-function-it-is-frustrating-that-everyone-talks-about-it-but-very-few-talk-about-its-54c90b9d0acd</a><br></p>
<p>Sagar, A. (n.d.). 5 Techniques to Prevent Overfitting in Neural
Networks<br> [Review of 5 Techniques to Prevent Overfitting in Neural
Networks]. KDnuggets.<br> <a href="https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html" class="uri">https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html</a><br></p>
<p>Taboga, M. (n.d.). Vec operator [Review of Vec operator].
StatLect.<br> <a href="https://www.statlect.com/matrix-algebra/vec-operator" class="uri">https://www.statlect.com/matrix-algebra/vec-operator</a><br></p>
<p>Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S.,
Raghavan, N., Singhal, U.,<br> Ramamoorthi, R., Barron, J. T., &amp; Ng,
R. (2020, June 18).<br> Fourier features let networks learn high
frequency functions in low dimensional domains.<br> arXiv.org. <a href="https://arxiv.org/abs/2006.10739" class="uri">https://arxiv.org/abs/2006.10739</a><br></p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

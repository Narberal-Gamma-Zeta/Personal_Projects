<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Edgar M." />

<meta name="date" content="2023-10-11" />

<title>MNIST Classification-Neural Network from Scratch</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource {
  margin-left: 3em;
  border-left: 1px solid #aaaaaa;
  padding-left: 4px;
  font-family: "Courier New", Courier, monospace;
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.er {
  color: #ff0000;
  font-weight: bold;
  font-family: "Courier New", Courier, monospace;
}
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">MNIST Classification-Neural Network from
Scratch</h1>
<h4 class="author">Edgar M.</h4>
<h4 class="date">10/11/23</h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The purpose of this notebook is to build a feedforward
classification<br> neural networks from scratch. What does from scratch
mean? It means<br> that I will not be using any machine learning
libraries (e.g. keras,<br> tensorflow, pytorch, etc.). I will only be
using the R libraries that<br> are pasted below. Neural network is made
up of input layer, either one or<br> two hidden layers, and output
layer. Feedforward neural network is for<br> MNIST data classification.
The first type of neural network is one with<br> no mappings. Continuing
the derivation. The second type of neural network<br> is with Fourier
feature mapping. Idea for Fourier feature mapping came<br> from the
paper, “Fourier Features Let Networks Learn High Frequency<br> Functions
in Low Dimensional Domains.” As I progress through this<br> notebook. I
will expand on the idea of neural network with no mapping and<br> neural
networks with Fourier feature mapping.<br></p>
<p>My original plan was to include all the derived math within this
notebook.<br> But my full derivation was more than 20 pages long, hence
I will only<br> include the most important parts from my derivation. My
Derivation is<br> longer than other standard derivation lenghts that
I’ve seen, because it<br> is my first classification neural network from
scratch and I did not skip<br> over most of the trivial easy to see
stuff. Mostly for my own sanity. As<br> later in the future. I want to
come back and find it very easy to see<br> what I did, without breaking
my brain. Basic math topics reader should<br> be familiar with are
matrix algebra and multivariable calculus. With all<br> that being said.
Let’s begin :)<br></p>
<p>Load necessary libraries and data.<br></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co">#load nessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co">#suppressMessages() is used to suppress messages from being printed in console</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(magrittr)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">library</span>(dplyr) <span class="sc">%&gt;%</span> <span class="fu">suppressMessages</span>()</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="fu">library</span>(readr) </span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="fu">library</span>(ggplot2)  </span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co">#this code is for loading MNIST data within kaggle</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co">#Data &lt;-read_csv(&quot;/kaggle/input/digit-recognizer/train.csv&quot;) %&gt;% suppressMessages()</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co">#load data from local machine </span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>Data <span class="ot">&lt;-</span><span class="fu">read_csv</span>(<span class="st">&quot;C:/Users/Eva-02/Downloads/Code/Personal Projects/MNIST Classification-Neural Network From Scratch/Data/train.csv&quot;</span>) </span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="co">#Convert Data into a matrix</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>Data <span class="ot">&lt;-</span><span class="fu">as.matrix</span>(Data) </span></code></pre></div>
</div>
<div id="parameter-setup-and-data-preparation-for-image-classification" class="section level2">
<h2>Parameter Setup and Data Preparation for Image Classification</h2>
<div id="image-matrix" class="section level3">
<h3>Image Matrix</h3>
<p>Counting the zeroth pixel as one and the 783th pixel as 784, then
there’s<br> 784 pixels in total. Each pixel is a number between 0 and
255, which<br> represents the intensity of the pixel. “0 means
background (white), 255”<br> “means foreground (black)” (Yann LeCun,
Corinna Cortes, Christopher J.C.<br> Burges, 1998).<br></p>
<p>According to THE MNIST DATABASE of handwritten digits. Each 0 to 9
digit<br> is from a 28 by 28 image matrix.<br></p>
<p><span class="math display">\[ \text{Image Matrix}\ = \begin{bmatrix}
pixel0 &amp; pixel1 &amp; pixel2 &amp;...pixel27 \\
pixel28 &amp; pixel29 &amp; pixel30 &amp;...pixel55\\
pixel56 &amp; pixel57 &amp; pixel58 &amp;...pixel83\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
pixel756 &amp; pixel757 &amp; pixel758 &amp;...pixel783\\
\end{bmatrix} \]</span></p>
<p>You can mathematically convert an image matrix into a row vector
by:<br></p>
<p><span class="math display">\[ Row Image Vector = [Vec((\text{Image
Matrix})^T)]^T \]</span></p>
<p><span class="math display">\[ Row Image Vector = \begin{bmatrix}
pixel0 &amp; pixel1 &amp; pixel2 &amp; pixel3 &amp; ...pixel783 \\
\end{bmatrix} \]</span></p>
<p>The first 5 columns and first 5 rows of the training data are shown
below.</p>
<pre><code>##      label pixel0 pixel1 pixel2 pixel3
## [1,]     1      0      0      0      0
## [2,]     0      0      0      0      0
## [3,]     1      0      0      0      0
## [4,]     4      0      0      0      0
## [5,]     0      0      0      0      0</code></pre>
<p>The last 5 columns and first 5 rows of the training data are shown
below.</p>
<pre><code>##      pixel779 pixel780 pixel781 pixel782 pixel783
## [1,]        0        0        0        0        0
## [2,]        0        0        0        0        0
## [3,]        0        0        0        0        0
## [4,]        0        0        0        0        0
## [5,]        0        0        0        0        0</code></pre>
<p>Each row of the MNIST dataset is as follows:<br> <span class="math display">\[\begin{bmatrix}
label &amp; pixel0 &amp; pixel1 &amp; pixel2 &amp; pixel3 &amp;
...pixel783 \\
\end{bmatrix}\]</span></p>
<p>Label column is the <strong>y</strong> dependent variable, and pixels
are the <strong>x</strong><br> independent variables. For each row we
have:<br></p>
<p><span class="math display">\[\begin{bmatrix}
[y] &amp; [Row Image Vector] \\
\end{bmatrix}\]</span><br></p>
</div>
<div id="parameter-configuration-and-data-splitting-for-training-and-testing" class="section level3">
<h3>Parameter Configuration and Data Splitting for Training and
Testing</h3>
<p>Let Q = number of classification categories, which in this case Q =
10.<br></p>
<p>Let N = Total number of observations, which in this case N =
42000.<br> <span class="math display">\[N = N_{training} + N_{testing}\
= training + testing\]</span> Good rule of thumb for the total number of
observations for training-testing<br> can be set as (%80-%20), (%90-%10)
or some other percentage.<br></p>
<p><span class="math display">\[N_{training} =N*percent\]</span> Total
number of observations for testing.<br> <span class="math display">\[N_{testing} =N-N_{training}\]</span></p>
<p>Let: <br> <span class="math display">\[n \in N_{training} \]</span>
Stochastic gradient descent (1 observation)<br> <span class="math display">\[n = 1 \]</span> Mini-batch gradient descent (more
than one, but less than all training observations).<br> <span class="math display">\[1 &lt; n &lt; N_{training} \]</span> Batch
gradient descent (all training observations).<br> <span class="math display">\[n = N_{training} \]</span></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co">#Number of classification categories</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>Q <span class="ot">&lt;-</span><span class="dv">10</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">#Total number of observations</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>N <span class="ot">&lt;-</span><span class="fu">nrow</span>(Data)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co">#Percent of training observations</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>Percent <span class="ot">&lt;-</span><span class="fl">0.9</span>  </span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co">#Training observations</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>N_training <span class="ot">&lt;-</span>N<span class="sc">*</span>Percent</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="co">#Testing observations</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>N_testing <span class="ot">&lt;-</span>N<span class="sc">-</span>N_training</span></code></pre></div>
<p>Separate training and testing data.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Randomly select N_training observations from the total N observations.</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="co"># Randomly select N_training rows</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># replace = TRUE means that the same row can be selected more than once.</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># replace = FALSE means that the same row cannot be selected more than once.</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>Randomly_selected_rows <span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">nrow</span>(Data), N_training, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co"># Create a new matrix with the selected rows</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>Data_training <span class="ot">&lt;-</span>Data[Randomly_selected_rows, ]</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="co"># Get the remaining rows</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>remaining_rows <span class="ot">&lt;-</span><span class="fu">setdiff</span>(<span class="fu">seq_len</span>(<span class="fu">nrow</span>(Data)), Randomly_selected_rows)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="co"># Create a new matrix with the remaining rows</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>Data_testing <span class="ot">&lt;-</span>Data[remaining_rows, ]</span></code></pre></div>
<p>Let’s separate the <strong>y</strong> column and the <strong>image
vector columns</strong>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co">#Training data</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>Y_Labels_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">1</span>]  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_training)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="co">#Testing data</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>Y_Labels_testing <span class="ot">&lt;-</span>Data_testing[, <span class="dv">1</span>]  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>Row_Image_Matrix_testing <span class="ot">&lt;-</span>Data_testing[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_testing)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span></code></pre></div>
</div>
<div id="scale-pixel-input-data" class="section level3">
<h3>Scale Pixel Input Data</h3>
<p>Why does the pixel input data need to be scaled? One reason is to
prevent<br> numerical overflow for softmax function. The softmax
function is&lt; used to<br> convert the output of the neural network
into a probability distribution.<br> The softmax function becomes
numerically unstable with very large values.<br> Thousands of inputs for
softmax quickly explodes softmax output value.<br> Scaled input values
helps to prevent numerical overflow for softmax.<br> Another reason is
to redude the number of iterations needed for the<br> neural network
convergence. The neural network converges faster with<br> scaled input
data.</p>
<p>After splitting the data into training and testing data. We need to
scale<br> the data. Calculate statistical values from training data
(e.g. mean,<br> standard deviation, min-max values, etc.). Then use
these values to scale<br> training and testing data with the same
statistical values. This is to<br> prevent data leakage from testing
data to training data. In other words.<br> We obtain consistency and
reliable performance evaluations for the model.<br></p>
<div id="generalized-min-max-normalization." class="section level4">
<h4>Generalized min-max normalization.</h4>
<p>We can scale between any two numbers with the general min-max<br>
normalization formula. Input data is between lower and upper bounds.
0<br> and 1 are the most common used lower and upper bounds. The
generalized<br> min-max normalization formula is defined as
follows:<br></p>
<p><span class="math display">\[ Rescale = lowerbound + \frac{[(i -
i_{min})*(upperbound - lowerbound)]}{i_{range}}  \]</span></p>
<p>Pixels values <span class="math inline">\(i\)</span> are between 0
and 255.<br> Rescaled pixel values can be between any number (e.g. 0 and
1).<br></p>
</div>
<div id="standardized-z-score-normalization." class="section level4">
<h4>Standardized z-score normalization.</h4>
<p>Another way to scale the pixel input data is to use the
standardized<br> z-score normalization formula. The standardized z-score
normalization<br> formula is defined as follows:<br></p>
<p><span class="math display">\[ Rescale = \frac{i - \mu}{\sigma}
\]</span></p>
<p>Where:<br></p>
<p><span class="math display">\[ i = \text{pixel value}
\]</span><br></p>
<p><span class="math display">\[ \mu = \text{mean of pixel values}
\]</span><br></p>
<p><span class="math display">\[ \sigma = \text{standard deviation of
pixel values} \]</span><br></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Set the scale_var to 0 for generalized min-max normalization.</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># Set the scale_var to 1 for standardized z-score normalization.</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>scale_var <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="cf">if</span> (scale_var <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># Generalized min-max normalization.</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>i_min <span class="ot">&lt;-</span> <span class="fu">min</span>(Row_Image_Matrix_training)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>i_range <span class="ot">&lt;-</span> <span class="fu">max</span>(Row_Image_Matrix_training) <span class="sc">-</span> i_min</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>lower_bound <span class="ot">&lt;-</span> <span class="dv">0</span>  <span class="co"># set your desired lower bound</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>upper_bound <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># set your desired upper bound</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a><span class="co">#Rescale training matrix.</span></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span>lower_bound <span class="sc">+</span> (((Row_Image_Matrix_training <span class="sc">-</span> i_min) <span class="sc">*</span> (upper_bound <span class="sc">-</span> lower_bound)) <span class="sc">/</span> i_range)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a><span class="co">#Rescale test matrix.</span></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a><span class="co"># Use the same statistical values from training data to scale testing data.</span></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>Row_Image_Matrix_testing <span class="ot">&lt;-</span>lower_bound <span class="sc">+</span> (((Row_Image_Matrix_testing <span class="sc">-</span> i_min) <span class="sc">*</span> (upper_bound <span class="sc">-</span> lower_bound)) <span class="sc">/</span> i_range)</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a><span class="co"># Standardized z-score normalization.</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a><span class="do">##############################</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a><span class="co"># Rescale training matrix.</span></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a><span class="co"># mean of training data</span></span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>mean_training <span class="ot">&lt;-</span><span class="fu">mean</span>(Row_Image_Matrix_training)</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a><span class="co"># Calculate the sample standard deviation </span></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>sd_training <span class="ot">&lt;-</span> <span class="fu">sd</span>(Row_Image_Matrix_training)</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a><span class="co">#Rescale training matrix.</span></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a><span class="co"># Standardize training data using the sample standard deviation and mean</span></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span> (Row_Image_Matrix_training <span class="sc">-</span> mean_training) <span class="sc">/</span> sd_training</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a><span class="co"># Rescale testing matrix.</span></span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a><span class="co"># Rescale test matrix.</span></span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a><span class="co"># Use the same statistical values from training data to scale testing data.</span></span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>Row_Image_Matrix_testing <span class="ot">&lt;-</span> (Row_Image_Matrix_testing <span class="sc">-</span> mean_training) <span class="sc">/</span> sd_training</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>}</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a><span class="co">#Scaled Training/Testing data.</span></span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a><span class="co"># Bind Y_Labels_training and Row_Image_Matrix_training </span></span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a>Data_training_scaled <span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="fu">t</span>(Y_Labels_training), Row_Image_Matrix_training)  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a><span class="co"># Bind Y_Labels_testing and Row_Image_Matrix_testing</span></span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>Data_testing_scaled <span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="fu">t</span>(Y_Labels_testing), Row_Image_Matrix_testing)  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span></code></pre></div>
</div>
</div>
</div>
<div id="feedforward" class="section level2">
<h2>Feedforward</h2>
<div id="no-mapping-in-neural-networks" class="section level3">
<h3>No Mapping in Neural Networks</h3>
<p>The M weight matrix is a matrix of random numbers. The multiplication
of<br> the M and X matrix creates planes in <strong>d</strong>
dimensions. A neural network is<br> nothing more than the optimization
of planes in <strong>d</strong> dimensions. The<br> planes in
<strong>d</strong> dimensions cross through some point
<strong>b</strong> on the <strong>y-axis</strong>.<br> What does no
mapping mean? It means that input data is not transformed by<br> any
function. In other words, the input X matrix is simply.<br></p>
<p><span class="math display">\[X_{i} = X_{d} \]</span><br></p>
<p>Whereas neural networks with Fourier Feature mapping, the input X
matrix<br> is transformed by some function. In other words, the input X
matrix is.<br></p>
<p><span class="math display">\[X_{i} = \gamma(X_{d})\]</span><br></p>
<p>Note that <span class="math inline">\(X_{i}\)</span> is shorthand for
<span class="math inline">\(X_{input}\)</span> and <span class="math inline">\(X_{d}\)</span> is shorthand for <span class="math inline">\(X_{data}\)</span>.<br></p>
</div>
<div id="fourier-feature-mapping-in-neural-networks" class="section level3">
<h3>Fourier Feature Mapping in Neural Networks</h3>
<p>Fourier feature mapping means that the neural network is trained on
the<br> scaled pixel values that are first passed through a Fourier
transform.<br> The Fourier series is defined as:<br></p>
<p><span class="math display">\[f(x) = a_{0} + \sum_{n=1}^{\infty}
\left[ a_{n}\cos\left(\frac{2\pi nx}{f}\right) +
b_{n}\sin\left(\frac{2\pi nx}{f}\right) \right] \]</span></p>
<p>where:<br></p>
<p><span class="math display">\[f = \text{frequency} \]</span></p>
<p><span class="math display">\[H = 1\ 2\ 3\ 4\ \ldots\ n =
\text{harmonic numbers}\]</span></p>
<p>I will build up to the Fourier series. Not the most
mathematically<br> rigorous explanation, but it will do. First define
the input vector <span class="math inline">\((X_{d})_{v}\)</span><br> as
a single observation. The input vector is size 1 x n. For Mnist<br>
dataset, input vector is size 1 x 784. Next, pass the input vector
through<br> an alternating sine cos function <span class="math inline">\(\gamma(X_{d})_{v}\)</span>.<br></p>
<p><span class="math display">\[\gamma(X_{d})_{v} = \cos\left(\frac{\pi
x_{1d}}{f_{1}}\right) + \sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  +
\cdots + \cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right) \]</span></p>
<p>Pass <span class="math inline">\(\gamma(X_{d})_{v}\)</span> matrix
through the bias operator.<br></p>
<p><span class="math display">\[\gamma(X_{d})_{v}
\underset{bias}{\rightarrow} X1_{v} = 1 + \cos\left(\frac{\pi
x_{1d}}{f_{1}}\right) + \sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  +
\cdots + \cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right) \]</span></p>
<p>Cos and sine map onto the x values for the X matrix. Note that <span class="math inline">\(x_{b} = 1\)</span>,<br> which represents the bias
node.<br></p>
<p><span class="math display">\[X1_{v}=1 + \cos\left(\frac{\pi
x_{1d}}{f_{1}}\right) + \sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  +
\cdots + \cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right) \]</span></p>
<p><span class="math display">\[X1_{v}= 1 + x_{1} + x_{2} + \cdots +
x_{n} \]</span></p>
<p>Multiply M weight vector by X1 vector.<br></p>
<p><span class="math display">\[M_{v}X1_{v} =w_{b}(1) +
w_{1}\cos\left(\frac{\pi x_{1d}}{f_{1}}\right) +
w_{2}\sin\left(\frac{2\pi x_{2d}}{f_{2}}\right)  + \cdots +
w_{n}\cos\left(\frac{\pi nx_{nd}}{f_{n}}\right) +
w_{n}\sin\left(\frac{\pi nx_{nd}}{f_{n}}\right)  \]</span></p>
<p><span class="math display">\[M_{v}X1_{v} = w_{b}(1) + w_{1}x_{1} +
w_{2}x_{2} + \cdots + w_{n}x_{n} \]</span></p>
<p><span class="math inline">\(M_{v}X1_{v}\)</span> can be seen as a
Fourier series. <span class="math inline">\(w_{cos}\)</span> are weights
that<br> correspond to the cosine terms. <span class="math inline">\(w_{sin}\)</span> are weights that correspond
to<br> the sine terms. <span class="math inline">\(w_{b}\)</span> is the
bias weight. The Fourier series can be<br> written as:<br></p>
<p><span class="math display">\[M_{v}X1_{v} = w_{b}(1) +
\sum_{n=1}^{\infty} \left[ w_{cos}\cos\left(\frac{2\pi nx}{f}\right) +
w_{sin}\sin\left(\frac{2\pi nx}{f}\right) \right] \]</span></p>
<p>In the papaer, “Fourier Features Let Networks Learn High
Frequency<br> Functions in Low Dimensional Domains” by Tancik et al. The
authors<br> recommend setting the frequency as ramdom points from a
Gaussian<br> distribution <span class="math inline">\(\mathcal{N}(0,\sigma^{2})\)</span>. What does
ramdomly sampling points from<br> a Gaussian distribution mean? Let’s
say that <span class="math inline">\(\sigma = 1\)</span>, which is about
68%<br> of the area under the curve. Then at <span class="math inline">\(\sigma = 1\)</span> means that points are<br>
randomly sampled from about 68% area under the curve. Let’s say that
<span class="math inline">\(\sigma = 2\)</span>,<br> which means that
points are randomly sampled from about 95% of the area<br> under the
curve. So on and so forth. The higher the sigma value, then<br> there’s
more area under the curve that can be randomly sampled for the<br> B
vector.<br></p>
<p><span class="math display">\[f = B = \text{randomly sampled points
from a Gaussian distribution} \]</span></p>
<p>Thus applying H (harmonic numbers) and B to <span class="math inline">\(\gamma(X_{d})_{v}\)</span>, you get<br> fourier
feature mapping.<br></p>
<p><span class="math display">\[X_{i} = \gamma(X_{d})_{v} =  \left[
\cos\left({H\pi B x}\right), \sin\left({H\pi B x}\right) \right]^T
\]</span></p>
</div>
<div id="x-input-matrix-preparation" class="section level3">
<h3>X Input Matrix Preparation</h3>
<p>For no mapping use regular pixel values. For Fourier feature mapping
use<br> Fourier series scaled pixel values. X training matrix for batch
gradient<br> descent is simply the transpose of the image
matrix.<br></p>
<p><span class="math display">\[ X1_{training} = Transpose(Image Matrix)
= \begin{bmatrix}
pixel0 &amp; pixel28 &amp; pixel56 &amp; \cdots &amp; pixel756 \\
pixel1 &amp; pixel29 &amp; pixel57 &amp; \cdots &amp; pixel757 \\
pixel2 &amp; pixel30 &amp; pixel58 &amp; \cdots &amp; pixel758 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
pixel27 &amp; pixel55 &amp; pixel83 &amp; \cdots &amp; pixel783 \\
\end{bmatrix} \]</span></p>
<p>Note that observation is used as a synonym for training example.
For<br> stochastic gradient descent. We randomly select 1 observation
from<br> training data. Then transpose to make each column an
observation. For<br> mini-batch gradient descent, we randomly select
some n random observations<br> from training data. Then transpose the
resulting matrix to make each<br> column an observation.<br></p>
<p>For example, let’s say we randomly select 3 observations (n=3)
from<br> training data. Then transpose to make each column as an
observation.<br></p>
<p><span class="math display">\[ X1_{training} = \begin{bmatrix}
observation24147 &amp; observation587 &amp; observation13371\\
\end{bmatrix}  =\]</span></p>
<p><span class="math display">\[ Transpose(Image Matrix) =
\begin{bmatrix}
pixel0 &amp; pixel28 &amp; pixel56 \\
pixel1 &amp; pixel29 &amp; pixel57 \\
pixel2 &amp; pixel30 &amp; pixel58 \\
\vdots &amp; \vdots &amp; \vdots \\
pixel27 &amp; pixel55 &amp; pixel83 \\
\end{bmatrix} \]</span></p>
<p>Note that I set the columns of the X1 matrix to be observations. One
can<br> likewise set the rows of the X1 matrix to be observations. It
doesn’t<br> matter if the rows or columns are set as observations. The
important<br> thing is to be consistent throughout the the mathematical
derivation and<br> the implemented code.<br></p>
</div>
<div id="simplification-of-weight-and-input-matrice" class="section level3">
<h3>Simplification of Weight and Input Matrice</h3>
<p>With every layer with an activation function, the none bias input
X<br> matrix is multiplied by the none bias weight matrix. Then the bias
input<br> X matrix is multiplied by the bias weight matrix. Then the two
resulting<br> matrices are added together. Here’s a question, can we
simplify the<br> multiplication of the weight and input matrices? The
answer is yes.<br> Before passing through the LeakyReLU activation
function. The weight<br> matrix and input matrices can be simplified as
follows:<br></p>
<p><span class="math display">\[m_{m \; x \; n} * x_{n \; x \; p} +
m_{bias, m \; x \; 1 } * x_{bias, 1 \; x \; p} = MX_{m \; x \; p}
\]</span></p>
<p>For example: <span class="math display">\[ \begin{bmatrix}
w1 &amp; w2 \\
w3 &amp; w4\\
\end{bmatrix}_{m \; x \; n} *  \begin{bmatrix}
x1 &amp; x3 \\
x2 &amp; x4\\
\end{bmatrix}_{n \; x \; p} + \begin{bmatrix}
wb1 \\
wb2 \\
\end{bmatrix}_{m \; x \; 1} *  \begin{bmatrix}
xb1 &amp; xb2 \\
\end{bmatrix}_{1 \; x \; p} = \]</span></p>
<p><span class="math display">\[\left[\begin{array}{cc}
w1 &amp; w2 \\
w3 &amp; w4 \\
\end{array}\right.
\left.\begin{array}{c}
\begin{bmatrix}
wb1 \\
wb2 \\
\end{bmatrix}
\end{array}\right]_{m \; x \; (n+1)}
*  
\begin{bmatrix}
x1 &amp; x3\\
x2 &amp; x4\\
[xb1 &amp; xb2]\\
\end{bmatrix}_{(n+1) \; x \; p} \]</span></p>
<p>Where: <span class="math display">\[\begin{bmatrix}xb1 &amp; xb2 = 1
&amp; 1 \end{bmatrix} \]</span> Thus:</p>
<p><span class="math display">\[ \left[\begin{array}{cc}
w1 &amp; w2 \\
w3 &amp; w4 \\
\end{array}\right.
\left.\begin{array}{c}
\begin{bmatrix}
wb1 \\
wb2 \\
\end{bmatrix}
\end{array}\right]_{m \; x \; (n+1)}
*  
\begin{bmatrix}
x1 &amp; x3\\
x2 &amp; x4\\
[1 &amp; 1]\\
\end{bmatrix}_{(n+1) \; x \; p} = MX_{m \; x \; p} \]</span></p>
<p>Define the bias operator as adding a row of ones to the input
matrix.<br></p>
<p><span class="math display">\[x_{n \; x \; p}
\underset{bias}{\rightarrow} X_{(n + 1) \; x \; p} \]</span></p>
<p>As the previous example shows: <span class="math display">\[\begin{bmatrix}
x1 &amp; x3 \\
x2 &amp; x4\\
\end{bmatrix}_{n \; x \; p} \underset{bias}{\rightarrow} \begin{bmatrix}
x1 &amp; x3\\
x2 &amp; x4\\
[1 &amp; 1]\\
\end{bmatrix}_{(n+1) \; x \; p} \]</span></p>
<p>Therefore, instead of doing two operations. That is to say.
Multiplying<br> bias matrix and non-bias matrix by their corresponding
input matrices,<br> then adding the multiplied matrices. Both weight
matrix and input matrix<br> can be combined into one weight and one
input matrix. Thus, simplifying<br> the computation into one
operation.<br></p>
</div>
<div id="leakyrelu-activation-function" class="section level3">
<h3>LeakyReLU Activation Function</h3>
<p>Activation functions in the hidden layers are used to introduce
non-linearity<br> into the neural network. LeakyReLU was chosen because
of it’s simplicity<br> and it’s ability to prevent the dying ReLU
problem. The dying ReLU problem<br> is when the ReLU activation function
outputs 0 for all negative inputs.<br> Thus, the gradient becomes 0 and
the network cannot perform backpropagation.<br> LeakyReLU solves this
problem by having a small positive slope for negative<br> inputs. Note
that setting alpha to 0 makes LeakyReLU the same as ReLU.<br> Hence
LeakyReLU is the more general form of ReLU. The LeakyReLU activation<br>
function is simply a piecewise function which is defined as
follows:<br></p>
<p><span class="math display">\[LeakyReLU(x) =
\begin{cases}
\alpha*x &amp; \text{if } x &lt; 0  \\
x &amp; \text{if } x \geq 0
\end{cases} \]</span><br></p>
<p>Plot for LeakyReLU.<br> <img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="softmax-activation-function" class="section level3">
<h3>Softmax Activation Function</h3>
<p>The softmax activation function is used to convert the output of
the<br> neural network into a probability distribution. Softmax
essentially<br> predicts the probability that an observation belongs to
a particular<br> class. The softmax function is defined as
follows:<br></p>
<p><span class="math display">\[Softmax(z) =
\frac{e^{z_{i}}}{\sum_{j=1}^{Q} e^{z_{j}}} \]</span></p>
<p>As previously stated:<br> <span class="math display">\[NumberClasses  = Q = 10 \]</span></p>
<p>For better numerical stability, the softmax function is modified as
follows:<br></p>
<p><span class="math display">\[Softmax(z)_{Max}  = \frac{e^{z_{i} -
max(z)}}{\sum_{j=1}^{Q} e^{z_{j} - max(z)}} \]</span></p>
<p>Taking log of both sides:<br></p>
<p><span class="math display">\[ln[Softmax(z)_{Max}] =
ln\left(\frac{e^{z_{i} - max(z)}}{\sum_{j=1}^{Q} e^{z_{j} -
max(z)}}\right) \]</span></p>
<p>Apply the log identity to the softmax function:<br></p>
<p><span class="math display">\[ln(\frac{a}{b}) =ln(a) -
ln(b)  \]</span></p>
<p><span class="math display">\[ln(Softmax(z)_{Max}) =  ln\left[e^{z_{i}
- max(z)}\right] - ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right]
\]</span></p>
<p><span class="math display">\[ln(Softmax(z)_{Max}) =  z_{i} - max(z) -
ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right] \]</span></p>
<p><span class="math display">\[e^{ln(Softmax(z)_{Max})}  = e^{z_{i} -
\max(z) - ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right]}
\]</span></p>
<p>After some algebraic manipulation, the final version of the softmax
function is redefined<br> as follows:<br></p>
<p><span class="math display">\[Softmax(z)_{Max}  = e^{z_{i} - \max(z) -
ln\left[\sum_{j=1}^{Q} e^{z_{j} - max(z)}\right]} \]</span></p>
</div>
<div id="one-hot-encoding-matrix" class="section level3">
<h3>One Hot Encoding Matrix</h3>
<p>The output of the NN is a vector of 10 probabilities, each
probability<br> represents the probability of the image being a digit
from 0 to 9. But<br> Y_Labels is not a vector of probabilities, it’s a
vector of digits. Hence,<br> Y_Labels needs to be converted into a
vector of probabilities. This is<br> done by using one-hot
encoding.<br></p>
<p>Since I set the columns of the X1 matrix to be observations. Then
each<br> column of the one hot encoding matrix is also an observation.
The one hot<br> encoding matrix is a matrix of zeros with only a single
1 in each column.<br> Mathematically you can think of each column as a
basis vector. In other<br> words, each column is a vector of zeros with
only a single 1. The position<br> of the 1 represents the class. Thus
each column represents an observation.<br> Neural network is trained to
classify images of digits from 0 to 9. Thus<br> row 1 represents digit
0, row 2 represents digit 1, row 3 represents<br> digit 2, so on and so
forth. Example of one hot encoding matrix for Q=10<br> classes and n=5
observations is as follows:<br></p>
<p><span class="math display">\[Y_{Q \; x \; n} = \text{One Hot Encoding
Matrix} \]</span><br></p>
<p><span class="math display">\[Y_{Q \; x \; n} = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix} \]</span><br></p>
<p>One Hot Encoding Training Data<br></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Initialize a list to store the basis vectors</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>basis_vectors_list_training <span class="ot">&lt;-</span><span class="fu">list</span>()</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># Loop over each column of the matrix</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_training) {</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>  <span class="co"># Get the label for the q-th column</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>  label <span class="ot">&lt;-</span> Y_Labels_training[<span class="dv">1</span>, q] <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  </span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>  <span class="co"># Create a vector of zeros with length Q</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>  basis_vectors_training <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, Q)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>  </span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>  <span class="co"># Set the (label+1)-th element to 1</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>  basis_vectors_training[label <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>  </span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>  <span class="co"># Add the basis vector to the list</span></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>  basis_vectors_list_training[[q]] <span class="ot">&lt;-</span>basis_vectors_training <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>}</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a><span class="co"># Convert the list of basis vectors to a matrix</span></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>Y_One_Hot_Encoding_training <span class="ot">&lt;-</span> <span class="fu">do.call</span>(cbind, basis_vectors_list_training)</span></code></pre></div>
<p>One Hot Encoding Testing Data<br></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Initialize a list to store the basis vectors</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>basis_vectors_list_testing <span class="ot">&lt;-</span><span class="fu">list</span>()</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co"># Loop over each column of the matrix</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_testing) {</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>  <span class="co"># Get the label for the q-th column</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>  label <span class="ot">&lt;-</span>Y_Labels_testing[<span class="dv">1</span>, q] <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>  </span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>  <span class="co"># Create a vector of zeros with length Q</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>  basis_vectors_testing <span class="ot">&lt;-</span><span class="fu">rep</span>(<span class="dv">0</span>, Q)</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>  </span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>  <span class="co"># Set the (label+1)-th element to 1</span></span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>  basis_vectors_testing[label <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>  </span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>  <span class="co"># Add the basis vector to the list</span></span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>  basis_vectors_list_testing[[q]] <span class="ot">&lt;-</span>basis_vectors_testing <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>}</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a><span class="co"># Convert the list of basis vectors to a matrix</span></span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>Y_One_Hot_Encoding_testing <span class="ot">&lt;-</span> <span class="fu">do.call</span>(cbind, basis_vectors_list_testing)</span></code></pre></div>
</div>
<div id="categorical-cross-entropy-loss-function" class="section level3">
<h3>Categorical Cross Entropy Loss Function</h3>
<p>The output from the softmax function is passed to the categorical
cross<br> entropy loss function. The purpose of the categorical cross
entropy loss<br> function is to measure the difference between the
predicted probabilities<br> and the actual probabilities. Note that the
hollow circle means hadamard<br> product (element-wise multiplication).
After the hadamard product, the<br> categorical cross entropy loss
function is summed across <strong>Q</strong> classes<br> and then summed
over <strong>n</strong> observations. Finally, the sum is divided by<br>
the number of observations. The mean categorical cross entropy loss<br>
function is defined as follows:<br></p>
<p><span class="math display">\[C_{mean} = -\frac{1}{n} \sum^{n}
\sum^{Q} [Y_{Q \; x \; n} \circ \ln(X_{out})_{Q \; x \; n}]
\]</span><br></p>
<p><span class="math display">\[C_{mean} = -mean\left[ \sum^{Q} [Y_{Q \;
x \; n} \circ \ln(X_{out})_{Q \; x \; n}] \right] \]</span><br></p>
<p>Cost function for each observation is a vector.<br> <span class="math display">\[C_{vector} = C_{v} = -[y_{v} \circ
\ln(x_{out})_{v}] \]</span><br></p>
<p>All cost function vectors make up the cost function matrix.<br> <span class="math display">\[C_{Matrix} = C_{M} = -Y_{Q \; x \; n} \circ
\ln(X_{out})_{Q \; x \; n} \]</span></p>
</div>
<div id="feedforward-summary" class="section level3">
<h3>Feedforward Summary</h3>
<p><span class="math display">\[NumberPixels  = 784 \]</span><br> <span class="math display">\[NumberObservations  = n \]</span><br> <span class="math display">\[NumberClasses  = Q = 10 \]</span> <br></p>
<div id="when-using-one-hidden-layer" class="section level4">
<h4>When Using One Hidden Layer</h4>
<p><span class="math display">\[\text{Number Hidden Neurons}= nu
\]</span> <br></p>
<p><span class="math display">\[Xi_{784 \; x \;
n}  \underset{bias}{\rightarrow}  (X1)_{(784+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M1)_{nu \; x \; 785} *
(X1)_{785 \; x \; n}  = (Z2)_{nu \; x \; n} \]</span><br> <span class="math display">\[LR[(Z2)]_{nu \; x \;
n}  \underset{bias}{\rightarrow}  (X2)_{(nu+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M2)_{Q \; x \; (nu+1)}
* (X2)_{(nu+1) \; x \; n}  = (Z3)_{Q \; x \; n} \]</span><br> <span class="math display">\[(Z3)_{Q \; x \; n}  = (Z_{out})_{Q \; x \;
n}\]</span><br></p>
</div>
<div id="when-using-two-hidden-layers" class="section level4">
<h4>When Using Two Hidden Layers</h4>
<p><span class="math display">\[\text{Number Hidden Neurons 1st layer }=
nu \]</span> <br> <span class="math display">\[\text{Number Hidden
Neurons 2nd layer }= nu2 \]</span> <br></p>
<p><span class="math display">\[Xi_{784 \; x \;
n}  \underset{bias}{\rightarrow}  (X1)_{(784+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M1)_{nu \; x \; 785} *
(X1)_{785 \; x \; n}  = (Z2)_{nu \; x \; n} \]</span><br> <span class="math display">\[LR[(Z2)]_{nu \; x \;
n}  \underset{bias}{\rightarrow}  (X2)_{(nu+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M2)_{nu2 \; x \;
(nu+1)} * (X2)_{(nu+1) \; x \; n}  = (Z3)_{nu2 \; x \; n} \]</span><br>
<span class="math display">\[LR[(Z3)]_{nu2 \; x \;
n}  \underset{bias}{\rightarrow}  (X3)_{(nu2+1) \; x \;
n}   \]</span><br> <span class="math display">\[(M3)_{Q \; x \; (nu2+1)}
* (X3)_{(nu2+1) \; x \; n}  = (Z4)_{Q \; x \; n} \]</span><br> <span class="math display">\[(Z4)_{Q \; x \; n}  = (Z_{out})_{Q \; x \;
n}\]</span><br></p>
</div>
<div id="output-layer" class="section level4">
<h4>Output Layer</h4>
<p><span class="math display">\[[Softmax(Z_{out})_{Max}]_{Q \; x \;
n}  = (X_{out})_{Q \; x \; n} \]</span><br></p>
<p><span class="math display">\[C_{Matrix} = C_{M} = -Y_{Q \; x \; n}
\circ \ln(X_{out})_{Q \; x \; n} \]</span><br> <span class="math display">\[C_{mean} = -mean\left[ \sum^{Q} [Y_{Q \; x \; n}
\circ \ln(X_{out})_{Q \; x \; n}] \right] \]</span><br></p>
</div>
</div>
</div>
<div id="backpropagation" class="section level2">
<h2>Backpropagation</h2>
<p>This is where the magic happens. Backpropagation is where the
neural<br> network learns. The neural network learns by minimizing the
cost function.<br> The cost function is minimized by calculating the
gradient of the cost<br> function with respect to the M weight matrices.
The gradient of the cost<br> function is calculated by using the chain
rule. The chain rule is used to<br> calculate the partial derivatives of
the cost function with respect to<br> the M weight matrices.
Backpropagation is nothing more than finding the<br> partial derivative
of the cost function with respect to the M weight<br> matrices.I did
this via the partial derivative tree diagram. The partial<br> derivative
tree diagram is a visual representation of the chain rule.<br></p>
<div id="one-hidden-layer" class="section level3">
<h3>One Hidden Layer</h3>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m2-weight-matrix" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M2 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M2} = \left(
\frac{\partial C^T}{\partial X3} * \frac{\partial X3^T}{\partial Z3}
\right)^T * \frac{\partial Z3}{\partial M2} \]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\left(\frac{\partial C^T}{\partial X3} *
\frac{\partial X3^T}{\partial Z3}\right)^T = (X3 - Y)_{Q \; x \; n}
\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial M2} =
(X2^T)_{n \; x \; (nu +1)} \]</span><br></p>
<p>thus:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M2} = (X3 -
Y)_{Q \; x \; n} * (X2^T)_{n \; x \; (nu+1)} \]</span><br></p>
<p>The matrix (X3 - Y) is obtained by the matrix multiplication of the
softmax<br> Jabobian matrix with the derivative of the C cost vector
with respect to<br> the output softmax vector.<br></p>
<p>For sanity check, the size of the M2 weight matrix must equal the
size of<br> the derivative of the C cost matrix with respect to the M2
weight matrix.<br></p>
<p>Size of M2 matrix is:<br></p>
<p><span class="math display">\[(M2)_{size}= Q \; x \;
(nu+1)\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M2}_{size} =
(Q \; x \; n) * (n \; x \; (nu+1))\]</span><br> <span class="math display">\[\frac{\partial C}{\partial M2}_{size} = Q \; x \;
(nu+1)\]</span><br></p>
<p><span class="math display">\[(M2)_{size} = \frac{\partial C}{\partial
M2}_{size}\]</span><br></p>
</div>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m1-weight-matrix" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M1 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M1} = \left[
\left( \left( \frac{\partial C^T}{\partial X3} * \frac{\partial
X3^T}{\partial Z3} \right) * \frac{\partial Z3}{\partial X2} \right)^T
\circ \frac{\partial X2}{\partial Z2} \right] * \frac{\partial
Z2}{\partial M1}\]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\left( \frac{\partial C^T}{\partial X3}
* \frac{\partial X3^T}{\partial Z3} \right) = (X3 - Y)^T_{n \; x \; Q}
\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial X2}=
(M2_{-b})_{Q \; x \; nu} \]</span> <br></p>
<p>The subscript -b means that the bias column is removed from the M2
weight<br> matrix. Note that leaky ReLU LR(z) = x2. The bias weights are
multiplied<br> by 1, hence there is no LR(z) activation function in the
bias node. Thus<br> when you take the derivative of the Z3 matrix with
respect to the X2<br> matrix, the bias column is removed because the
bias node has no x2<br> activation function. The derivative dZ3/dX2= 0
for the bias column.<br></p>
<p>You also have:<br></p>
<p><span class="math display">\[\frac{\partial X2}{\partial Z2} =
{[\partial LR(X2_{-b})}]_{nu \; x \; n}\]</span><br></p>
<p>Similarly, the subscript -b means that the bias row is removed from
the<br> X2 matrix. The bias node has only ones, which are not dependent
on previous<br> weights. Thus, when you take the derivative of the X2
matrix with respect<br> to the Z2 matrix, the bias row is removed
because the bias node is not<br> connected to previous nodes. The
derivative dX2/dZ2= 0 for the bias row.<br></p>
<p>You finally have:<br></p>
<p><span class="math display">\[\frac{\partial Z2}{\partial M1}= X1^T_{n
\; x \; 785} \]</span><br></p>
<p>Thus you have:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1} = [[(X3 -
Y)^T * M2_{-b}]^T \circ \partial LR(X2_{-b})] * X1^T \]</span><br></p>
<p>Again for sanity check, the size of the M1 weight matrix must equal
the<br> size of the derivative of the C cost matrix with respect to the
M1 weight<br></p>
<p>Size of M1 matrix is:<br></p>
<p><span class="math display">\[(M1)_{size}= nu \; x \;
785\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1}_{size} =
[[(n \; x \; Q)*(Q \; x \; nu)]^T \circ (nu \; x \; n)] * (n \; x \;
785)\]</span> <span class="math display">\[                               = [(n \; x \;
nu)^T \circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = [(nu \; x \; n)
\circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = (nu \; x \; n) *
(n \; x \; 785)\]</span> <span class="math display">\[\frac{\partial
C}{\partial M1}_{size} = nu \; x \; 785\]</span></p>
<p><span class="math display">\[(M1)_{size} = \frac{\partial C}{\partial
M1}_{size}\]</span><br></p>
</div>
</div>
<div id="two-hidden-layers" class="section level3">
<h3>Two Hidden Layers</h3>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m3-weight-matrix" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M3 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M3} = \left(
\frac{\partial C^T}{\partial X4} * \frac{\partial X4^T}{\partial Z4}
\right)^T * \frac{\partial Z4}{\partial M3} \]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M3} = (X4 -
Y)_{Q \; x \; n} * (X3)_{n \; x \; (nu2+1)}^T \]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\left(\frac{\partial C}{\partial
Z4}\right)^T = (X4 - Y)_{Q \; x \; n} \]</span><br></p>
<p>For sanity check, the size of the M3 weight matrix must equal the
size of<br> the derivative of the C cost matrix with respect to the M3
weight matrix.<br></p>
<p>Size of M3 matrix is:<br></p>
<p><span class="math display">\[(M3)_{size}= Q \; x \;
(nu2+1)\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M3}_{size} =
(Q \; x \; n) * (n \; x \; (nu2+1))\]</span><br> <span class="math display">\[\frac{\partial C}{\partial M3}_{size} = Q \; x \;
(nu2+1)\]</span><br></p>
<p><span class="math display">\[(M3)_{size} = \frac{\partial C}{\partial
M2}_{size}\]</span><br></p>
</div>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m2-weight-matrix-1" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M2 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M2} = \left[
\left( \left( \frac{\partial C^T}{\partial X4} * \frac{\partial
X4^T}{\partial Z4} \right) * \frac{\partial Z4}{\partial X3} \right)^T
\circ \frac{\partial X3}{\partial Z3} \right] * \frac{\partial
Z3}{\partial M2}\]</span><br> <span class="math display">\[\frac{\partial C}{\partial M2} = \left[ \left(
\left(\frac{\partial C}{\partial Z4}\right) * \frac{\partial
Z4}{\partial X3} \right)^T \circ \frac{\partial X3}{\partial Z3} \right]
* \frac{\partial Z3}{\partial M2}\]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial Z3} = \left(
\left(\frac{\partial C}{\partial Z4}\right) * \frac{\partial
Z4}{\partial X3} \right)^T  \circ \frac{\partial X3}{\partial Z3}
\]</span><br></p>
<p>Then:<br></p>
<p><span class="math display">\[\frac{\partial Z4}{\partial X3}=
(M3_{-b})_{Q \; x \; nu2} \]</span> <br></p>
<p><span class="math display">\[\frac{\partial X3}{\partial Z3}=
{[\partial LR(X3_{-b})}]_{nu2 \; x \; n}\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial M2}= X2^T_{n
\; x \; (nu+1)} \]</span><br></p>
<p>Thus you have:<br> <span class="math display">\[\frac{\partial
C}{\partial M2} = \left[\left(\frac{\partial C}{\partial Z4} *
M3_{-b}\right)^T \circ \partial LR(X3_{-b})\right] * X2^T \]</span><br>
<span class="math display">\[\frac{\partial C}{\partial M2} = [[(X4 -
Y)^T * M3_{-b}]^T \circ \partial LR(X3_{-b})] * X2^T \]</span><br></p>
<p>The size of the M2 weight matrix must equal the size of the
derivative of<br> the C cost matrix with respect to the M2
weight<br></p>
<p>Size of M2 matrix is:<br></p>
<p><span class="math display">\[(M2)_{size}= nu2 \; x \; (nu +
1)\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M2}_{size} =
[[(n \; x \; Q)*(Q \; x \; nu2)]^T \circ (nu2 \; x \; n)] * (n \; x \;
(nu+1))\]</span> <span class="math display">\[                               = [(n \; x \;
nu2)^T \circ (nu2 \; x \; n)] * (n \; x \; (nu+1))\]</span> <span class="math display">\[                               = [(nu2 \; x \; n)
\circ (nu2 \; x \; n)] * (n \; x \; (nu +1))\]</span> <span class="math display">\[                               = (nu2 \; x \; n)
* (n \; x \; (nu +1))\]</span> <span class="math display">\[\frac{\partial C}{\partial M2}_{size} = nu2 \; x
\; (nu +1)\]</span></p>
<p><span class="math display">\[(M2)_{size} = \frac{\partial C}{\partial
M2}_{size}\]</span><br></p>
</div>
<div id="derivative-of-the-c-cost-matrix-with-respect-to-the-m1-weight-matrix-1" class="section level4">
<h4>Derivative of The C Cost Matrix With Respect to The M1 Weight
Matrix</h4>
<p><span class="math display">\[\frac{\partial C}{\partial M1} =
\left[\left[\left[ \left( \left( \frac{\partial C^T}{\partial X4} *
\frac{\partial X4^T}{\partial Z4} \right) * \frac{\partial Z4}{\partial
X3} \right)^T \circ \frac{\partial X3}{\partial Z3} \right]^T *
\frac{\partial Z3}{\partial X2}\right]^T \circ \frac{\partial
X2}{\partial Z2}\right] * \frac{\partial Z2}{\partial
M1}\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1} =
\left[\left[ \left(\frac{\partial C}{\partial Z3}\right)^T *
\frac{\partial Z3}{\partial X2}\right]^T \circ \frac{\partial
X2}{\partial Z2}\right] * \frac{\partial Z2}{\partial
M1}\]</span><br></p>
<p>where:<br></p>
<p><span class="math display">\[\frac{\partial Z3}{\partial X2}=
(M2_{-b})_{nu2 \; x \; nu} \]</span> <br></p>
<p><span class="math display">\[\frac{\partial X2}{\partial Z2}=
{[\partial LR(X2_{-b})}]_{nu \; x \; n}\]</span><br></p>
<p><span class="math display">\[\frac{\partial Z2}{\partial M1}= X1^T_{n
\; x \; 785} \]</span><br></p>
<p>Thus:<br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1} =
\left[\left[ \left(\frac{\partial C}{\partial Z3}\right)^T *
M2_{-b}\right]^T \circ \partial LR(X2_{-b})\right] * X1^T
\]</span><br></p>
<p>The size of the M1 weight matrix must equal the size of the
derivative of<br> the C cost matrix with respect to the M1
weight<br></p>
<p>Size of M1 matrix is:<br></p>
<p><span class="math display">\[(M1)_{size}= nu \; x \;
785\]</span><br></p>
<p><span class="math display">\[\frac{\partial C}{\partial M1}_{size} =
[[(n \; x \; nu2)*(nu2 \; x \; nu)]^T \circ (nu \; x \; n)] * (n \; x \;
785)\]</span> <span class="math display">\[                               = [(n \; x \;
nu)^T \circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = [(nu \; x \; n)
\circ (nu \; x \; n)] * (n \; x \; 785)\]</span> <span class="math display">\[                               = (nu \; x \; n) *
(n \; x \; 785)\]</span> <span class="math display">\[\frac{\partial
C}{\partial M1}_{size} = nu \; x \; 785\]</span></p>
<p><span class="math display">\[(M1)_{size} = \frac{\partial C}{\partial
M1}_{size}\]</span><br></p>
</div>
</div>
</div>
<div id="gradient-descent" class="section level2">
<h2>Gradient Descent</h2>
<p>Gradient descent is applied at the end of backpropagation.
Gradient<br> descent is an algorithm that minimizes the cost function by
iteratively<br> updating the M weight matrices. The M weight matrices
are updated by<br> subtracting the M weight matrix by the gradient of
the cost function with<br> respect to the M weight matrices multiplied
by the learning rate. The<br> learning rate is a hyperparameter that
determines the size of the step<br> taken in the direction of the
gradient. The learning rate is a<br> hyperparameter that needs to be
tuned. If the learning rate is too small,<br> then the algorithm will
take a long time to converge. If the learning<br> rate is too large,
then the algorithm will diverge.<br></p>
<div id="basic-gradient-descent-algorithm" class="section level3">
<h3>Basic Gradient Descent Algorithm</h3>
<p><span class="math display">\[M_{t+1} = M_{t} - \alpha \nabla f(M_{t})
\]</span></p>
<p><span class="math display">\[\alpha = \text{learning rate set through
trial and error} \]</span></p>
<div id="one-hidden-layer-1" class="section level4">
<h4>One Hidden Layer</h4>
<p>For the M2 weight matrix:<br> <span class="math display">\[(M2)_{t+1}
= (M2)_{t} - \alpha \frac{\partial C}{\partial M2} \]</span></p>
<p>For the M1 weight matrix:<br> <span class="math display">\[(M1)_{t+1}
= (M1)_{t} - \alpha \frac{\partial C}{\partial M1} \]</span></p>
</div>
<div id="two-hidden-layers-1" class="section level4">
<h4>Two Hidden Layers</h4>
<p>For the M3 weight matrix:<br> <span class="math display">\[(M3)_{t+1}
= (M3)_{t} - \alpha \frac{\partial C}{\partial M3} \]</span></p>
<p>For the M2 weight matrix:<br> <span class="math display">\[(M2)_{t+1}
= (M2)_{t} - \alpha \frac{\partial C}{\partial M2} \]</span></p>
<p>For the M1 weight matrix:<br> <span class="math display">\[(M1)_{n+1}
= (M1)_{t} - \alpha \frac{\partial C}{\partial M1} \]</span></p>
</div>
</div>
<div id="gradient-descent-with-decaying-learning-rate" class="section level3">
<h3>Gradient Descent With Decaying Learning Rate</h3>
<p>Basically the same as the basic gradient descent algorithm, except
the<br> learning rate is decaying with each iteration/epoch. Variable t
stands<br> for the iteration/epoch number. Where t can refer to the
iteration if<br> using SGD or Mini-batch GD. Variable t can refer to the
epoch if using<br> Batch GD. The learning rate is decaying by a factor
of 1/t.<br></p>
<p><span class="math display">\[\alpha = \alpha_{1}*(1/t) \]</span></p>
<p><span class="math display">\[\alpha_{1} = \text{learning rate alpha1
set through trial and error} \]</span></p>
<p><span class="math display">\[M_{t+1} = M_{t} - \alpha \nabla f(M_{t})
\]</span></p>
</div>
<div id="gradient-descent-with-adam-optimizer" class="section level3">
<h3>Gradient Descent With Adam Optimizer</h3>
<p>Adam optimizer is a combination of RMSprop and momentum. Adam
optimizer<br> is a more advanced gradient descent algorithm. Adam
optimizer is more<br> stable and converges faster than basic gradient
descent. Adam optimizer<br> is the recommended gradient descent
algorithm for neural networks. Beta1 is<br> typically set to 0.9. Beta2
is typically set to 0.999. Epsilon is typically<br> set to 10^-8. Adam
optimzer is defined as follows:<br></p>
<p><span class="math display">\[M_{t} = M_{t-1} - \alpha
\frac{\hat{v1}_{t}}{\sqrt{\hat{v2}_{t}} + \epsilon} \]</span></p>
<p>Initialize:<br></p>
<p><span class="math display">\[v1_{0} = 0 \leftarrow \text{Initialize
1st Momentum} \]</span> <span class="math display">\[v2_{0} = 0
\leftarrow \text{Initialize 2nd Momentum} \]</span> <span class="math display">\[t_{0} = 0 \leftarrow \text{Initialize timestep}
\]</span> <span class="math display">\[\beta_{1} = 0.9 \]</span> <span class="math display">\[\beta_{2} = 0.999 \]</span> <span class="math display">\[\epsilon = 10^{-8} \]</span> <span class="math display">\[\alpha = \text{learning rate set through trial
and error} \]</span></p>
<p>Note v at (t-1) is the momentum from the previous iteration/epoch.
1st and<br> 2nd momentum at any time step t is defined as
follows.<br></p>
<p><span class="math display">\[v1_{t} = \beta_{1}v2_{t-1} +
\left[(1-\beta_{1})\nabla f(M_{t-1})\right] \]</span></p>
<p><span class="math display">\[v2_{t} = \beta_{2}v2_{t-1} +
\left[(1-\beta_{2}) \left[\nabla f(M_{t-1})\right]^2\right]
\]</span></p>
<p><span class="math display">\[\left[\nabla f(M_{t-1})\right]^2 =
\nabla f(M_{t-1}) \circ \nabla f(M_{t-1})\]</span></p>
<p>Note that Beta1 and Beta2 are raised to the power of t. Hence if t=3,
then<br> (Beta1)^3 and (Beta2)^3. Bias-corrected 1st and 2nd momentum at
any time<br> step t is defined as follows:<br></p>
<p><span class="math display">\[\hat{v1}_{t} =
\frac{v1_{t}}{1-\beta_{1}^{t}} \]</span> <span class="math display">\[\hat{v2}_{t} = \frac{v2_{t}}{1-\beta_{2}^{t}}
\]</span></p>
<div id="one-hidden-layer-adam-optimizer-for-m2-weight-matrix" class="section level4">
<h4>One Hidden Layer Adam Optimizer for M2 Weight Matrix</h4>
<p>For the M2 weight matrix Initialize v1, v2, and t to 0. Then 1st
and<br> 2nd momentum at any time step t for M2 matrix.<br></p>
<p><span class="math display">\[v1_{t} = \beta_{1}v_{t-1} +
\left[(1-\beta_{1})\frac{\partial C}{\partial M2}\right] \]</span></p>
<p><span class="math display">\[v2_{t} = \beta_{2}v_{t-1} +
\left[(1-\beta_{2}) \left[\frac{\partial C}{\partial M2}\right]^2\right]
\]</span></p>
<p><span class="math display">\[\left[\frac{\partial C}{\partial
M2}\right]^2 = \frac{\partial C}{\partial M2} \circ \frac{\partial
C}{\partial M2}\]</span></p>
<p>Then calculate Bias-corrected 1st and 2nd momentum at any time step t
for M2<br> matrix.<br></p>
<p><span class="math display">\[\hat{v1}_{t} =
\frac{v1_{t}}{1-\beta_{1}^{t}} \]</span> <span class="math display">\[\hat{v2}_{t} = \frac{v2_{t}}{1-\beta_{2}^{t}}
\]</span></p>
<p>Finally, the M2 weight matrix is updated.<br></p>
<p><span class="math display">\[M2_{t} = M2_{t-1} - \alpha
\frac{\hat{v1}_{t}}{\sqrt{\hat{v2}_{t}} + \epsilon} \]</span></p>
<p>Note that <span class="math inline">\(\hat{v1}_{t}\)</span> and <span class="math inline">\(\hat{v2}_{t}\)</span> are matrices with same
dimension as M2. Hence in the formula, <span class="math inline">\(\frac{\hat{v1}_{t}}{\sqrt{\hat{v2}_{t}} +
\epsilon}\)</span><br> represents element-wise division, where each
element in <span class="math inline">\(\hat{v1}_{t}\)</span> is divided
by the corresponding element in <span class="math inline">\(\sqrt{\hat{v2}_{t}} + \epsilon\)</span>.<br></p>
</div>
</div>
</div>
<div id="training-and-accuracy-of-neural-network" class="section level2">
<h2>Training and Accuracy of Neural Network</h2>
<div id="training-phase" class="section level3">
<h3>Training Phase</h3>
<p>What is the training phase for NN? The training phase is composed
of<br> feedforward, backpropagation, then gradient descent. Then start
over<br> again. Feedforward, backpropagation, and finally gradient
descent. So on<br> and so forth. One iteration of feedforward,
backpropagation, and gradient<br> descent through every single
observation is called an epoch.So then, after<br> how many epochs do you
need to stop training? Training stoppage is<br> determined by the most
optimal solution from the testing dataset. Once the<br> minima for the
testing data is reached, training is optimized. The minima<br> for the
testing data is determined by the mean categorical cross entropy<br>
loss function.<br></p>
</div>
<div id="accuracy-of-neural-network" class="section level3">
<h3>Accuracy of Neural Network</h3>
<p><span class="math inline">\(X_{out}\)</span> is the output
probability. Each column is an observation. Assuming<br> that the
highest predicted class from each column represents the model’s<br>
final classification decision. Then extracting the highest
probability<br> from each column, <span class="math inline">\((X_{out})_{highest}\)</span> is a vector with the
highest<br> probability for each column.<br></p>
<p><span class="math display">\[X_{out}
\underset{collapse\;vector}{\rightarrow} [(X_{out})_{highest}]_{1 \; x
\;n} \]</span></p>
<p><span class="math inline">\(Y\)</span> is the one hot encoding matrix
which contains all one hot encoding<br> vectors. Matrix <span class="math inline">\(Y\)</span> contains all the correct probabilities
for each<br> observation. In theory, if 100% accuracy was ever reached,
then element<br> wise multiplication of <span class="math inline">\(X_{out} \circ Y= X_{out}\)</span>. But if 100%
accuracy<br> is not reached, then <span class="math inline">\(X_{out}
\circ Y \neq X_{out}\)</span>. The difference between<br> <span class="math inline">\(X_{out} \circ Y\)</span> and <span class="math inline">\(X_{out}\)</span> is the error. The error is the
difference between<br> the predicted probability and the correct
probability. <span class="math inline">\(X_{out} \circ Y\)</span> has
mostly<br> zeros. Each column only contains one non zero value. Hence if
you collapse<br> <span class="math inline">\(X_{out} \circ Y\)</span>
into a vector. The collapsed vector contains the correct<br> predictions
and the incorrect predictions for all observations.<br></p>
<p><span class="math display">\[(X_{out})_{Q \; x \; n} \circ Y_{Q \; x
\; n} \underset{collapse\;vector}{\rightarrow} [(X_{out} \circ
Y)_{v}]_{1 \; x \;n} \]</span></p>
<p>We can find out the correct prediction for each observation by
subtracting<br> <span class="math inline">\([(X_{out} \circ Y)_{v}]_{1
\; x \;n}\)</span> by <span class="math inline">\([(X_{out})_{highest}]_{1 \; x \;n}\)</span>. The
zeros in the vector<br> means the NN made the correct prediction for
that observation. The non<br> zero values in the vector means the NN
made the incorrect prediction for<br> that observation.<br></p>
<p><span class="math display">\[[[(X_{out})_{highest}]_{1 \; x \;n} -
(X_{out} \circ Y)_{v}]_{1 \; x \;n}\]</span></p>
<p>Count number of zeros to obtain the number of correct
predictions.<br></p>
<p><span class="math display">\[count([[(X_{out})_{highest}]_{1 \; x
\;n} - (X_{out} \circ Y)_{v}]_{1 \; x \;n})\]</span></p>
<p>Thus accuracy = (count)(100%)/n, where n=total number of
observations.<br></p>
<p><span class="math display">\[Accuracy \; \% = \frac{count}{n} * 100\%
\]</span></p>
</div>
</div>
<div id="setting-up-model-parameters-and-data-structures" class="section level2">
<h2>Setting Up Model Parameters and Data Structures</h2>
<div id="tunenable-hyperparameters" class="section level3">
<h3>Tunenable Hyperparameters</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co">#12</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co"># &quot;No mapping&quot; = No mapping</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co"># &quot;Feature Mapping&quot; = Fourier Feature Mapping</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>NoMapping_vs_FourierFeatureMapping <span class="ot">&lt;-</span><span class="st">&quot;No mapping&quot;</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co"># Sigma value for Gaussian distribution for Fourier Feature Mapping</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>sigma <span class="ot">&lt;-</span><span class="dv">60</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co">#hyperparameter for learning rate</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>Alpha_ONE <span class="ot">&lt;-</span><span class="dv">10</span><span class="sc">^-</span><span class="dv">3</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="co">#Beta1</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>Beta1 <span class="ot">&lt;-</span><span class="fl">0.9</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a><span class="co">#Beta2</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>Beta2 <span class="ot">&lt;-</span><span class="fl">0.999</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a><span class="co">#Epsilon</span></span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>Epsilon <span class="ot">&lt;-</span><span class="dv">10</span><span class="sc">^-</span><span class="dv">8</span></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a><span class="co"># Initialize 1st moment at t=1</span></span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>v1 <span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a><span class="co"># Initialize 2nd moment at t=1</span></span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>v2 <span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a><span class="co"># GRADIENT DESCENT ALGORITHMS </span></span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a><span class="co"># &quot;constant&quot; = constant learning rate</span></span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a><span class="co"># &quot;decaying&quot; = decaying learning rate (1/t)</span></span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a><span class="co">#     &quot;adam&quot; = adam optimizer</span></span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a>gradient_descent_algorithm <span class="ot">&lt;-</span><span class="st">&quot;adam&quot;</span></span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a><span class="co">#Number of training observations used in each batch.</span></span>
<span id="cb10-34"><a href="#cb10-34" tabindex="-1"></a><span class="co"># Set n=1 for SGD (1 observation)</span></span>
<span id="cb10-35"><a href="#cb10-35" tabindex="-1"></a><span class="co"># Set 1 &lt;n&lt; N_training for Mini-batch GD (more than 1, but less than all observations)</span></span>
<span id="cb10-36"><a href="#cb10-36" tabindex="-1"></a><span class="co"># Set n= N_training, Batch GD (All observation)</span></span>
<span id="cb10-37"><a href="#cb10-37" tabindex="-1"></a>n <span class="ot">&lt;-</span><span class="dv">350</span></span>
<span id="cb10-38"><a href="#cb10-38" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" tabindex="-1"></a><span class="co"># Setting LeakyReLU_alpha = 0 is the same as using ReLU.</span></span>
<span id="cb10-40"><a href="#cb10-40" tabindex="-1"></a>LeakyReLU_alpha <span class="ot">&lt;-</span><span class="fl">0.1</span></span>
<span id="cb10-41"><a href="#cb10-41" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" tabindex="-1"></a><span class="co"># 1 for one hidden layer and 2 for two hidden layers.</span></span>
<span id="cb10-43"><a href="#cb10-43" tabindex="-1"></a><span class="co"># Code is only set up for one or two hidden layers.</span></span>
<span id="cb10-44"><a href="#cb10-44" tabindex="-1"></a><span class="co"># Thus 1 and 2 are the only allowed values for num_hidden_layers.</span></span>
<span id="cb10-45"><a href="#cb10-45" tabindex="-1"></a>num_hidden_layers <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb10-46"><a href="#cb10-46" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" tabindex="-1"></a><span class="co"># Code not set up for one neuron in hidden layer, thus n=1 is not allowed.</span></span>
<span id="cb10-48"><a href="#cb10-48" tabindex="-1"></a><span class="co"># Apart from that. Any other positive integer is allowed.</span></span>
<span id="cb10-49"><a href="#cb10-49" tabindex="-1"></a><span class="co"># Number of hidden neurons in 1st hidden layer.</span></span>
<span id="cb10-50"><a href="#cb10-50" tabindex="-1"></a>nu <span class="ot">&lt;-</span><span class="dv">100</span></span>
<span id="cb10-51"><a href="#cb10-51" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" tabindex="-1"></a><span class="co"># Same as other hidden layer, n=1 is not allowed.</span></span>
<span id="cb10-53"><a href="#cb10-53" tabindex="-1"></a><span class="co"># Number of hidden neurons in 2nd hidden layer.</span></span>
<span id="cb10-54"><a href="#cb10-54" tabindex="-1"></a>nu2 <span class="ot">&lt;-</span><span class="dv">20</span></span>
<span id="cb10-55"><a href="#cb10-55" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb10-57"><a href="#cb10-57" tabindex="-1"></a><span class="do">##############################################</span></span>
<span id="cb10-58"><a href="#cb10-58" tabindex="-1"></a><span class="co"># SGD AND MINI BATCH GRADIENT DESCENT</span></span>
<span id="cb10-59"><a href="#cb10-59" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb10-60"><a href="#cb10-60" tabindex="-1"></a><span class="co"># Total number of observations for training</span></span>
<span id="cb10-61"><a href="#cb10-61" tabindex="-1"></a>Tb <span class="ot">&lt;-</span>N_training</span>
<span id="cb10-62"><a href="#cb10-62" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" tabindex="-1"></a><span class="co"># Number of observations per iteration</span></span>
<span id="cb10-64"><a href="#cb10-64" tabindex="-1"></a>n_iteration <span class="ot">&lt;-</span>n</span>
<span id="cb10-65"><a href="#cb10-65" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" tabindex="-1"></a><span class="co"># Number of batches = number of iterations per epoch</span></span>
<span id="cb10-67"><a href="#cb10-67" tabindex="-1"></a>number_batches <span class="ot">&lt;-</span>Tb<span class="sc">/</span>n_iteration</span>
<span id="cb10-68"><a href="#cb10-68" tabindex="-1"></a>number_iterations_epoch <span class="ot">&lt;-</span>number_batches</span>
<span id="cb10-69"><a href="#cb10-69" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" tabindex="-1"></a><span class="co"># Total Number of epochs</span></span>
<span id="cb10-71"><a href="#cb10-71" tabindex="-1"></a>Epoch <span class="ot">&lt;-</span><span class="dv">4</span></span>
<span id="cb10-72"><a href="#cb10-72" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" tabindex="-1"></a><span class="co"># (Epoch*number_iterations_epoch) = total number of iterations per epoch</span></span>
<span id="cb10-74"><a href="#cb10-74" tabindex="-1"></a>Epochs <span class="ot">&lt;-</span>Epoch<span class="sc">*</span>number_iterations_epoch</span>
<span id="cb10-75"><a href="#cb10-75" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb10-77"><a href="#cb10-77" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb10-78"><a href="#cb10-78" tabindex="-1"></a><span class="co">#BATCH GRADIENT DESCENT</span></span>
<span id="cb10-79"><a href="#cb10-79" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb10-80"><a href="#cb10-80" tabindex="-1"></a><span class="co"># Total Number of epochs</span></span>
<span id="cb10-81"><a href="#cb10-81" tabindex="-1"></a>Epochs <span class="ot">&lt;-</span><span class="dv">3</span></span>
<span id="cb10-82"><a href="#cb10-82" tabindex="-1"></a></span>
<span id="cb10-83"><a href="#cb10-83" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="weight-matrices" class="section level3">
<h3>Weight Matrices</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="do">##############################################</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co"># ONE HIDDEN LAYER</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co"># Size of matrix M1.</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>M1_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>M1_n_col <span class="ot">&lt;-</span><span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a><span class="co"># Size of matrix X1.</span></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>X1_m_rows <span class="ot">&lt;-</span><span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>X1_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a><span class="co"># Size of matrix Z2.</span></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>Z2_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>Z2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a><span class="co"># Size of LeakyReLU.</span></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>LeakyReLU_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>LeakyReLU_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a><span class="co"># Size of matrix M2.</span></span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>M2_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>M2_n_col <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a><span class="co"># Size of matrix X2</span></span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a><span class="co"># Add 1 to the number of rows of LeakyReLU to account for the bias.</span></span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a>X2_m_rows <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a>X2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a><span class="co"># Size of matrix Z3.</span></span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a>Z3_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>Z3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-33"><a href="#cb11-33" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" tabindex="-1"></a><span class="co"># Size of Softmax. Where softmax = X3 =X_out</span></span>
<span id="cb11-35"><a href="#cb11-35" tabindex="-1"></a>X3_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-36"><a href="#cb11-36" tabindex="-1"></a>X3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-37"><a href="#cb11-37" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb11-38"><a href="#cb11-38" tabindex="-1"></a><span class="do">##############################################</span></span>
<span id="cb11-39"><a href="#cb11-39" tabindex="-1"></a><span class="co"># TWO HIDDEN LAYERS</span></span>
<span id="cb11-40"><a href="#cb11-40" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb11-41"><a href="#cb11-41" tabindex="-1"></a><span class="co"># Size of matrix M1.</span></span>
<span id="cb11-42"><a href="#cb11-42" tabindex="-1"></a>M1_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-43"><a href="#cb11-43" tabindex="-1"></a>M1_n_col <span class="ot">&lt;-</span> <span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-44"><a href="#cb11-44" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" tabindex="-1"></a><span class="co"># Size of matrix X1.</span></span>
<span id="cb11-46"><a href="#cb11-46" tabindex="-1"></a>X1_m_rows <span class="ot">&lt;-</span> <span class="fu">ncol</span>(Row_Image_Matrix_training) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-47"><a href="#cb11-47" tabindex="-1"></a>X1_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-48"><a href="#cb11-48" tabindex="-1"></a></span>
<span id="cb11-49"><a href="#cb11-49" tabindex="-1"></a><span class="co"># Size of matrix Z2.</span></span>
<span id="cb11-50"><a href="#cb11-50" tabindex="-1"></a>Z2_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-51"><a href="#cb11-51" tabindex="-1"></a>Z2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-52"><a href="#cb11-52" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" tabindex="-1"></a><span class="co"># Size of LeakyReLU.</span></span>
<span id="cb11-54"><a href="#cb11-54" tabindex="-1"></a>LeakyReLU_m_rows <span class="ot">&lt;-</span>nu</span>
<span id="cb11-55"><a href="#cb11-55" tabindex="-1"></a>LeakyReLU_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-56"><a href="#cb11-56" tabindex="-1"></a></span>
<span id="cb11-57"><a href="#cb11-57" tabindex="-1"></a><span class="co"># Size of matrix M2.</span></span>
<span id="cb11-58"><a href="#cb11-58" tabindex="-1"></a>M2_m_rows <span class="ot">&lt;-</span>nu2</span>
<span id="cb11-59"><a href="#cb11-59" tabindex="-1"></a>M2_n_col <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-60"><a href="#cb11-60" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" tabindex="-1"></a><span class="co"># Size of matrix X2</span></span>
<span id="cb11-62"><a href="#cb11-62" tabindex="-1"></a><span class="co"># Add 1 to the number of rows of LeakyReLU to account for the bias.</span></span>
<span id="cb11-63"><a href="#cb11-63" tabindex="-1"></a>X2_m_rows <span class="ot">&lt;-</span>nu <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-64"><a href="#cb11-64" tabindex="-1"></a>X2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-65"><a href="#cb11-65" tabindex="-1"></a></span>
<span id="cb11-66"><a href="#cb11-66" tabindex="-1"></a><span class="co"># Size of matrix Z3.</span></span>
<span id="cb11-67"><a href="#cb11-67" tabindex="-1"></a>Z3_m_rows <span class="ot">&lt;-</span>nu2</span>
<span id="cb11-68"><a href="#cb11-68" tabindex="-1"></a>Z3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-69"><a href="#cb11-69" tabindex="-1"></a></span>
<span id="cb11-70"><a href="#cb11-70" tabindex="-1"></a><span class="co"># Size of LeakyReLU2.</span></span>
<span id="cb11-71"><a href="#cb11-71" tabindex="-1"></a>LeakyReLU_2_m_rows <span class="ot">&lt;-</span>nu2</span>
<span id="cb11-72"><a href="#cb11-72" tabindex="-1"></a>LeakyReLU_2_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-73"><a href="#cb11-73" tabindex="-1"></a></span>
<span id="cb11-74"><a href="#cb11-74" tabindex="-1"></a><span class="co"># Size of matrix M3.</span></span>
<span id="cb11-75"><a href="#cb11-75" tabindex="-1"></a>M3_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-76"><a href="#cb11-76" tabindex="-1"></a>M3_n_col <span class="ot">&lt;-</span>nu2 <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-77"><a href="#cb11-77" tabindex="-1"></a></span>
<span id="cb11-78"><a href="#cb11-78" tabindex="-1"></a><span class="co"># Size of matrix X3</span></span>
<span id="cb11-79"><a href="#cb11-79" tabindex="-1"></a>X3_m_rows <span class="ot">&lt;-</span>nu2 <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb11-80"><a href="#cb11-80" tabindex="-1"></a>X3_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-81"><a href="#cb11-81" tabindex="-1"></a></span>
<span id="cb11-82"><a href="#cb11-82" tabindex="-1"></a><span class="co"># Size of matrix Z4.</span></span>
<span id="cb11-83"><a href="#cb11-83" tabindex="-1"></a>Z4_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-84"><a href="#cb11-84" tabindex="-1"></a>Z4_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-85"><a href="#cb11-85" tabindex="-1"></a></span>
<span id="cb11-86"><a href="#cb11-86" tabindex="-1"></a><span class="co"># Size of Softmax. Where softmax = X4 =X_out</span></span>
<span id="cb11-87"><a href="#cb11-87" tabindex="-1"></a>X4_m_rows <span class="ot">&lt;-</span>Q</span>
<span id="cb11-88"><a href="#cb11-88" tabindex="-1"></a>X4_n_col <span class="ot">&lt;-</span>n</span>
<span id="cb11-89"><a href="#cb11-89" tabindex="-1"></a>}</span></code></pre></div>
<p>Set seed is used to make sure that the same random numbers are
generated<br> each time the code is run.<br></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span></code></pre></div>
<p>Weight matrices.<br></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Define the standard deviation for weight initialization (e.g., 1e-1 for small values)</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>weight_sd <span class="ot">&lt;-</span> <span class="fl">1e-1</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>  <span class="co"># When using One Hidden Layer</span></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>  M1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M1_m_rows <span class="sc">*</span> M1_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M1_m_rows, <span class="at">ncol =</span> M1_n_col)</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>  M2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M2_m_rows <span class="sc">*</span> M2_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M2_m_rows, <span class="at">ncol =</span> M2_n_col)</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>  <span class="co"># When using Two Hidden Layers</span></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>  M1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M1_m_rows <span class="sc">*</span> M1_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M1_m_rows, <span class="at">ncol =</span> M1_n_col)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>  M2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M2_m_rows <span class="sc">*</span> M2_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M2_m_rows, <span class="at">ncol =</span> M2_n_col)</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>  M3 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(M3_m_rows <span class="sc">*</span> M3_n_col, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> weight_sd), <span class="at">nrow =</span> M3_m_rows, <span class="at">ncol =</span> M3_n_col)</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="organizing-neural-network-metrics-with-lists-and-dataframes" class="section level3">
<h3>Organizing Neural Network Metrics with Lists and Dataframes</h3>
<p>Create a list to store M matrix for each epoch.<br></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>M1_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>M2_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>M1_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>M2_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>M3_list <span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, Epochs)</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>}</span></code></pre></div>
<p>Create an empty dataframe to store the mean categorical cross entropy
loss<br> for each epoch.<br></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>CCEntropy_Loss <span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">iteration =</span> <span class="fu">numeric</span>(Epochs), <span class="at">epoch =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_x_out =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_testing_x_out =</span> <span class="fu">numeric</span>(Epochs))</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>CCEntropy_Loss <span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">epoch =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_x_out =</span> <span class="fu">numeric</span>(Epochs), <span class="at">CCEL_testing_x_out =</span> <span class="fu">numeric</span>(Epochs))</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>}</span></code></pre></div>
<p>Create an empty dataframe to store training and testing accuracy for
each<br> epoch.<br></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>Accuracy_Percent <span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">Training_percent =</span> <span class="fu">numeric</span>(Epochs), <span class="at">Testing_percent =</span> <span class="fu">numeric</span>(Epochs))</span></code></pre></div>
</div>
</div>
<div id="fourier-feature-mapping-for-gradient-descent" class="section level2">
<h2>Fourier Feature Mapping for Gradient Descent</h2>
<div id="sampling-from-a-gaussian-distribution-with-mean-0-and-variance-sigma2" class="section level3">
<h3>Sampling From a Gaussian Distribution With Mean 0 and Variance
(sigma)^2</h3>
<p>Ramdom sampled points are stored in B matrix.<br></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Row size matrix X(data)</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>Xd_m_rows <span class="ot">&lt;-</span><span class="fu">ncol</span>(Row_Image_Matrix_training)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="co"># B Matrix = Sample from the Gaussian distribution with mean 0 and variance sigma^2</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(Xd_m_rows <span class="sc">*</span> <span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="co"># Hf is harmonic frequencies (1 2 3 4...#number pixels)</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>Hf <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">1</span>, Xd_m_rows), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a><span class="co"># B martrix for training data</span></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a><span class="co"># bind column n_iteration times</span></span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>B_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>Hf_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a><span class="co"># bind column N_training times</span></span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a>B_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb17-22"><a href="#cb17-22" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" tabindex="-1"></a>Hf_training <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-24"><a href="#cb17-24" tabindex="-1"></a>}</span>
<span id="cb17-25"><a href="#cb17-25" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-27"><a href="#cb17-27" tabindex="-1"></a><span class="co"># B matrix for testing data</span></span>
<span id="cb17-28"><a href="#cb17-28" tabindex="-1"></a>B_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-29"><a href="#cb17-29" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" tabindex="-1"></a><span class="co"># Hf matrix for testing data</span></span>
<span id="cb17-31"><a href="#cb17-31" tabindex="-1"></a>Hf_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-32"><a href="#cb17-32" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-33"><a href="#cb17-33" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-35"><a href="#cb17-35" tabindex="-1"></a><span class="co"># B matrix for data prediction</span></span>
<span id="cb17-36"><a href="#cb17-36" tabindex="-1"></a>B_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(B, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-37"><a href="#cb17-37" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" tabindex="-1"></a><span class="co"># Hf matrix for data prediction</span></span>
<span id="cb17-39"><a href="#cb17-39" tabindex="-1"></a>Hf_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Hf, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-40"><a href="#cb17-40" tabindex="-1"></a><span class="do">######</span></span>
<span id="cb17-41"><a href="#cb17-41" tabindex="-1"></a></span>
<span id="cb17-42"><a href="#cb17-42" tabindex="-1"></a><span class="co"># Hf*pi*B </span></span>
<span id="cb17-43"><a href="#cb17-43" tabindex="-1"></a>Hf_pi_B_training <span class="ot">&lt;-</span>Hf_training<span class="sc">*</span>pi<span class="sc">*</span>B_training</span>
<span id="cb17-44"><a href="#cb17-44" tabindex="-1"></a>Hf_pi_B_testing <span class="ot">&lt;-</span>Hf_testing<span class="sc">*</span>pi<span class="sc">*</span>B_testing</span>
<span id="cb17-45"><a href="#cb17-45" tabindex="-1"></a>Hf_pi_B_pred <span class="ot">&lt;-</span>Hf_pred<span class="sc">*</span>pi<span class="sc">*</span>B_pred</span></code></pre></div>
</div>
<div id="fourier-feature-mapping-for-batch-gradient-descent-training-data" class="section level3">
<h3>Fourier Feature Mapping for Batch Gradient Descent (Training
Data)</h3>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> N_training) {</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>      X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>      X1_map_vs_no_map <span class="ot">&lt;-</span> X1_training</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>    },</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>    <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>      <span class="co"># Xi = X1_training</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>      <span class="co"># Batch Gradient Descent</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>      X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>      Hf_pi_B_Xi <span class="ot">&lt;-</span> Hf_pi_B_training <span class="sc">*</span> X1_training</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>      <span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>      cos_element_wise <span class="ot">&lt;-</span> <span class="fu">cos</span>(Hf_pi_B_Xi)</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>      <span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>      sin_element_wise <span class="ot">&lt;-</span> <span class="fu">sin</span>(Hf_pi_B_Xi)</span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>      <span class="do">###############################################</span></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>      <span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>      one_zero_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a>      <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a>      one_zero_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>      cos_matrix <span class="ot">&lt;-</span> one_zero_M <span class="sc">*</span> cos_element_wise</span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a>      <span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a>      zero_one_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a>      <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a>      zero_one_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V, N_training,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_training, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>      sin_matrix <span class="ot">&lt;-</span> zero_one_M <span class="sc">*</span> sin_element_wise</span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a></span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a>      <span class="co"># Fourier Feature Mapping</span></span>
<span id="cb18-39"><a href="#cb18-39" tabindex="-1"></a>      gamma_Xi <span class="ot">&lt;-</span> cos_matrix <span class="sc">+</span> sin_matrix</span>
<span id="cb18-40"><a href="#cb18-40" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" tabindex="-1"></a>      X1_map_vs_no_map <span class="ot">&lt;-</span> gamma_Xi</span>
<span id="cb18-42"><a href="#cb18-42" tabindex="-1"></a>         })</span>
<span id="cb18-43"><a href="#cb18-43" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="fourier-feature-mapping-for-sgd-mini-batch-and-batch-gradient-descent-testing-data" class="section level3">
<h3>Fourier Feature Mapping for SGD, Mini-Batch and Batch Gradient
Descent (Testing Data)</h3>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="co"># Transpose to make each column an observation.</span></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>    X1_testing <span class="ot">&lt;-</span> Row_Image_Matrix_testing <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>    X1_map_vs_no_map_testing <span class="ot">&lt;-</span>X1_testing</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a>       },</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb19-12"><a href="#cb19-12" tabindex="-1"></a>    <span class="co">#Xi = X1_testing</span></span>
<span id="cb19-13"><a href="#cb19-13" tabindex="-1"></a>    X1_testing <span class="ot">&lt;-</span>Row_Image_Matrix_testing <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb19-14"><a href="#cb19-14" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" tabindex="-1"></a>    Hf_pi_B_Xi_testing <span class="ot">&lt;-</span>Hf_pi_B_testing <span class="sc">*</span> X1_testing</span>
<span id="cb19-16"><a href="#cb19-16" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" tabindex="-1"></a>    <span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb19-18"><a href="#cb19-18" tabindex="-1"></a>    cos_element_wise_testing <span class="ot">&lt;-</span><span class="fu">cos</span>(Hf_pi_B_Xi_testing)</span>
<span id="cb19-19"><a href="#cb19-19" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" tabindex="-1"></a>    <span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb19-21"><a href="#cb19-21" tabindex="-1"></a>    sin_element_wise_testing <span class="ot">&lt;-</span><span class="fu">sin</span>(Hf_pi_B_Xi_testing)</span>
<span id="cb19-22"><a href="#cb19-22" tabindex="-1"></a>    <span class="do">###############################################</span></span>
<span id="cb19-23"><a href="#cb19-23" tabindex="-1"></a>    <span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb19-24"><a href="#cb19-24" tabindex="-1"></a>    one_zero_V_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb19-25"><a href="#cb19-25" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" tabindex="-1"></a>    <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb19-27"><a href="#cb19-27" tabindex="-1"></a>    one_zero_M_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V_testing, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb19-28"><a href="#cb19-28" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" tabindex="-1"></a>    cos_matrix_testing <span class="ot">&lt;-</span>one_zero_M_testing <span class="sc">*</span> cos_element_wise_testing</span>
<span id="cb19-30"><a href="#cb19-30" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" tabindex="-1"></a>    <span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb19-32"><a href="#cb19-32" tabindex="-1"></a>    zero_one_V_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb19-33"><a href="#cb19-33" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" tabindex="-1"></a>    <span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb19-35"><a href="#cb19-35" tabindex="-1"></a>    zero_one_M_testing <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V_testing, N_testing,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> N_testing, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb19-36"><a href="#cb19-36" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" tabindex="-1"></a>    sin_matrix_testing <span class="ot">&lt;-</span>zero_one_M_testing <span class="sc">*</span> sin_element_wise_testing</span>
<span id="cb19-38"><a href="#cb19-38" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" tabindex="-1"></a>    <span class="co"># Fourier Feature Mapping</span></span>
<span id="cb19-40"><a href="#cb19-40" tabindex="-1"></a>    gamma_Xi_testing <span class="ot">&lt;-</span> cos_matrix_testing <span class="sc">+</span> sin_matrix_testing</span>
<span id="cb19-41"><a href="#cb19-41" tabindex="-1"></a></span>
<span id="cb19-42"><a href="#cb19-42" tabindex="-1"></a>    X1_map_vs_no_map_testing <span class="ot">&lt;-</span>gamma_Xi_testing</span>
<span id="cb19-43"><a href="#cb19-43" tabindex="-1"></a>       })</span></code></pre></div>
</div>
</div>
<div id="putting-it-all-together-neural-network-code" class="section level2">
<h2>Putting It All Together: Neural Network Code</h2>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># if n &lt; N_training, then SGD and mini batch gradient descent</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="co"># if n = N_training, then batch gradient descent</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="co">#12</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="co"># SGD AND MINI BATCH GRADIENT DESCENT</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a><span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs) {</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a><span class="co"># Data Preprocessing for Training Data</span></span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a><span class="co"># Separate training</span></span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a><span class="co"># Randomly select n_iteration rows</span></span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a><span class="co"># replace = TRUE means that the same row can be selected more than once.</span></span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a><span class="co"># replace = FALSE means that the same row cannot be selected more than once.</span></span>
<span id="cb20-19"><a href="#cb20-19" tabindex="-1"></a>Randomly_selected_rows <span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">nrow</span>(Data_training_scaled), n_iteration, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-20"><a href="#cb20-20" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" tabindex="-1"></a><span class="co"># Create a new matrix with the selected rows</span></span>
<span id="cb20-23"><a href="#cb20-23" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-24"><a href="#cb20-24" tabindex="-1"></a><span class="co">#SGD </span></span>
<span id="cb20-25"><a href="#cb20-25" tabindex="-1"></a>Data_training <span class="ot">&lt;-</span>Data_training_scaled[Randomly_selected_rows, ] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-26"><a href="#cb20-26" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-27"><a href="#cb20-27" tabindex="-1"></a><span class="co">#Mini Batch Gradient Descent</span></span>
<span id="cb20-28"><a href="#cb20-28" tabindex="-1"></a>Data_training <span class="ot">&lt;-</span>Data_training_scaled[Randomly_selected_rows, ] </span>
<span id="cb20-29"><a href="#cb20-29" tabindex="-1"></a>}</span>
<span id="cb20-30"><a href="#cb20-30" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-32"><a href="#cb20-32" tabindex="-1"></a><span class="co"># Let&#39;s separate the **y** column and the **image vector columns**.</span></span>
<span id="cb20-33"><a href="#cb20-33" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-34"><a href="#cb20-34" tabindex="-1"></a><span class="co">#Training data</span></span>
<span id="cb20-35"><a href="#cb20-35" tabindex="-1"></a>Y_Labels_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">1</span>]  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()  <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-36"><a href="#cb20-36" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" tabindex="-1"></a>Row_Image_Matrix_training <span class="ot">&lt;-</span>Data_training[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_training)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-38"><a href="#cb20-38" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-40"><a href="#cb20-40" tabindex="-1"></a><span class="co">#One Hot Encoding Training Data</span></span>
<span id="cb20-41"><a href="#cb20-41" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-42"><a href="#cb20-42" tabindex="-1"></a><span class="co"># Initialize a list to store the basis vectors</span></span>
<span id="cb20-43"><a href="#cb20-43" tabindex="-1"></a>basis_vectors_list_training <span class="ot">&lt;-</span><span class="fu">list</span>()</span>
<span id="cb20-44"><a href="#cb20-44" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" tabindex="-1"></a><span class="co"># Loop over each column of the matrix</span></span>
<span id="cb20-46"><a href="#cb20-46" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iteration) {</span>
<span id="cb20-47"><a href="#cb20-47" tabindex="-1"></a>  <span class="co"># Get the label for the q-th column</span></span>
<span id="cb20-48"><a href="#cb20-48" tabindex="-1"></a>  label <span class="ot">&lt;-</span> Y_Labels_training[<span class="dv">1</span>, q] <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb20-49"><a href="#cb20-49" tabindex="-1"></a>  </span>
<span id="cb20-50"><a href="#cb20-50" tabindex="-1"></a>  <span class="co"># Create a vector of zeros with length Q</span></span>
<span id="cb20-51"><a href="#cb20-51" tabindex="-1"></a>  basis_vectors_training <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, Q)</span>
<span id="cb20-52"><a href="#cb20-52" tabindex="-1"></a>  </span>
<span id="cb20-53"><a href="#cb20-53" tabindex="-1"></a>  <span class="co"># Set the (label+1)-th element to 1</span></span>
<span id="cb20-54"><a href="#cb20-54" tabindex="-1"></a>  basis_vectors_training[label <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb20-55"><a href="#cb20-55" tabindex="-1"></a>  </span>
<span id="cb20-56"><a href="#cb20-56" tabindex="-1"></a>  <span class="co"># Add the basis vector to the list</span></span>
<span id="cb20-57"><a href="#cb20-57" tabindex="-1"></a>  basis_vectors_list_training[[q]] <span class="ot">&lt;-</span>basis_vectors_training <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-58"><a href="#cb20-58" tabindex="-1"></a>}</span>
<span id="cb20-59"><a href="#cb20-59" tabindex="-1"></a></span>
<span id="cb20-60"><a href="#cb20-60" tabindex="-1"></a><span class="co"># Convert the list of basis vectors to a matrix</span></span>
<span id="cb20-61"><a href="#cb20-61" tabindex="-1"></a>Y_One_Hot_Encoding_training <span class="ot">&lt;-</span> <span class="fu">do.call</span>(cbind, basis_vectors_list_training)</span>
<span id="cb20-62"><a href="#cb20-62" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-63"><a href="#cb20-63" tabindex="-1"></a>  <span class="co"># Code for Training forward forward pass</span></span>
<span id="cb20-64"><a href="#cb20-64" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-65"><a href="#cb20-65" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb20-66"><a href="#cb20-66" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb20-67"><a href="#cb20-67" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb20-68"><a href="#cb20-68" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-69"><a href="#cb20-69" tabindex="-1"></a>X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-70"><a href="#cb20-70" tabindex="-1"></a></span>
<span id="cb20-71"><a href="#cb20-71" tabindex="-1"></a>X1_map_vs_no_map <span class="ot">&lt;-</span>X1_training</span>
<span id="cb20-72"><a href="#cb20-72" tabindex="-1"></a>       },</span>
<span id="cb20-73"><a href="#cb20-73" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-74"><a href="#cb20-74" tabindex="-1"></a><span class="co">#Xi = X1_training</span></span>
<span id="cb20-75"><a href="#cb20-75" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-76"><a href="#cb20-76" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-77"><a href="#cb20-77" tabindex="-1"></a>  X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training</span>
<span id="cb20-78"><a href="#cb20-78" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-79"><a href="#cb20-79" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-80"><a href="#cb20-80" tabindex="-1"></a>  X1_training <span class="ot">&lt;-</span> Row_Image_Matrix_training <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb20-81"><a href="#cb20-81" tabindex="-1"></a>}</span>
<span id="cb20-82"><a href="#cb20-82" tabindex="-1"></a></span>
<span id="cb20-83"><a href="#cb20-83" tabindex="-1"></a>Hf_pi_B_Xi <span class="ot">&lt;-</span>Hf_pi_B_training <span class="sc">*</span> X1_training</span>
<span id="cb20-84"><a href="#cb20-84" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" tabindex="-1"></a><span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb20-86"><a href="#cb20-86" tabindex="-1"></a>cos_element_wise <span class="ot">&lt;-</span><span class="fu">cos</span>(Hf_pi_B_Xi)</span>
<span id="cb20-87"><a href="#cb20-87" tabindex="-1"></a></span>
<span id="cb20-88"><a href="#cb20-88" tabindex="-1"></a><span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb20-89"><a href="#cb20-89" tabindex="-1"></a>sin_element_wise <span class="ot">&lt;-</span><span class="fu">sin</span>(Hf_pi_B_Xi)</span>
<span id="cb20-90"><a href="#cb20-90" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-91"><a href="#cb20-91" tabindex="-1"></a><span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb20-92"><a href="#cb20-92" tabindex="-1"></a>one_zero_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb20-93"><a href="#cb20-93" tabindex="-1"></a></span>
<span id="cb20-94"><a href="#cb20-94" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb20-95"><a href="#cb20-95" tabindex="-1"></a>one_zero_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb20-96"><a href="#cb20-96" tabindex="-1"></a></span>
<span id="cb20-97"><a href="#cb20-97" tabindex="-1"></a>cos_matrix <span class="ot">&lt;-</span>one_zero_M <span class="sc">*</span> cos_element_wise</span>
<span id="cb20-98"><a href="#cb20-98" tabindex="-1"></a></span>
<span id="cb20-99"><a href="#cb20-99" tabindex="-1"></a><span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb20-100"><a href="#cb20-100" tabindex="-1"></a>zero_one_V <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-101"><a href="#cb20-101" tabindex="-1"></a></span>
<span id="cb20-102"><a href="#cb20-102" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb20-103"><a href="#cb20-103" tabindex="-1"></a>zero_one_M <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V, n_iteration,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_iteration, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-104"><a href="#cb20-104" tabindex="-1"></a></span>
<span id="cb20-105"><a href="#cb20-105" tabindex="-1"></a>sin_matrix <span class="ot">&lt;-</span>zero_one_M <span class="sc">*</span> sin_element_wise</span>
<span id="cb20-106"><a href="#cb20-106" tabindex="-1"></a></span>
<span id="cb20-107"><a href="#cb20-107" tabindex="-1"></a><span class="co"># Fourier Feature Mapping</span></span>
<span id="cb20-108"><a href="#cb20-108" tabindex="-1"></a>gamma_Xi <span class="ot">&lt;-</span> cos_matrix <span class="sc">+</span> sin_matrix</span>
<span id="cb20-109"><a href="#cb20-109" tabindex="-1"></a></span>
<span id="cb20-110"><a href="#cb20-110" tabindex="-1"></a>X1_map_vs_no_map <span class="ot">&lt;-</span>gamma_Xi</span>
<span id="cb20-111"><a href="#cb20-111" tabindex="-1"></a>       })</span>
<span id="cb20-112"><a href="#cb20-112" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb20-113"><a href="#cb20-113" tabindex="-1"></a><span class="co"># Add bias row. Row of 1&#39;s</span></span>
<span id="cb20-114"><a href="#cb20-114" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-115"><a href="#cb20-115" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-116"><a href="#cb20-116" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb20-117"><a href="#cb20-117" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-118"><a href="#cb20-118" tabindex="-1"></a>  X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(<span class="fu">t</span>(X1_map_vs_no_map), <span class="fu">c</span>(<span class="dv">1</span>))</span>
<span id="cb20-119"><a href="#cb20-119" tabindex="-1"></a>       },</span>
<span id="cb20-120"><a href="#cb20-120" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-121"><a href="#cb20-121" tabindex="-1"></a>  X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map)))</span>
<span id="cb20-122"><a href="#cb20-122" tabindex="-1"></a>      })</span>
<span id="cb20-123"><a href="#cb20-123" tabindex="-1"></a></span>
<span id="cb20-124"><a href="#cb20-124" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-125"><a href="#cb20-125" tabindex="-1"></a></span>
<span id="cb20-126"><a href="#cb20-126" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-127"><a href="#cb20-127" tabindex="-1"></a>  X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map)))</span>
<span id="cb20-128"><a href="#cb20-128" tabindex="-1"></a>}</span>
<span id="cb20-129"><a href="#cb20-129" tabindex="-1"></a></span>
<span id="cb20-130"><a href="#cb20-130" tabindex="-1"></a>Z2 <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1</span>
<span id="cb20-131"><a href="#cb20-131" tabindex="-1"></a></span>
<span id="cb20-132"><a href="#cb20-132" tabindex="-1"></a><span class="co"># LeakyReLU activation function</span></span>
<span id="cb20-133"><a href="#cb20-133" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-134"><a href="#cb20-134" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-135"><a href="#cb20-135" tabindex="-1"></a>  LeakyReLU <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x)) <span class="sc">%&gt;%</span> </span>
<span id="cb20-136"><a href="#cb20-136" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-137"><a href="#cb20-137" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-138"><a href="#cb20-138" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-139"><a href="#cb20-139" tabindex="-1"></a>  LeakyReLU <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-140"><a href="#cb20-140" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-141"><a href="#cb20-141" tabindex="-1"></a>}</span>
<span id="cb20-142"><a href="#cb20-142" tabindex="-1"></a></span>
<span id="cb20-143"><a href="#cb20-143" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-144"><a href="#cb20-144" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-145"><a href="#cb20-145" tabindex="-1"></a></span>
<span id="cb20-146"><a href="#cb20-146" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-147"><a href="#cb20-147" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-148"><a href="#cb20-148" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-149"><a href="#cb20-149" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-150"><a href="#cb20-150" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-151"><a href="#cb20-151" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-152"><a href="#cb20-152" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-153"><a href="#cb20-153" tabindex="-1"></a></span>
<span id="cb20-154"><a href="#cb20-154" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-155"><a href="#cb20-155" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-156"><a href="#cb20-156" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-157"><a href="#cb20-157" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-158"><a href="#cb20-158" tabindex="-1"></a>Z3 <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-159"><a href="#cb20-159" tabindex="-1"></a></span>
<span id="cb20-160"><a href="#cb20-160" tabindex="-1"></a><span class="co"># LeakyReLU activation function</span></span>
<span id="cb20-161"><a href="#cb20-161" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-162"><a href="#cb20-162" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-163"><a href="#cb20-163" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x)) <span class="sc">%&gt;%</span> </span>
<span id="cb20-164"><a href="#cb20-164" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-165"><a href="#cb20-165" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-166"><a href="#cb20-166" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-167"><a href="#cb20-167" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-168"><a href="#cb20-168" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-169"><a href="#cb20-169" tabindex="-1"></a>}</span>
<span id="cb20-170"><a href="#cb20-170" tabindex="-1"></a></span>
<span id="cb20-171"><a href="#cb20-171" tabindex="-1"></a>X3 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-172"><a href="#cb20-172" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-173"><a href="#cb20-173" tabindex="-1"></a></span>
<span id="cb20-174"><a href="#cb20-174" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3</span>
<span id="cb20-175"><a href="#cb20-175" tabindex="-1"></a>}</span>
<span id="cb20-176"><a href="#cb20-176" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-177"><a href="#cb20-177" tabindex="-1"></a><span class="co"># Define Softmax(Max)</span></span>
<span id="cb20-178"><a href="#cb20-178" tabindex="-1"></a>softmax_MAX <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb20-179"><a href="#cb20-179" tabindex="-1"></a>  <span class="co"># zi-max(z)</span></span>
<span id="cb20-180"><a href="#cb20-180" tabindex="-1"></a>  zi_max <span class="ot">&lt;-</span>z <span class="sc">-</span> <span class="fu">max</span>(z)</span>
<span id="cb20-181"><a href="#cb20-181" tabindex="-1"></a>  <span class="co"># e^(zi-max(z))</span></span>
<span id="cb20-182"><a href="#cb20-182" tabindex="-1"></a>  exp_zi_max <span class="ot">&lt;-</span><span class="fu">exp</span>(zi_max)</span>
<span id="cb20-183"><a href="#cb20-183" tabindex="-1"></a>  <span class="co"># e^(zi-max(z)-log(sum(e^(zj-max(z)))</span></span>
<span id="cb20-184"><a href="#cb20-184" tabindex="-1"></a>  <span class="fu">exp</span>(zi_max <span class="sc">-</span> <span class="fu">log</span>(<span class="fu">sum</span>(exp_zi_max)))</span>
<span id="cb20-185"><a href="#cb20-185" tabindex="-1"></a>}</span>
<span id="cb20-186"><a href="#cb20-186" tabindex="-1"></a></span>
<span id="cb20-187"><a href="#cb20-187" tabindex="-1"></a><span class="co"># note that:</span></span>
<span id="cb20-188"><a href="#cb20-188" tabindex="-1"></a><span class="co"># apply(Z_out, 1 ----means its row wise</span></span>
<span id="cb20-189"><a href="#cb20-189" tabindex="-1"></a><span class="co"># apply(Z_out, 2 ----means its column wise</span></span>
<span id="cb20-190"><a href="#cb20-190" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-191"><a href="#cb20-191" tabindex="-1"></a>X_out <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-192"><a href="#cb20-192" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-193"><a href="#cb20-193" tabindex="-1"></a><span class="co"># Code for Testing forward forward pass</span></span>
<span id="cb20-194"><a href="#cb20-194" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-195"><a href="#cb20-195" tabindex="-1"></a><span class="co"># Add bias row. Row of 1&#39;s</span></span>
<span id="cb20-196"><a href="#cb20-196" tabindex="-1"></a>X1_test <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map_testing, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map_testing)))</span>
<span id="cb20-197"><a href="#cb20-197" tabindex="-1"></a></span>
<span id="cb20-198"><a href="#cb20-198" tabindex="-1"></a>Z2_testing <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1_test</span>
<span id="cb20-199"><a href="#cb20-199" tabindex="-1"></a></span>
<span id="cb20-200"><a href="#cb20-200" tabindex="-1"></a>LeakyReLU_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-201"><a href="#cb20-201" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-202"><a href="#cb20-202" tabindex="-1"></a></span>
<span id="cb20-203"><a href="#cb20-203" tabindex="-1"></a>X2_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-204"><a href="#cb20-204" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-205"><a href="#cb20-205" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-206"><a href="#cb20-206" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-207"><a href="#cb20-207" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-208"><a href="#cb20-208" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-209"><a href="#cb20-209" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-210"><a href="#cb20-210" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-211"><a href="#cb20-211" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-212"><a href="#cb20-212" tabindex="-1"></a></span>
<span id="cb20-213"><a href="#cb20-213" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-214"><a href="#cb20-214" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-215"><a href="#cb20-215" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-216"><a href="#cb20-216" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-217"><a href="#cb20-217" tabindex="-1"></a>Z3_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-218"><a href="#cb20-218" tabindex="-1"></a></span>
<span id="cb20-219"><a href="#cb20-219" tabindex="-1"></a>LeakyReLU2_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-220"><a href="#cb20-220" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-221"><a href="#cb20-221" tabindex="-1"></a></span>
<span id="cb20-222"><a href="#cb20-222" tabindex="-1"></a>X3_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-223"><a href="#cb20-223" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-224"><a href="#cb20-224" tabindex="-1"></a></span>
<span id="cb20-225"><a href="#cb20-225" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3_testing</span>
<span id="cb20-226"><a href="#cb20-226" tabindex="-1"></a>}</span>
<span id="cb20-227"><a href="#cb20-227" tabindex="-1"></a></span>
<span id="cb20-228"><a href="#cb20-228" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-229"><a href="#cb20-229" tabindex="-1"></a>X_out_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out_testing, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-230"><a href="#cb20-230" tabindex="-1"></a></span>
<span id="cb20-231"><a href="#cb20-231" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-232"><a href="#cb20-232" tabindex="-1"></a>  <span class="co"># Code for backpropagation training</span></span>
<span id="cb20-233"><a href="#cb20-233" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-234"><a href="#cb20-234" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-235"><a href="#cb20-235" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-236"><a href="#cb20-236" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-237"><a href="#cb20-237" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-238"><a href="#cb20-238" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-239"><a href="#cb20-239" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-240"><a href="#cb20-240" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-241"><a href="#cb20-241" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t <span class="ot">&lt;-</span> (X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-242"><a href="#cb20-242" tabindex="-1"></a></span>
<span id="cb20-243"><a href="#cb20-243" tabindex="-1"></a>  dZ3_out_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-244"><a href="#cb20-244" tabindex="-1"></a></span>
<span id="cb20-245"><a href="#cb20-245" tabindex="-1"></a>  <span class="co"># dC/dM2 = [(dC/dX_out)^t *(dX_out/dZ3_out)^t]^t * dZ3_out/dM2</span></span>
<span id="cb20-246"><a href="#cb20-246" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span> dc_dX_out_t_times_dX_out_dZ3_out_t <span class="sc">%*%</span> dZ3_out_dM2 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-247"><a href="#cb20-247" tabindex="-1"></a></span>
<span id="cb20-248"><a href="#cb20-248" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-249"><a href="#cb20-249" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-250"><a href="#cb20-250" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-251"><a href="#cb20-251" tabindex="-1"></a>  <span class="co"># (dC/dX_out)^t *(dX_out/dZ3_out)^t</span></span>
<span id="cb20-252"><a href="#cb20-252" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t)</span>
<span id="cb20-253"><a href="#cb20-253" tabindex="-1"></a></span>
<span id="cb20-254"><a href="#cb20-254" tabindex="-1"></a>  dZ3_out_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-255"><a href="#cb20-255" tabindex="-1"></a></span>
<span id="cb20-256"><a href="#cb20-256" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-257"><a href="#cb20-257" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-258"><a href="#cb20-258" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-259"><a href="#cb20-259" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-260"><a href="#cb20-260" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-261"><a href="#cb20-261" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-262"><a href="#cb20-262" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-263"><a href="#cb20-263" tabindex="-1"></a>  }</span>
<span id="cb20-264"><a href="#cb20-264" tabindex="-1"></a></span>
<span id="cb20-265"><a href="#cb20-265" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-266"><a href="#cb20-266" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-267"><a href="#cb20-267" tabindex="-1"></a></span>
<span id="cb20-268"><a href="#cb20-268" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-269"><a href="#cb20-269" tabindex="-1"></a></span>
<span id="cb20-270"><a href="#cb20-270" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="sc">%*%</span> dZ3_out_dX2)) <span class="sc">*</span> dX2_dZ2) <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-271"><a href="#cb20-271" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-272"><a href="#cb20-272" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-273"><a href="#cb20-273" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-274"><a href="#cb20-274" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-275"><a href="#cb20-275" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-276"><a href="#cb20-276" tabindex="-1"></a>  <span class="co"># DC/DM3</span></span>
<span id="cb20-277"><a href="#cb20-277" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-278"><a href="#cb20-278" tabindex="-1"></a>  dc_dZ_out_t <span class="ot">&lt;-</span>(X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-279"><a href="#cb20-279" tabindex="-1"></a></span>
<span id="cb20-280"><a href="#cb20-280" tabindex="-1"></a>  dZ_out_dM3 <span class="ot">&lt;-</span><span class="fu">t</span>(X3)</span>
<span id="cb20-281"><a href="#cb20-281" tabindex="-1"></a></span>
<span id="cb20-282"><a href="#cb20-282" tabindex="-1"></a>  dC_dM3 <span class="ot">&lt;-</span>dc_dZ_out_t <span class="sc">%*%</span> dZ_out_dM3 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-283"><a href="#cb20-283" tabindex="-1"></a></span>
<span id="cb20-284"><a href="#cb20-284" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-285"><a href="#cb20-285" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-286"><a href="#cb20-286" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-287"><a href="#cb20-287" tabindex="-1"></a>  dc_dZ_out <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dZ_out_t)</span>
<span id="cb20-288"><a href="#cb20-288" tabindex="-1"></a></span>
<span id="cb20-289"><a href="#cb20-289" tabindex="-1"></a>  dZ4_dX3 <span class="ot">&lt;-</span> M3[, <span class="sc">-</span><span class="fu">ncol</span>(M3)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-290"><a href="#cb20-290" tabindex="-1"></a></span>
<span id="cb20-291"><a href="#cb20-291" tabindex="-1"></a>  <span class="co"># dX3/dZ3 (element wise)</span></span>
<span id="cb20-292"><a href="#cb20-292" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-293"><a href="#cb20-293" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-294"><a href="#cb20-294" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-295"><a href="#cb20-295" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-296"><a href="#cb20-296" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-297"><a href="#cb20-297" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-298"><a href="#cb20-298" tabindex="-1"></a>  }</span>
<span id="cb20-299"><a href="#cb20-299" tabindex="-1"></a></span>
<span id="cb20-300"><a href="#cb20-300" tabindex="-1"></a>  dX3_dZ3 <span class="ot">&lt;-</span> dX3_dZ3_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX3_dZ3_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-301"><a href="#cb20-301" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-302"><a href="#cb20-302" tabindex="-1"></a></span>
<span id="cb20-303"><a href="#cb20-303" tabindex="-1"></a>  dZ3_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-304"><a href="#cb20-304" tabindex="-1"></a></span>
<span id="cb20-305"><a href="#cb20-305" tabindex="-1"></a>  dC_dZ3 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dc_dZ_out <span class="sc">%*%</span> dZ4_dX3)) <span class="sc">*</span> dX3_dZ3</span>
<span id="cb20-306"><a href="#cb20-306" tabindex="-1"></a></span>
<span id="cb20-307"><a href="#cb20-307" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span>dC_dZ3 <span class="sc">%*%</span> dZ3_dM2</span>
<span id="cb20-308"><a href="#cb20-308" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-309"><a href="#cb20-309" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-310"><a href="#cb20-310" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-311"><a href="#cb20-311" tabindex="-1"></a>  dC_dZ3_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dC_dZ3)</span>
<span id="cb20-312"><a href="#cb20-312" tabindex="-1"></a></span>
<span id="cb20-313"><a href="#cb20-313" tabindex="-1"></a>  dZ3_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-314"><a href="#cb20-314" tabindex="-1"></a></span>
<span id="cb20-315"><a href="#cb20-315" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-316"><a href="#cb20-316" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-317"><a href="#cb20-317" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-318"><a href="#cb20-318" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-319"><a href="#cb20-319" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-320"><a href="#cb20-320" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-321"><a href="#cb20-321" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-322"><a href="#cb20-322" tabindex="-1"></a>  }</span>
<span id="cb20-323"><a href="#cb20-323" tabindex="-1"></a></span>
<span id="cb20-324"><a href="#cb20-324" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-325"><a href="#cb20-325" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-326"><a href="#cb20-326" tabindex="-1"></a></span>
<span id="cb20-327"><a href="#cb20-327" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-328"><a href="#cb20-328" tabindex="-1"></a></span>
<span id="cb20-329"><a href="#cb20-329" tabindex="-1"></a>  dC_dZ2 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dC_dZ3_t <span class="sc">%*%</span> dZ3_dX2)) <span class="sc">*</span> dX2_dZ2</span>
<span id="cb20-330"><a href="#cb20-330" tabindex="-1"></a></span>
<span id="cb20-331"><a href="#cb20-331" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span>dC_dZ2 <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-332"><a href="#cb20-332" tabindex="-1"></a>}</span>
<span id="cb20-333"><a href="#cb20-333" tabindex="-1"></a></span>
<span id="cb20-334"><a href="#cb20-334" tabindex="-1"></a><span class="do">##############################################################</span></span>
<span id="cb20-335"><a href="#cb20-335" tabindex="-1"></a>  <span class="co"># Gradient Descent</span></span>
<span id="cb20-336"><a href="#cb20-336" tabindex="-1"></a><span class="do">#############################################################</span></span>
<span id="cb20-337"><a href="#cb20-337" tabindex="-1"></a><span class="co"># note that in SGD and mini-batch, Epochs = Epoch*number_iterations_epoch</span></span>
<span id="cb20-338"><a href="#cb20-338" tabindex="-1"></a><span class="co"># Update weights and biases (based on backpropagation)</span></span>
<span id="cb20-339"><a href="#cb20-339" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-340"><a href="#cb20-340" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-341"><a href="#cb20-341" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-342"><a href="#cb20-342" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-343"><a href="#cb20-343" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-344"><a href="#cb20-344" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-345"><a href="#cb20-345" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-346"><a href="#cb20-346" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-347"><a href="#cb20-347" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-348"><a href="#cb20-348" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-349"><a href="#cb20-349" tabindex="-1"></a></span>
<span id="cb20-350"><a href="#cb20-350" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-351"><a href="#cb20-351" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-352"><a href="#cb20-352" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-353"><a href="#cb20-353" tabindex="-1"></a>}</span>
<span id="cb20-354"><a href="#cb20-354" tabindex="-1"></a>       },</span>
<span id="cb20-355"><a href="#cb20-355" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-356"><a href="#cb20-356" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-357"><a href="#cb20-357" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-358"><a href="#cb20-358" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-359"><a href="#cb20-359" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-360"><a href="#cb20-360" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-361"><a href="#cb20-361" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-362"><a href="#cb20-362" tabindex="-1"></a></span>
<span id="cb20-363"><a href="#cb20-363" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-364"><a href="#cb20-364" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-365"><a href="#cb20-365" tabindex="-1"></a>}</span>
<span id="cb20-366"><a href="#cb20-366" tabindex="-1"></a>       },</span>
<span id="cb20-367"><a href="#cb20-367" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-368"><a href="#cb20-368" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-369"><a href="#cb20-369" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-370"><a href="#cb20-370" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-371"><a href="#cb20-371" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-372"><a href="#cb20-372" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-373"><a href="#cb20-373" tabindex="-1"></a><span class="co"># M2 </span></span>
<span id="cb20-374"><a href="#cb20-374" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-375"><a href="#cb20-375" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-376"><a href="#cb20-376" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-377"><a href="#cb20-377" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-378"><a href="#cb20-378" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-379"><a href="#cb20-379" tabindex="-1"></a></span>
<span id="cb20-380"><a href="#cb20-380" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-381"><a href="#cb20-381" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-382"><a href="#cb20-382" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-383"><a href="#cb20-383" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-384"><a href="#cb20-384" tabindex="-1"></a></span>
<span id="cb20-385"><a href="#cb20-385" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-386"><a href="#cb20-386" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-387"><a href="#cb20-387" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-388"><a href="#cb20-388" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-389"><a href="#cb20-389" tabindex="-1"></a>}</span>
<span id="cb20-390"><a href="#cb20-390" tabindex="-1"></a></span>
<span id="cb20-391"><a href="#cb20-391" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-392"><a href="#cb20-392" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-393"><a href="#cb20-393" tabindex="-1"></a></span>
<span id="cb20-394"><a href="#cb20-394" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-395"><a href="#cb20-395" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-396"><a href="#cb20-396" tabindex="-1"></a></span>
<span id="cb20-397"><a href="#cb20-397" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-398"><a href="#cb20-398" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-399"><a href="#cb20-399" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-400"><a href="#cb20-400" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-401"><a href="#cb20-401" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-402"><a href="#cb20-402" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-403"><a href="#cb20-403" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-404"><a href="#cb20-404" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1 </span>
<span id="cb20-405"><a href="#cb20-405" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-406"><a href="#cb20-406" tabindex="-1"></a></span>
<span id="cb20-407"><a href="#cb20-407" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-408"><a href="#cb20-408" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-409"><a href="#cb20-409" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-410"><a href="#cb20-410" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-411"><a href="#cb20-411" tabindex="-1"></a></span>
<span id="cb20-412"><a href="#cb20-412" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-413"><a href="#cb20-413" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-414"><a href="#cb20-414" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-415"><a href="#cb20-415" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-416"><a href="#cb20-416" tabindex="-1"></a>}</span>
<span id="cb20-417"><a href="#cb20-417" tabindex="-1"></a></span>
<span id="cb20-418"><a href="#cb20-418" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-419"><a href="#cb20-419" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-420"><a href="#cb20-420" tabindex="-1"></a></span>
<span id="cb20-421"><a href="#cb20-421" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-422"><a href="#cb20-422" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-423"><a href="#cb20-423" tabindex="-1"></a></span>
<span id="cb20-424"><a href="#cb20-424" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-425"><a href="#cb20-425" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-426"><a href="#cb20-426" tabindex="-1"></a></span>
<span id="cb20-427"><a href="#cb20-427" tabindex="-1"></a>}</span>
<span id="cb20-428"><a href="#cb20-428" tabindex="-1"></a>       }</span>
<span id="cb20-429"><a href="#cb20-429" tabindex="-1"></a>)</span>
<span id="cb20-430"><a href="#cb20-430" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-431"><a href="#cb20-431" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-432"><a href="#cb20-432" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-433"><a href="#cb20-433" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-434"><a href="#cb20-434" tabindex="-1"></a></span>
<span id="cb20-435"><a href="#cb20-435" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-436"><a href="#cb20-436" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-437"><a href="#cb20-437" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-438"><a href="#cb20-438" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-439"><a href="#cb20-439" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-440"><a href="#cb20-440" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-441"><a href="#cb20-441" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-442"><a href="#cb20-442" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-443"><a href="#cb20-443" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-444"><a href="#cb20-444" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-445"><a href="#cb20-445" tabindex="-1"></a></span>
<span id="cb20-446"><a href="#cb20-446" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-447"><a href="#cb20-447" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-448"><a href="#cb20-448" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-449"><a href="#cb20-449" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-450"><a href="#cb20-450" tabindex="-1"></a>}</span>
<span id="cb20-451"><a href="#cb20-451" tabindex="-1"></a>       },</span>
<span id="cb20-452"><a href="#cb20-452" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-453"><a href="#cb20-453" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-454"><a href="#cb20-454" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-455"><a href="#cb20-455" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-456"><a href="#cb20-456" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-457"><a href="#cb20-457" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-458"><a href="#cb20-458" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-459"><a href="#cb20-459" tabindex="-1"></a></span>
<span id="cb20-460"><a href="#cb20-460" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-461"><a href="#cb20-461" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-462"><a href="#cb20-462" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-463"><a href="#cb20-463" tabindex="-1"></a>}</span>
<span id="cb20-464"><a href="#cb20-464" tabindex="-1"></a>       },</span>
<span id="cb20-465"><a href="#cb20-465" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-466"><a href="#cb20-466" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-467"><a href="#cb20-467" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-468"><a href="#cb20-468" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-469"><a href="#cb20-469" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-470"><a href="#cb20-470" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-471"><a href="#cb20-471" tabindex="-1"></a><span class="co"># M3</span></span>
<span id="cb20-472"><a href="#cb20-472" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-473"><a href="#cb20-473" tabindex="-1"></a></span>
<span id="cb20-474"><a href="#cb20-474" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-475"><a href="#cb20-475" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-476"><a href="#cb20-476" tabindex="-1"></a>v1_prev_M3 <span class="ot">&lt;-</span> v1_M3</span>
<span id="cb20-477"><a href="#cb20-477" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-478"><a href="#cb20-478" tabindex="-1"></a></span>
<span id="cb20-479"><a href="#cb20-479" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-480"><a href="#cb20-480" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-481"><a href="#cb20-481" tabindex="-1"></a>v2_prev_M3 <span class="ot">&lt;-</span> v2_M3</span>
<span id="cb20-482"><a href="#cb20-482" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM3<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-483"><a href="#cb20-483" tabindex="-1"></a></span>
<span id="cb20-484"><a href="#cb20-484" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-485"><a href="#cb20-485" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-486"><a href="#cb20-486" tabindex="-1"></a>  v1_M3 <span class="ot">&lt;-</span> v1_prev_M3</span>
<span id="cb20-487"><a href="#cb20-487" tabindex="-1"></a>  v2_M3 <span class="ot">&lt;-</span> v2_prev_M3</span>
<span id="cb20-488"><a href="#cb20-488" tabindex="-1"></a>}</span>
<span id="cb20-489"><a href="#cb20-489" tabindex="-1"></a></span>
<span id="cb20-490"><a href="#cb20-490" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-491"><a href="#cb20-491" tabindex="-1"></a>v1_hat_M3 <span class="ot">&lt;-</span> v1_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-492"><a href="#cb20-492" tabindex="-1"></a></span>
<span id="cb20-493"><a href="#cb20-493" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-494"><a href="#cb20-494" tabindex="-1"></a>v2_hat_M3 <span class="ot">&lt;-</span> v2_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-495"><a href="#cb20-495" tabindex="-1"></a></span>
<span id="cb20-496"><a href="#cb20-496" tabindex="-1"></a><span class="co"># Update M3 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-497"><a href="#cb20-497" tabindex="-1"></a>M3 <span class="ot">&lt;-</span> M3 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M3 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M3) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-498"><a href="#cb20-498" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-499"><a href="#cb20-499" tabindex="-1"></a><span class="co"># M2</span></span>
<span id="cb20-500"><a href="#cb20-500" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-501"><a href="#cb20-501" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-502"><a href="#cb20-502" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-503"><a href="#cb20-503" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-504"><a href="#cb20-504" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-505"><a href="#cb20-505" tabindex="-1"></a></span>
<span id="cb20-506"><a href="#cb20-506" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-507"><a href="#cb20-507" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-508"><a href="#cb20-508" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-509"><a href="#cb20-509" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-510"><a href="#cb20-510" tabindex="-1"></a></span>
<span id="cb20-511"><a href="#cb20-511" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-512"><a href="#cb20-512" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-513"><a href="#cb20-513" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-514"><a href="#cb20-514" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-515"><a href="#cb20-515" tabindex="-1"></a>}</span>
<span id="cb20-516"><a href="#cb20-516" tabindex="-1"></a></span>
<span id="cb20-517"><a href="#cb20-517" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-518"><a href="#cb20-518" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-519"><a href="#cb20-519" tabindex="-1"></a></span>
<span id="cb20-520"><a href="#cb20-520" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-521"><a href="#cb20-521" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-522"><a href="#cb20-522" tabindex="-1"></a></span>
<span id="cb20-523"><a href="#cb20-523" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-524"><a href="#cb20-524" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-525"><a href="#cb20-525" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-526"><a href="#cb20-526" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-527"><a href="#cb20-527" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-528"><a href="#cb20-528" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-529"><a href="#cb20-529" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-530"><a href="#cb20-530" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1</span>
<span id="cb20-531"><a href="#cb20-531" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-532"><a href="#cb20-532" tabindex="-1"></a></span>
<span id="cb20-533"><a href="#cb20-533" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-534"><a href="#cb20-534" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-535"><a href="#cb20-535" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-536"><a href="#cb20-536" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-537"><a href="#cb20-537" tabindex="-1"></a></span>
<span id="cb20-538"><a href="#cb20-538" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-539"><a href="#cb20-539" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-540"><a href="#cb20-540" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-541"><a href="#cb20-541" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-542"><a href="#cb20-542" tabindex="-1"></a>}</span>
<span id="cb20-543"><a href="#cb20-543" tabindex="-1"></a></span>
<span id="cb20-544"><a href="#cb20-544" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-545"><a href="#cb20-545" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-546"><a href="#cb20-546" tabindex="-1"></a></span>
<span id="cb20-547"><a href="#cb20-547" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-548"><a href="#cb20-548" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-549"><a href="#cb20-549" tabindex="-1"></a></span>
<span id="cb20-550"><a href="#cb20-550" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-551"><a href="#cb20-551" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-552"><a href="#cb20-552" tabindex="-1"></a></span>
<span id="cb20-553"><a href="#cb20-553" tabindex="-1"></a>}</span>
<span id="cb20-554"><a href="#cb20-554" tabindex="-1"></a>       }</span>
<span id="cb20-555"><a href="#cb20-555" tabindex="-1"></a>)</span>
<span id="cb20-556"><a href="#cb20-556" tabindex="-1"></a></span>
<span id="cb20-557"><a href="#cb20-557" tabindex="-1"></a>  <span class="co">#Store M3 for each epoch</span></span>
<span id="cb20-558"><a href="#cb20-558" tabindex="-1"></a>M3_list[[epoch]] <span class="ot">&lt;-</span>M3</span>
<span id="cb20-559"><a href="#cb20-559" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-560"><a href="#cb20-560" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-561"><a href="#cb20-561" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-562"><a href="#cb20-562" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-563"><a href="#cb20-563" tabindex="-1"></a></span>
<span id="cb20-564"><a href="#cb20-564" tabindex="-1"></a>}</span>
<span id="cb20-565"><a href="#cb20-565" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-566"><a href="#cb20-566" tabindex="-1"></a>  <span class="co"># Compute Mean Categorical Cross Entropy Loss and store it in the CCEntropy_Loss dataframe</span></span>
<span id="cb20-567"><a href="#cb20-567" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-568"><a href="#cb20-568" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-569"><a href="#cb20-569" tabindex="-1"></a>  <span class="co"># Mean Categorical Cross Entropy Loss Training</span></span>
<span id="cb20-570"><a href="#cb20-570" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-571"><a href="#cb20-571" tabindex="-1"></a>  <span class="co">#Categorical Cross Entropy Loss</span></span>
<span id="cb20-572"><a href="#cb20-572" tabindex="-1"></a>  Cost_training <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_training <span class="sc">*</span> <span class="fu">log</span>(X_out) </span>
<span id="cb20-573"><a href="#cb20-573" tabindex="-1"></a></span>
<span id="cb20-574"><a href="#cb20-574" tabindex="-1"></a>  <span class="co"># Sums up the CCE Loss for each observation</span></span>
<span id="cb20-575"><a href="#cb20-575" tabindex="-1"></a>  Cost_training <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_training)</span>
<span id="cb20-576"><a href="#cb20-576" tabindex="-1"></a></span>
<span id="cb20-577"><a href="#cb20-577" tabindex="-1"></a>  <span class="co">#Stores mean CCE Loss X_out for each epoch</span></span>
<span id="cb20-578"><a href="#cb20-578" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch] <span class="ot">&lt;-</span><span class="fu">round</span>(<span class="fu">mean</span>(Cost_training), <span class="dv">2</span>)</span>
<span id="cb20-579"><a href="#cb20-579" tabindex="-1"></a></span>
<span id="cb20-580"><a href="#cb20-580" tabindex="-1"></a>  <span class="co">#Stores iteration number(it says [epoch] but its actually iteration number)</span></span>
<span id="cb20-581"><a href="#cb20-581" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>iteration[epoch] <span class="ot">&lt;-</span>epoch</span>
<span id="cb20-582"><a href="#cb20-582" tabindex="-1"></a></span>
<span id="cb20-583"><a href="#cb20-583" tabindex="-1"></a>  <span class="co">#Stores epoch number</span></span>
<span id="cb20-584"><a href="#cb20-584" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>epoch[epoch] <span class="ot">&lt;-</span><span class="fu">round</span>(CCEntropy_Loss<span class="sc">$</span>iteration[epoch]<span class="sc">/</span>number_iterations_epoch, <span class="dv">2</span>)</span>
<span id="cb20-585"><a href="#cb20-585" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-586"><a href="#cb20-586" tabindex="-1"></a>  <span class="co"># Mean Categorical Cross Entropy Loss Testing</span></span>
<span id="cb20-587"><a href="#cb20-587" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-588"><a href="#cb20-588" tabindex="-1"></a>  <span class="co">#Categorical Cross Entropy Loss</span></span>
<span id="cb20-589"><a href="#cb20-589" tabindex="-1"></a>  Cost_testing <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_testing <span class="sc">*</span> <span class="fu">log</span>(X_out_testing)  </span>
<span id="cb20-590"><a href="#cb20-590" tabindex="-1"></a></span>
<span id="cb20-591"><a href="#cb20-591" tabindex="-1"></a>  <span class="co"># Sums up the CCE Loss for each observation  </span></span>
<span id="cb20-592"><a href="#cb20-592" tabindex="-1"></a>  Cost_testing <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_testing)</span>
<span id="cb20-593"><a href="#cb20-593" tabindex="-1"></a></span>
<span id="cb20-594"><a href="#cb20-594" tabindex="-1"></a>  <span class="co">#Stores mean CCE Loss X_out (testing) for each epoch</span></span>
<span id="cb20-595"><a href="#cb20-595" tabindex="-1"></a>  CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch] <span class="ot">&lt;-</span><span class="fu">round</span>(<span class="fu">mean</span>(Cost_testing), <span class="dv">2</span>)</span>
<span id="cb20-596"><a href="#cb20-596" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-597"><a href="#cb20-597" tabindex="-1"></a>  <span class="co"># Compute Accuracy for each epoch</span></span>
<span id="cb20-598"><a href="#cb20-598" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-599"><a href="#cb20-599" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-600"><a href="#cb20-600" tabindex="-1"></a>  <span class="co">#Accuracy Training</span></span>
<span id="cb20-601"><a href="#cb20-601" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-602"><a href="#cb20-602" tabindex="-1"></a>  <span class="co"># Find the highest probability for each observation in X_out</span></span>
<span id="cb20-603"><a href="#cb20-603" tabindex="-1"></a>  X_out_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out, <span class="dv">2</span>, max)</span>
<span id="cb20-604"><a href="#cb20-604" tabindex="-1"></a></span>
<span id="cb20-605"><a href="#cb20-605" tabindex="-1"></a>  X_out_hadamard_Y_training <span class="ot">&lt;-</span>X_out <span class="sc">*</span> Y_One_Hot_Encoding_training</span>
<span id="cb20-606"><a href="#cb20-606" tabindex="-1"></a></span>
<span id="cb20-607"><a href="#cb20-607" tabindex="-1"></a>  <span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-608"><a href="#cb20-608" tabindex="-1"></a>  X_out_hadamard_Y_v_training <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_hadamard_Y_training, <span class="dv">2</span>, max)</span>
<span id="cb20-609"><a href="#cb20-609" tabindex="-1"></a></span>
<span id="cb20-610"><a href="#cb20-610" tabindex="-1"></a>  <span class="co"># Subtract the two vectors</span></span>
<span id="cb20-611"><a href="#cb20-611" tabindex="-1"></a>  difference_training <span class="ot">&lt;-</span> X_out_highest <span class="sc">-</span> X_out_hadamard_Y_v_training</span>
<span id="cb20-612"><a href="#cb20-612" tabindex="-1"></a></span>
<span id="cb20-613"><a href="#cb20-613" tabindex="-1"></a>  <span class="co"># Count the number of zeros</span></span>
<span id="cb20-614"><a href="#cb20-614" tabindex="-1"></a>  count_zeros_training <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_training <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-615"><a href="#cb20-615" tabindex="-1"></a></span>
<span id="cb20-616"><a href="#cb20-616" tabindex="-1"></a>  <span class="co"># Accuaracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-617"><a href="#cb20-617" tabindex="-1"></a>  accuracy_percent_training <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_training <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_training, <span class="dv">2</span>)</span>
<span id="cb20-618"><a href="#cb20-618" tabindex="-1"></a></span>
<span id="cb20-619"><a href="#cb20-619" tabindex="-1"></a>  <span class="co">#Stores training accuracy percentage for each epoch</span></span>
<span id="cb20-620"><a href="#cb20-620" tabindex="-1"></a>  Accuracy_Percent<span class="sc">$</span>Training_percent[epoch] <span class="ot">&lt;-</span>accuracy_percent_training</span>
<span id="cb20-621"><a href="#cb20-621" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-622"><a href="#cb20-622" tabindex="-1"></a><span class="co">#Accuracy Testing</span></span>
<span id="cb20-623"><a href="#cb20-623" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-624"><a href="#cb20-624" tabindex="-1"></a><span class="co"># Find the highest probability for each observation in X_out_testing</span></span>
<span id="cb20-625"><a href="#cb20-625" tabindex="-1"></a>X_out_testing_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-626"><a href="#cb20-626" tabindex="-1"></a></span>
<span id="cb20-627"><a href="#cb20-627" tabindex="-1"></a>X_out_testing_hadamard_Y_testing <span class="ot">&lt;-</span>X_out_testing <span class="sc">*</span> Y_One_Hot_Encoding_testing</span>
<span id="cb20-628"><a href="#cb20-628" tabindex="-1"></a></span>
<span id="cb20-629"><a href="#cb20-629" tabindex="-1"></a><span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-630"><a href="#cb20-630" tabindex="-1"></a>X_out_testing_hadamard_Y_v_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing_hadamard_Y_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-631"><a href="#cb20-631" tabindex="-1"></a></span>
<span id="cb20-632"><a href="#cb20-632" tabindex="-1"></a><span class="co"># Subtract the two vectors</span></span>
<span id="cb20-633"><a href="#cb20-633" tabindex="-1"></a>difference_testing <span class="ot">&lt;-</span> X_out_testing_highest <span class="sc">-</span> X_out_testing_hadamard_Y_v_testing</span>
<span id="cb20-634"><a href="#cb20-634" tabindex="-1"></a></span>
<span id="cb20-635"><a href="#cb20-635" tabindex="-1"></a><span class="co"># Count the number of zeros</span></span>
<span id="cb20-636"><a href="#cb20-636" tabindex="-1"></a>count_zeros_testing <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_testing <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-637"><a href="#cb20-637" tabindex="-1"></a></span>
<span id="cb20-638"><a href="#cb20-638" tabindex="-1"></a><span class="co"># Accuaracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-639"><a href="#cb20-639" tabindex="-1"></a>accuracy_percent_testing <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_testing <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_testing, <span class="dv">2</span>)</span>
<span id="cb20-640"><a href="#cb20-640" tabindex="-1"></a></span>
<span id="cb20-641"><a href="#cb20-641" tabindex="-1"></a><span class="co">#Stores testing accuracy percentage for each epoch</span></span>
<span id="cb20-642"><a href="#cb20-642" tabindex="-1"></a>Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch] <span class="ot">&lt;-</span>accuracy_percent_testing</span>
<span id="cb20-643"><a href="#cb20-643" tabindex="-1"></a> </span>
<span id="cb20-644"><a href="#cb20-644" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-645"><a href="#cb20-645" tabindex="-1"></a>  <span class="co"># Comment out #cat() to prevent results from being printed out on console</span></span>
<span id="cb20-646"><a href="#cb20-646" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;iteration: &quot;</span>, epoch, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-647"><a href="#cb20-647" tabindex="-1"></a>    <span class="st">&quot;epoch: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>epoch[epoch], <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-648"><a href="#cb20-648" tabindex="-1"></a>    <span class="st">&quot;Training Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-649"><a href="#cb20-649" tabindex="-1"></a>    <span class="st">&quot;Testing Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-650"><a href="#cb20-650" tabindex="-1"></a>    <span class="st">&quot;Training Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Training_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-651"><a href="#cb20-651" tabindex="-1"></a>    <span class="st">&quot;Testing Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb20-652"><a href="#cb20-652" tabindex="-1"></a></span>
<span id="cb20-653"><a href="#cb20-653" tabindex="-1"></a>}</span>
<span id="cb20-654"><a href="#cb20-654" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-655"><a href="#cb20-655" tabindex="-1"></a><span class="co">#12</span></span>
<span id="cb20-656"><a href="#cb20-656" tabindex="-1"></a><span class="co">#BATCH GRADIENT DESCENT</span></span>
<span id="cb20-657"><a href="#cb20-657" tabindex="-1"></a><span class="do">#############################################################################################################</span></span>
<span id="cb20-658"><a href="#cb20-658" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-659"><a href="#cb20-659" tabindex="-1"></a><span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs) {</span>
<span id="cb20-660"><a href="#cb20-660" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-661"><a href="#cb20-661" tabindex="-1"></a>  <span class="co"># Code for Training forward forward pass</span></span>
<span id="cb20-662"><a href="#cb20-662" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-663"><a href="#cb20-663" tabindex="-1"></a><span class="co"># Batch Gradient Descent</span></span>
<span id="cb20-664"><a href="#cb20-664" tabindex="-1"></a>X1 <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map)))</span>
<span id="cb20-665"><a href="#cb20-665" tabindex="-1"></a></span>
<span id="cb20-666"><a href="#cb20-666" tabindex="-1"></a>Z2 <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1</span>
<span id="cb20-667"><a href="#cb20-667" tabindex="-1"></a></span>
<span id="cb20-668"><a href="#cb20-668" tabindex="-1"></a>LeakyReLU <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-669"><a href="#cb20-669" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-670"><a href="#cb20-670" tabindex="-1"></a></span>
<span id="cb20-671"><a href="#cb20-671" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-672"><a href="#cb20-672" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-673"><a href="#cb20-673" tabindex="-1"></a></span>
<span id="cb20-674"><a href="#cb20-674" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-675"><a href="#cb20-675" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-676"><a href="#cb20-676" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-677"><a href="#cb20-677" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-678"><a href="#cb20-678" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-679"><a href="#cb20-679" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-680"><a href="#cb20-680" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-681"><a href="#cb20-681" tabindex="-1"></a></span>
<span id="cb20-682"><a href="#cb20-682" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-683"><a href="#cb20-683" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-684"><a href="#cb20-684" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-685"><a href="#cb20-685" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-686"><a href="#cb20-686" tabindex="-1"></a>Z3 <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2</span>
<span id="cb20-687"><a href="#cb20-687" tabindex="-1"></a></span>
<span id="cb20-688"><a href="#cb20-688" tabindex="-1"></a><span class="co"># LeakyReLU activation function</span></span>
<span id="cb20-689"><a href="#cb20-689" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-690"><a href="#cb20-690" tabindex="-1"></a>  <span class="co"># SGD </span></span>
<span id="cb20-691"><a href="#cb20-691" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x)) <span class="sc">%&gt;%</span> </span>
<span id="cb20-692"><a href="#cb20-692" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-693"><a href="#cb20-693" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-694"><a href="#cb20-694" tabindex="-1"></a>  <span class="co"># Mini Batch Gradient Descent</span></span>
<span id="cb20-695"><a href="#cb20-695" tabindex="-1"></a>  LeakyReLU2 <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-696"><a href="#cb20-696" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-697"><a href="#cb20-697" tabindex="-1"></a>}</span>
<span id="cb20-698"><a href="#cb20-698" tabindex="-1"></a></span>
<span id="cb20-699"><a href="#cb20-699" tabindex="-1"></a>X3 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-700"><a href="#cb20-700" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-701"><a href="#cb20-701" tabindex="-1"></a></span>
<span id="cb20-702"><a href="#cb20-702" tabindex="-1"></a>Z_out <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3</span>
<span id="cb20-703"><a href="#cb20-703" tabindex="-1"></a>}</span>
<span id="cb20-704"><a href="#cb20-704" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-705"><a href="#cb20-705" tabindex="-1"></a></span>
<span id="cb20-706"><a href="#cb20-706" tabindex="-1"></a><span class="co"># Define Softmax(Max)</span></span>
<span id="cb20-707"><a href="#cb20-707" tabindex="-1"></a>softmax_MAX <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb20-708"><a href="#cb20-708" tabindex="-1"></a>  <span class="co"># zi-max(z)</span></span>
<span id="cb20-709"><a href="#cb20-709" tabindex="-1"></a>  zi_max <span class="ot">&lt;-</span> z <span class="sc">-</span> <span class="fu">max</span>(z)</span>
<span id="cb20-710"><a href="#cb20-710" tabindex="-1"></a>  <span class="co"># e^(zi-max(z))</span></span>
<span id="cb20-711"><a href="#cb20-711" tabindex="-1"></a>  exp_zi_max <span class="ot">&lt;-</span> <span class="fu">exp</span>(zi_max)</span>
<span id="cb20-712"><a href="#cb20-712" tabindex="-1"></a>  <span class="co"># e^(zi-max(z)-log(sum(e^(zj-max(z)))</span></span>
<span id="cb20-713"><a href="#cb20-713" tabindex="-1"></a>  <span class="fu">exp</span>(zi_max <span class="sc">-</span> <span class="fu">log</span>(<span class="fu">sum</span>(exp_zi_max)))</span>
<span id="cb20-714"><a href="#cb20-714" tabindex="-1"></a>}</span>
<span id="cb20-715"><a href="#cb20-715" tabindex="-1"></a></span>
<span id="cb20-716"><a href="#cb20-716" tabindex="-1"></a><span class="co"># note that:</span></span>
<span id="cb20-717"><a href="#cb20-717" tabindex="-1"></a><span class="co"># apply(Z_out, 1 ----means its row wise</span></span>
<span id="cb20-718"><a href="#cb20-718" tabindex="-1"></a><span class="co"># apply(Z_out, 2 ----means its column wise</span></span>
<span id="cb20-719"><a href="#cb20-719" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-720"><a href="#cb20-720" tabindex="-1"></a>X_out <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-721"><a href="#cb20-721" tabindex="-1"></a></span>
<span id="cb20-722"><a href="#cb20-722" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-723"><a href="#cb20-723" tabindex="-1"></a><span class="co"># Code for Testing forward forward pass</span></span>
<span id="cb20-724"><a href="#cb20-724" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-725"><a href="#cb20-725" tabindex="-1"></a><span class="co"># Add bias row. Row of 1&#39;s</span></span>
<span id="cb20-726"><a href="#cb20-726" tabindex="-1"></a>X1_test <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map_testing, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map_testing)))</span>
<span id="cb20-727"><a href="#cb20-727" tabindex="-1"></a></span>
<span id="cb20-728"><a href="#cb20-728" tabindex="-1"></a>Z2_testing <span class="ot">&lt;-</span> M1 <span class="sc">%*%</span> X1_test</span>
<span id="cb20-729"><a href="#cb20-729" tabindex="-1"></a></span>
<span id="cb20-730"><a href="#cb20-730" tabindex="-1"></a>LeakyReLU_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-731"><a href="#cb20-731" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-732"><a href="#cb20-732" tabindex="-1"></a></span>
<span id="cb20-733"><a href="#cb20-733" tabindex="-1"></a>X2_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-734"><a href="#cb20-734" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-735"><a href="#cb20-735" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-736"><a href="#cb20-736" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb20-737"><a href="#cb20-737" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-738"><a href="#cb20-738" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-739"><a href="#cb20-739" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-740"><a href="#cb20-740" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-741"><a href="#cb20-741" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-742"><a href="#cb20-742" tabindex="-1"></a></span>
<span id="cb20-743"><a href="#cb20-743" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-744"><a href="#cb20-744" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-745"><a href="#cb20-745" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-746"><a href="#cb20-746" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-747"><a href="#cb20-747" tabindex="-1"></a>Z3_testing <span class="ot">&lt;-</span> M2 <span class="sc">%*%</span> X2_testing</span>
<span id="cb20-748"><a href="#cb20-748" tabindex="-1"></a></span>
<span id="cb20-749"><a href="#cb20-749" tabindex="-1"></a>LeakyReLU2_testing <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3_testing, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb20-750"><a href="#cb20-750" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-751"><a href="#cb20-751" tabindex="-1"></a></span>
<span id="cb20-752"><a href="#cb20-752" tabindex="-1"></a>X3_testing <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2_testing, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb20-753"><a href="#cb20-753" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb20-754"><a href="#cb20-754" tabindex="-1"></a></span>
<span id="cb20-755"><a href="#cb20-755" tabindex="-1"></a>Z_out_testing <span class="ot">&lt;-</span> M3 <span class="sc">%*%</span> X3_testing</span>
<span id="cb20-756"><a href="#cb20-756" tabindex="-1"></a>}</span>
<span id="cb20-757"><a href="#cb20-757" tabindex="-1"></a></span>
<span id="cb20-758"><a href="#cb20-758" tabindex="-1"></a><span class="co"># Apply the softmax function to each column</span></span>
<span id="cb20-759"><a href="#cb20-759" tabindex="-1"></a>X_out_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(Z_out_testing, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb20-760"><a href="#cb20-760" tabindex="-1"></a></span>
<span id="cb20-761"><a href="#cb20-761" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-762"><a href="#cb20-762" tabindex="-1"></a>  <span class="co"># Code for backpropagation training</span></span>
<span id="cb20-763"><a href="#cb20-763" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-764"><a href="#cb20-764" tabindex="-1"></a> <span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-765"><a href="#cb20-765" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-766"><a href="#cb20-766" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-767"><a href="#cb20-767" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-768"><a href="#cb20-768" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-769"><a href="#cb20-769" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-770"><a href="#cb20-770" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-771"><a href="#cb20-771" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t <span class="ot">&lt;-</span> (X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-772"><a href="#cb20-772" tabindex="-1"></a></span>
<span id="cb20-773"><a href="#cb20-773" tabindex="-1"></a>  dZ3_out_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-774"><a href="#cb20-774" tabindex="-1"></a></span>
<span id="cb20-775"><a href="#cb20-775" tabindex="-1"></a>  <span class="co"># dC/dM2 = [(dC/dX_out)^t *(dX_out/dZ3_out)^t]^t * dZ3_out/dM2</span></span>
<span id="cb20-776"><a href="#cb20-776" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span> dc_dX_out_t_times_dX_out_dZ3_out_t <span class="sc">%*%</span> dZ3_out_dM2 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-777"><a href="#cb20-777" tabindex="-1"></a></span>
<span id="cb20-778"><a href="#cb20-778" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-779"><a href="#cb20-779" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-780"><a href="#cb20-780" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-781"><a href="#cb20-781" tabindex="-1"></a>  <span class="co"># (dC/dX_out)^t *(dX_out/dZ3_out)^t</span></span>
<span id="cb20-782"><a href="#cb20-782" tabindex="-1"></a>  dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t)</span>
<span id="cb20-783"><a href="#cb20-783" tabindex="-1"></a></span>
<span id="cb20-784"><a href="#cb20-784" tabindex="-1"></a>  dZ3_out_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-785"><a href="#cb20-785" tabindex="-1"></a></span>
<span id="cb20-786"><a href="#cb20-786" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-787"><a href="#cb20-787" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-788"><a href="#cb20-788" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-789"><a href="#cb20-789" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-790"><a href="#cb20-790" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-791"><a href="#cb20-791" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-792"><a href="#cb20-792" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-793"><a href="#cb20-793" tabindex="-1"></a>  }</span>
<span id="cb20-794"><a href="#cb20-794" tabindex="-1"></a></span>
<span id="cb20-795"><a href="#cb20-795" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-796"><a href="#cb20-796" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-797"><a href="#cb20-797" tabindex="-1"></a></span>
<span id="cb20-798"><a href="#cb20-798" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-799"><a href="#cb20-799" tabindex="-1"></a></span>
<span id="cb20-800"><a href="#cb20-800" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(dc_dX_out_t_times_dX_out_dZ3_out_t_t <span class="sc">%*%</span> dZ3_out_dX2)) <span class="sc">*</span> dX2_dZ2) <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-801"><a href="#cb20-801" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-802"><a href="#cb20-802" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-803"><a href="#cb20-803" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-804"><a href="#cb20-804" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-805"><a href="#cb20-805" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-806"><a href="#cb20-806" tabindex="-1"></a>  <span class="co"># DC/DM3</span></span>
<span id="cb20-807"><a href="#cb20-807" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-808"><a href="#cb20-808" tabindex="-1"></a>  dc_dZ_out_t <span class="ot">&lt;-</span>(X_out <span class="sc">-</span> Y_One_Hot_Encoding_training) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-809"><a href="#cb20-809" tabindex="-1"></a></span>
<span id="cb20-810"><a href="#cb20-810" tabindex="-1"></a>  dZ_out_dM3 <span class="ot">&lt;-</span><span class="fu">t</span>(X3)</span>
<span id="cb20-811"><a href="#cb20-811" tabindex="-1"></a></span>
<span id="cb20-812"><a href="#cb20-812" tabindex="-1"></a>  dC_dM3 <span class="ot">&lt;-</span>dc_dZ_out_t <span class="sc">%*%</span> dZ_out_dM3 <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-813"><a href="#cb20-813" tabindex="-1"></a></span>
<span id="cb20-814"><a href="#cb20-814" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-815"><a href="#cb20-815" tabindex="-1"></a>  <span class="co"># DC/DM2</span></span>
<span id="cb20-816"><a href="#cb20-816" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-817"><a href="#cb20-817" tabindex="-1"></a>  dc_dZ_out <span class="ot">&lt;-</span> <span class="fu">t</span>(dc_dZ_out_t)</span>
<span id="cb20-818"><a href="#cb20-818" tabindex="-1"></a></span>
<span id="cb20-819"><a href="#cb20-819" tabindex="-1"></a>  dZ4_dX3 <span class="ot">&lt;-</span> M3[, <span class="sc">-</span><span class="fu">ncol</span>(M3)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-820"><a href="#cb20-820" tabindex="-1"></a></span>
<span id="cb20-821"><a href="#cb20-821" tabindex="-1"></a>  <span class="co"># dX3/dZ3 (element wise)</span></span>
<span id="cb20-822"><a href="#cb20-822" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-823"><a href="#cb20-823" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-824"><a href="#cb20-824" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-825"><a href="#cb20-825" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-826"><a href="#cb20-826" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-827"><a href="#cb20-827" tabindex="-1"></a>    dX3_dZ3_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X3, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-828"><a href="#cb20-828" tabindex="-1"></a>  }</span>
<span id="cb20-829"><a href="#cb20-829" tabindex="-1"></a></span>
<span id="cb20-830"><a href="#cb20-830" tabindex="-1"></a>  dX3_dZ3 <span class="ot">&lt;-</span> dX3_dZ3_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX3_dZ3_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-831"><a href="#cb20-831" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-832"><a href="#cb20-832" tabindex="-1"></a></span>
<span id="cb20-833"><a href="#cb20-833" tabindex="-1"></a>  dZ3_dM2 <span class="ot">&lt;-</span> <span class="fu">t</span>(X2)</span>
<span id="cb20-834"><a href="#cb20-834" tabindex="-1"></a></span>
<span id="cb20-835"><a href="#cb20-835" tabindex="-1"></a>  dC_dZ3 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dc_dZ_out <span class="sc">%*%</span> dZ4_dX3)) <span class="sc">*</span> dX3_dZ3</span>
<span id="cb20-836"><a href="#cb20-836" tabindex="-1"></a></span>
<span id="cb20-837"><a href="#cb20-837" tabindex="-1"></a>  dC_dM2 <span class="ot">&lt;-</span>dC_dZ3 <span class="sc">%*%</span> dZ3_dM2</span>
<span id="cb20-838"><a href="#cb20-838" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-839"><a href="#cb20-839" tabindex="-1"></a>  <span class="co"># DC/DM1</span></span>
<span id="cb20-840"><a href="#cb20-840" tabindex="-1"></a>  <span class="do">###############</span></span>
<span id="cb20-841"><a href="#cb20-841" tabindex="-1"></a>  dC_dZ3_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dC_dZ3)</span>
<span id="cb20-842"><a href="#cb20-842" tabindex="-1"></a></span>
<span id="cb20-843"><a href="#cb20-843" tabindex="-1"></a>  dZ3_dX2 <span class="ot">&lt;-</span> M2[, <span class="sc">-</span><span class="fu">ncol</span>(M2)] <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-844"><a href="#cb20-844" tabindex="-1"></a></span>
<span id="cb20-845"><a href="#cb20-845" tabindex="-1"></a>  <span class="co"># dX2/dZ2 (element wise)</span></span>
<span id="cb20-846"><a href="#cb20-846" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb20-847"><a href="#cb20-847" tabindex="-1"></a>    <span class="co"># SGD </span></span>
<span id="cb20-848"><a href="#cb20-848" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-849"><a href="#cb20-849" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-850"><a href="#cb20-850" tabindex="-1"></a>    <span class="co"># Mini Batch Gradient </span></span>
<span id="cb20-851"><a href="#cb20-851" tabindex="-1"></a>    dX2_dZ2_element_wise <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(X2, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, LeakyReLU_alpha))) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb20-852"><a href="#cb20-852" tabindex="-1"></a>  }</span>
<span id="cb20-853"><a href="#cb20-853" tabindex="-1"></a></span>
<span id="cb20-854"><a href="#cb20-854" tabindex="-1"></a>  dX2_dZ2 <span class="ot">&lt;-</span> dX2_dZ2_element_wise[<span class="sc">-</span><span class="fu">nrow</span>(dX2_dZ2_element_wise),] <span class="sc">%&gt;%</span> </span>
<span id="cb20-855"><a href="#cb20-855" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb20-856"><a href="#cb20-856" tabindex="-1"></a></span>
<span id="cb20-857"><a href="#cb20-857" tabindex="-1"></a>  dZ2_dM1 <span class="ot">&lt;-</span> <span class="fu">t</span>(X1)</span>
<span id="cb20-858"><a href="#cb20-858" tabindex="-1"></a></span>
<span id="cb20-859"><a href="#cb20-859" tabindex="-1"></a>  dC_dZ2 <span class="ot">&lt;-</span>(<span class="fu">t</span>(dC_dZ3_t <span class="sc">%*%</span> dZ3_dX2)) <span class="sc">*</span> dX2_dZ2</span>
<span id="cb20-860"><a href="#cb20-860" tabindex="-1"></a></span>
<span id="cb20-861"><a href="#cb20-861" tabindex="-1"></a>  dC_dM1 <span class="ot">&lt;-</span>dC_dZ2 <span class="sc">%*%</span> dZ2_dM1</span>
<span id="cb20-862"><a href="#cb20-862" tabindex="-1"></a>}</span>
<span id="cb20-863"><a href="#cb20-863" tabindex="-1"></a></span>
<span id="cb20-864"><a href="#cb20-864" tabindex="-1"></a><span class="do">##############################################################</span></span>
<span id="cb20-865"><a href="#cb20-865" tabindex="-1"></a>  <span class="co"># Gradient Descent</span></span>
<span id="cb20-866"><a href="#cb20-866" tabindex="-1"></a><span class="do">#############################################################</span></span>
<span id="cb20-867"><a href="#cb20-867" tabindex="-1"></a><span class="co"># note that in SGD and minibatch Epochs = Epoch*number_iterations_epoch</span></span>
<span id="cb20-868"><a href="#cb20-868" tabindex="-1"></a><span class="co"># Update weights and biases (based on backpropagation)</span></span>
<span id="cb20-869"><a href="#cb20-869" tabindex="-1"></a>  </span>
<span id="cb20-870"><a href="#cb20-870" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb20-871"><a href="#cb20-871" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-872"><a href="#cb20-872" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb20-873"><a href="#cb20-873" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-874"><a href="#cb20-874" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-875"><a href="#cb20-875" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-876"><a href="#cb20-876" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-877"><a href="#cb20-877" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-878"><a href="#cb20-878" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-879"><a href="#cb20-879" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-880"><a href="#cb20-880" tabindex="-1"></a></span>
<span id="cb20-881"><a href="#cb20-881" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-882"><a href="#cb20-882" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-883"><a href="#cb20-883" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-884"><a href="#cb20-884" tabindex="-1"></a>}</span>
<span id="cb20-885"><a href="#cb20-885" tabindex="-1"></a>       },</span>
<span id="cb20-886"><a href="#cb20-886" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-887"><a href="#cb20-887" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-888"><a href="#cb20-888" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-889"><a href="#cb20-889" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-890"><a href="#cb20-890" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-891"><a href="#cb20-891" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-892"><a href="#cb20-892" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-893"><a href="#cb20-893" tabindex="-1"></a></span>
<span id="cb20-894"><a href="#cb20-894" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-895"><a href="#cb20-895" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-896"><a href="#cb20-896" tabindex="-1"></a>}</span>
<span id="cb20-897"><a href="#cb20-897" tabindex="-1"></a>       },</span>
<span id="cb20-898"><a href="#cb20-898" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-899"><a href="#cb20-899" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-900"><a href="#cb20-900" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-901"><a href="#cb20-901" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-902"><a href="#cb20-902" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-903"><a href="#cb20-903" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-904"><a href="#cb20-904" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-905"><a href="#cb20-905" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-906"><a href="#cb20-906" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-907"><a href="#cb20-907" tabindex="-1"></a><span class="co"># M2 </span></span>
<span id="cb20-908"><a href="#cb20-908" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-909"><a href="#cb20-909" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-910"><a href="#cb20-910" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-911"><a href="#cb20-911" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-912"><a href="#cb20-912" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-913"><a href="#cb20-913" tabindex="-1"></a></span>
<span id="cb20-914"><a href="#cb20-914" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-915"><a href="#cb20-915" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-916"><a href="#cb20-916" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-917"><a href="#cb20-917" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-918"><a href="#cb20-918" tabindex="-1"></a></span>
<span id="cb20-919"><a href="#cb20-919" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-920"><a href="#cb20-920" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-921"><a href="#cb20-921" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-922"><a href="#cb20-922" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-923"><a href="#cb20-923" tabindex="-1"></a>}</span>
<span id="cb20-924"><a href="#cb20-924" tabindex="-1"></a></span>
<span id="cb20-925"><a href="#cb20-925" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-926"><a href="#cb20-926" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-927"><a href="#cb20-927" tabindex="-1"></a></span>
<span id="cb20-928"><a href="#cb20-928" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-929"><a href="#cb20-929" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-930"><a href="#cb20-930" tabindex="-1"></a></span>
<span id="cb20-931"><a href="#cb20-931" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-932"><a href="#cb20-932" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-933"><a href="#cb20-933" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-934"><a href="#cb20-934" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-935"><a href="#cb20-935" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-936"><a href="#cb20-936" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-937"><a href="#cb20-937" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-938"><a href="#cb20-938" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1 </span>
<span id="cb20-939"><a href="#cb20-939" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-940"><a href="#cb20-940" tabindex="-1"></a></span>
<span id="cb20-941"><a href="#cb20-941" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-942"><a href="#cb20-942" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-943"><a href="#cb20-943" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-944"><a href="#cb20-944" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-945"><a href="#cb20-945" tabindex="-1"></a></span>
<span id="cb20-946"><a href="#cb20-946" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-947"><a href="#cb20-947" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-948"><a href="#cb20-948" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-949"><a href="#cb20-949" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-950"><a href="#cb20-950" tabindex="-1"></a>}</span>
<span id="cb20-951"><a href="#cb20-951" tabindex="-1"></a></span>
<span id="cb20-952"><a href="#cb20-952" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-953"><a href="#cb20-953" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-954"><a href="#cb20-954" tabindex="-1"></a></span>
<span id="cb20-955"><a href="#cb20-955" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-956"><a href="#cb20-956" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-957"><a href="#cb20-957" tabindex="-1"></a></span>
<span id="cb20-958"><a href="#cb20-958" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-959"><a href="#cb20-959" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-960"><a href="#cb20-960" tabindex="-1"></a></span>
<span id="cb20-961"><a href="#cb20-961" tabindex="-1"></a>} </span>
<span id="cb20-962"><a href="#cb20-962" tabindex="-1"></a>       }</span>
<span id="cb20-963"><a href="#cb20-963" tabindex="-1"></a>)</span>
<span id="cb20-964"><a href="#cb20-964" tabindex="-1"></a></span>
<span id="cb20-965"><a href="#cb20-965" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-966"><a href="#cb20-966" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-967"><a href="#cb20-967" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-968"><a href="#cb20-968" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-969"><a href="#cb20-969" tabindex="-1"></a></span>
<span id="cb20-970"><a href="#cb20-970" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb20-971"><a href="#cb20-971" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-972"><a href="#cb20-972" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb20-973"><a href="#cb20-973" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-974"><a href="#cb20-974" tabindex="-1"></a><span class="cf">switch</span>(gradient_descent_algorithm,</span>
<span id="cb20-975"><a href="#cb20-975" tabindex="-1"></a>       <span class="st">&quot;constant&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-976"><a href="#cb20-976" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-977"><a href="#cb20-977" tabindex="-1"></a><span class="co"># # When using constant learning rate</span></span>
<span id="cb20-978"><a href="#cb20-978" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-979"><a href="#cb20-979" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE</span>
<span id="cb20-980"><a href="#cb20-980" tabindex="-1"></a></span>
<span id="cb20-981"><a href="#cb20-981" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-982"><a href="#cb20-982" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-983"><a href="#cb20-983" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-984"><a href="#cb20-984" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-985"><a href="#cb20-985" tabindex="-1"></a>}</span>
<span id="cb20-986"><a href="#cb20-986" tabindex="-1"></a>       },</span>
<span id="cb20-987"><a href="#cb20-987" tabindex="-1"></a>       <span class="st">&quot;decaying&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-988"><a href="#cb20-988" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-989"><a href="#cb20-989" tabindex="-1"></a><span class="co"># When using decaying learning rate</span></span>
<span id="cb20-990"><a href="#cb20-990" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-991"><a href="#cb20-991" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-992"><a href="#cb20-992" tabindex="-1"></a>  <span class="co"># Update the learning rate for each epoch/iteration</span></span>
<span id="cb20-993"><a href="#cb20-993" tabindex="-1"></a>Learning_Rate <span class="ot">&lt;-</span>Alpha_ONE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>i)</span>
<span id="cb20-994"><a href="#cb20-994" tabindex="-1"></a></span>
<span id="cb20-995"><a href="#cb20-995" tabindex="-1"></a>M3 <span class="ot">&lt;-</span>M3 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-996"><a href="#cb20-996" tabindex="-1"></a>M2 <span class="ot">&lt;-</span>M2 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-997"><a href="#cb20-997" tabindex="-1"></a>M1 <span class="ot">&lt;-</span>M1 <span class="sc">-</span> (Learning_Rate <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-998"><a href="#cb20-998" tabindex="-1"></a>}</span>
<span id="cb20-999"><a href="#cb20-999" tabindex="-1"></a>       },</span>
<span id="cb20-1000"><a href="#cb20-1000" tabindex="-1"></a>       <span class="st">&quot;adam&quot;</span> <span class="ot">=</span> {</span>
<span id="cb20-1001"><a href="#cb20-1001" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1002"><a href="#cb20-1002" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-1003"><a href="#cb20-1003" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1004"><a href="#cb20-1004" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1005"><a href="#cb20-1005" tabindex="-1"></a><span class="co"># When using Adam optimizer</span></span>
<span id="cb20-1006"><a href="#cb20-1006" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1007"><a href="#cb20-1007" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Epochs){</span>
<span id="cb20-1008"><a href="#cb20-1008" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1009"><a href="#cb20-1009" tabindex="-1"></a><span class="co"># M3</span></span>
<span id="cb20-1010"><a href="#cb20-1010" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1011"><a href="#cb20-1011" tabindex="-1"></a></span>
<span id="cb20-1012"><a href="#cb20-1012" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-1013"><a href="#cb20-1013" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-1014"><a href="#cb20-1014" tabindex="-1"></a>v1_prev_M3 <span class="ot">&lt;-</span> v1_M3</span>
<span id="cb20-1015"><a href="#cb20-1015" tabindex="-1"></a>v1_M3 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM3)</span>
<span id="cb20-1016"><a href="#cb20-1016" tabindex="-1"></a></span>
<span id="cb20-1017"><a href="#cb20-1017" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-1018"><a href="#cb20-1018" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-1019"><a href="#cb20-1019" tabindex="-1"></a>v2_prev_M3 <span class="ot">&lt;-</span> v2_M3</span>
<span id="cb20-1020"><a href="#cb20-1020" tabindex="-1"></a>v2_M3 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M3) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM3<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-1021"><a href="#cb20-1021" tabindex="-1"></a></span>
<span id="cb20-1022"><a href="#cb20-1022" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-1023"><a href="#cb20-1023" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-1024"><a href="#cb20-1024" tabindex="-1"></a>  v1_M3 <span class="ot">&lt;-</span> v1_prev_M3</span>
<span id="cb20-1025"><a href="#cb20-1025" tabindex="-1"></a>  v2_M3 <span class="ot">&lt;-</span> v2_prev_M3</span>
<span id="cb20-1026"><a href="#cb20-1026" tabindex="-1"></a>}</span>
<span id="cb20-1027"><a href="#cb20-1027" tabindex="-1"></a></span>
<span id="cb20-1028"><a href="#cb20-1028" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-1029"><a href="#cb20-1029" tabindex="-1"></a>v1_hat_M3 <span class="ot">&lt;-</span> v1_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-1030"><a href="#cb20-1030" tabindex="-1"></a></span>
<span id="cb20-1031"><a href="#cb20-1031" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-1032"><a href="#cb20-1032" tabindex="-1"></a>v2_hat_M3 <span class="ot">&lt;-</span> v2_M3 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-1033"><a href="#cb20-1033" tabindex="-1"></a></span>
<span id="cb20-1034"><a href="#cb20-1034" tabindex="-1"></a><span class="co"># Update M3 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-1035"><a href="#cb20-1035" tabindex="-1"></a>M3 <span class="ot">&lt;-</span> M3 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M3 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M3) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-1036"><a href="#cb20-1036" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1037"><a href="#cb20-1037" tabindex="-1"></a><span class="co"># M2</span></span>
<span id="cb20-1038"><a href="#cb20-1038" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1039"><a href="#cb20-1039" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-1040"><a href="#cb20-1040" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-1041"><a href="#cb20-1041" tabindex="-1"></a>v1_prev_M2 <span class="ot">&lt;-</span> v1_M2</span>
<span id="cb20-1042"><a href="#cb20-1042" tabindex="-1"></a>v1_M2 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM2)</span>
<span id="cb20-1043"><a href="#cb20-1043" tabindex="-1"></a></span>
<span id="cb20-1044"><a href="#cb20-1044" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-1045"><a href="#cb20-1045" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-1046"><a href="#cb20-1046" tabindex="-1"></a>v2_prev_M2 <span class="ot">&lt;-</span> v2_M2</span>
<span id="cb20-1047"><a href="#cb20-1047" tabindex="-1"></a>v2_M2 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M2) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-1048"><a href="#cb20-1048" tabindex="-1"></a></span>
<span id="cb20-1049"><a href="#cb20-1049" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-1050"><a href="#cb20-1050" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-1051"><a href="#cb20-1051" tabindex="-1"></a>  v1_M2 <span class="ot">&lt;-</span> v1_prev_M2</span>
<span id="cb20-1052"><a href="#cb20-1052" tabindex="-1"></a>  v2_M2 <span class="ot">&lt;-</span> v2_prev_M2</span>
<span id="cb20-1053"><a href="#cb20-1053" tabindex="-1"></a>}</span>
<span id="cb20-1054"><a href="#cb20-1054" tabindex="-1"></a></span>
<span id="cb20-1055"><a href="#cb20-1055" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-1056"><a href="#cb20-1056" tabindex="-1"></a>v1_hat_M2 <span class="ot">&lt;-</span> v1_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-1057"><a href="#cb20-1057" tabindex="-1"></a></span>
<span id="cb20-1058"><a href="#cb20-1058" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-1059"><a href="#cb20-1059" tabindex="-1"></a>v2_hat_M2 <span class="ot">&lt;-</span> v2_M2 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-1060"><a href="#cb20-1060" tabindex="-1"></a></span>
<span id="cb20-1061"><a href="#cb20-1061" tabindex="-1"></a><span class="co"># Update M2 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-1062"><a href="#cb20-1062" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> M2 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M2 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M2) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-1063"><a href="#cb20-1063" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1064"><a href="#cb20-1064" tabindex="-1"></a><span class="co"># M1</span></span>
<span id="cb20-1065"><a href="#cb20-1065" tabindex="-1"></a><span class="do">#####</span></span>
<span id="cb20-1066"><a href="#cb20-1066" tabindex="-1"></a><span class="co"># 1st moment</span></span>
<span id="cb20-1067"><a href="#cb20-1067" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span>v1 <span class="co"># Initialize v1=0 at t=1</span></span>
<span id="cb20-1068"><a href="#cb20-1068" tabindex="-1"></a>v1_prev_M1 <span class="ot">&lt;-</span> v1_M1</span>
<span id="cb20-1069"><a href="#cb20-1069" tabindex="-1"></a>v1_M1 <span class="ot">&lt;-</span> (Beta1 <span class="sc">*</span> v1_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta1) <span class="sc">*</span> dC_dM1)</span>
<span id="cb20-1070"><a href="#cb20-1070" tabindex="-1"></a></span>
<span id="cb20-1071"><a href="#cb20-1071" tabindex="-1"></a><span class="co"># 2nd moment</span></span>
<span id="cb20-1072"><a href="#cb20-1072" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span>v2 <span class="co"># Initialize v2=0 at t=1</span></span>
<span id="cb20-1073"><a href="#cb20-1073" tabindex="-1"></a>v2_prev_M1 <span class="ot">&lt;-</span> v2_M1</span>
<span id="cb20-1074"><a href="#cb20-1074" tabindex="-1"></a>v2_M1 <span class="ot">&lt;-</span> (Beta2 <span class="sc">*</span> v2_M1) <span class="sc">+</span> ((<span class="dv">1</span> <span class="sc">-</span> Beta2) <span class="sc">*</span> (dC_dM1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-1075"><a href="#cb20-1075" tabindex="-1"></a></span>
<span id="cb20-1076"><a href="#cb20-1076" tabindex="-1"></a><span class="co"># Stores the previous values of v1 and v2 starting at i=2</span></span>
<span id="cb20-1077"><a href="#cb20-1077" tabindex="-1"></a><span class="cf">if</span> (i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb20-1078"><a href="#cb20-1078" tabindex="-1"></a>  v1_M1 <span class="ot">&lt;-</span> v1_prev_M1</span>
<span id="cb20-1079"><a href="#cb20-1079" tabindex="-1"></a>  v2_M1 <span class="ot">&lt;-</span> v2_prev_M1</span>
<span id="cb20-1080"><a href="#cb20-1080" tabindex="-1"></a>}</span>
<span id="cb20-1081"><a href="#cb20-1081" tabindex="-1"></a></span>
<span id="cb20-1082"><a href="#cb20-1082" tabindex="-1"></a><span class="co"># Bias corrected 1st moment</span></span>
<span id="cb20-1083"><a href="#cb20-1083" tabindex="-1"></a>v1_hat_M1 <span class="ot">&lt;-</span> v1_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta1<span class="sc">^</span>i))</span>
<span id="cb20-1084"><a href="#cb20-1084" tabindex="-1"></a></span>
<span id="cb20-1085"><a href="#cb20-1085" tabindex="-1"></a><span class="co"># Bias corrected 2nd moment</span></span>
<span id="cb20-1086"><a href="#cb20-1086" tabindex="-1"></a>v2_hat_M1 <span class="ot">&lt;-</span> v2_M1 <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> (Beta2<span class="sc">^</span>i))</span>
<span id="cb20-1087"><a href="#cb20-1087" tabindex="-1"></a></span>
<span id="cb20-1088"><a href="#cb20-1088" tabindex="-1"></a><span class="co"># Update M1 using the bias-corrected moments and learning rate</span></span>
<span id="cb20-1089"><a href="#cb20-1089" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> M1 <span class="sc">-</span> (Alpha_ONE <span class="sc">*</span> (v1_hat_M1 <span class="sc">/</span> (<span class="fu">sqrt</span>(v2_hat_M1) <span class="sc">+</span> Epsilon)))</span>
<span id="cb20-1090"><a href="#cb20-1090" tabindex="-1"></a>}</span>
<span id="cb20-1091"><a href="#cb20-1091" tabindex="-1"></a>       }</span>
<span id="cb20-1092"><a href="#cb20-1092" tabindex="-1"></a>)</span>
<span id="cb20-1093"><a href="#cb20-1093" tabindex="-1"></a></span>
<span id="cb20-1094"><a href="#cb20-1094" tabindex="-1"></a>  <span class="co">#Store M3 for each epoch</span></span>
<span id="cb20-1095"><a href="#cb20-1095" tabindex="-1"></a>M3_list[[epoch]] <span class="ot">&lt;-</span>M3</span>
<span id="cb20-1096"><a href="#cb20-1096" tabindex="-1"></a>  <span class="co">#Store M2 for each epoch</span></span>
<span id="cb20-1097"><a href="#cb20-1097" tabindex="-1"></a>M2_list[[epoch]] <span class="ot">&lt;-</span>M2</span>
<span id="cb20-1098"><a href="#cb20-1098" tabindex="-1"></a>  <span class="co">#Store M1 for each epoch</span></span>
<span id="cb20-1099"><a href="#cb20-1099" tabindex="-1"></a>M1_list[[epoch]] <span class="ot">&lt;-</span>M1</span>
<span id="cb20-1100"><a href="#cb20-1100" tabindex="-1"></a></span>
<span id="cb20-1101"><a href="#cb20-1101" tabindex="-1"></a>}</span>
<span id="cb20-1102"><a href="#cb20-1102" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1103"><a href="#cb20-1103" tabindex="-1"></a>  <span class="co"># Compute Mean Categorical Cross Entropy Loss and store it in the CCEntropy_Loss dataframe</span></span>
<span id="cb20-1104"><a href="#cb20-1104" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1105"><a href="#cb20-1105" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1106"><a href="#cb20-1106" tabindex="-1"></a><span class="co"># Mean Categorical Cross Entropy Loss Training</span></span>
<span id="cb20-1107"><a href="#cb20-1107" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1108"><a href="#cb20-1108" tabindex="-1"></a><span class="co"># Catagorical Cross Entropy Loss</span></span>
<span id="cb20-1109"><a href="#cb20-1109" tabindex="-1"></a>Cost_training <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_training <span class="sc">*</span> <span class="fu">log</span>(X_out)</span>
<span id="cb20-1110"><a href="#cb20-1110" tabindex="-1"></a></span>
<span id="cb20-1111"><a href="#cb20-1111" tabindex="-1"></a><span class="co"># Sums up the CCE Loss for each observation</span></span>
<span id="cb20-1112"><a href="#cb20-1112" tabindex="-1"></a>Cost_training <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_training)</span>
<span id="cb20-1113"><a href="#cb20-1113" tabindex="-1"></a></span>
<span id="cb20-1114"><a href="#cb20-1114" tabindex="-1"></a><span class="co"># Stores mean CCE Loss X_out for each epoch</span></span>
<span id="cb20-1115"><a href="#cb20-1115" tabindex="-1"></a>CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch] <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>(Cost_training), <span class="dv">2</span>)</span>
<span id="cb20-1116"><a href="#cb20-1116" tabindex="-1"></a></span>
<span id="cb20-1117"><a href="#cb20-1117" tabindex="-1"></a><span class="co"># Stores epoch number for each epoch</span></span>
<span id="cb20-1118"><a href="#cb20-1118" tabindex="-1"></a>CCEntropy_Loss<span class="sc">$</span>epoch[epoch] <span class="ot">&lt;-</span> epoch</span>
<span id="cb20-1119"><a href="#cb20-1119" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1120"><a href="#cb20-1120" tabindex="-1"></a><span class="co"># Mean Categorical Cross Entropy Loss Testing</span></span>
<span id="cb20-1121"><a href="#cb20-1121" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1122"><a href="#cb20-1122" tabindex="-1"></a><span class="co"># Catagorical Cross Entropy Loss</span></span>
<span id="cb20-1123"><a href="#cb20-1123" tabindex="-1"></a>Cost_testing <span class="ot">&lt;-</span> <span class="sc">-</span>Y_One_Hot_Encoding_testing <span class="sc">*</span> <span class="fu">log</span>(X_out_testing) </span>
<span id="cb20-1124"><a href="#cb20-1124" tabindex="-1"></a></span>
<span id="cb20-1125"><a href="#cb20-1125" tabindex="-1"></a><span class="co"># Sums up the CCE Loss for each observation</span></span>
<span id="cb20-1126"><a href="#cb20-1126" tabindex="-1"></a>Cost_testing <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Cost_testing)</span>
<span id="cb20-1127"><a href="#cb20-1127" tabindex="-1"></a></span>
<span id="cb20-1128"><a href="#cb20-1128" tabindex="-1"></a><span class="co"># Stores mean CCE Loss X_out (testing) for each epoch</span></span>
<span id="cb20-1129"><a href="#cb20-1129" tabindex="-1"></a>CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch] <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>(Cost_testing), <span class="dv">2</span>)</span>
<span id="cb20-1130"><a href="#cb20-1130" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1131"><a href="#cb20-1131" tabindex="-1"></a>  <span class="co"># Compute Accuracy for each epoch</span></span>
<span id="cb20-1132"><a href="#cb20-1132" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb20-1133"><a href="#cb20-1133" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1134"><a href="#cb20-1134" tabindex="-1"></a><span class="co">#Accuracy Training</span></span>
<span id="cb20-1135"><a href="#cb20-1135" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1136"><a href="#cb20-1136" tabindex="-1"></a><span class="co"># Find the highest probability for each observation in X_out</span></span>
<span id="cb20-1137"><a href="#cb20-1137" tabindex="-1"></a>X_out_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out, <span class="dv">2</span>, max)</span>
<span id="cb20-1138"><a href="#cb20-1138" tabindex="-1"></a></span>
<span id="cb20-1139"><a href="#cb20-1139" tabindex="-1"></a>X_out_hadamard_Y_training <span class="ot">&lt;-</span> X_out <span class="sc">*</span> Y_One_Hot_Encoding_training</span>
<span id="cb20-1140"><a href="#cb20-1140" tabindex="-1"></a></span>
<span id="cb20-1141"><a href="#cb20-1141" tabindex="-1"></a><span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-1142"><a href="#cb20-1142" tabindex="-1"></a>X_out_hadamard_Y_v_training <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_hadamard_Y_training, <span class="dv">2</span>, max)</span>
<span id="cb20-1143"><a href="#cb20-1143" tabindex="-1"></a></span>
<span id="cb20-1144"><a href="#cb20-1144" tabindex="-1"></a><span class="co"># Subtract the two vectors</span></span>
<span id="cb20-1145"><a href="#cb20-1145" tabindex="-1"></a>difference_training <span class="ot">&lt;-</span> X_out_highest <span class="sc">-</span> X_out_hadamard_Y_v_training</span>
<span id="cb20-1146"><a href="#cb20-1146" tabindex="-1"></a></span>
<span id="cb20-1147"><a href="#cb20-1147" tabindex="-1"></a><span class="co"># Count the number of zeros</span></span>
<span id="cb20-1148"><a href="#cb20-1148" tabindex="-1"></a>count_zeros_training <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_training <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-1149"><a href="#cb20-1149" tabindex="-1"></a></span>
<span id="cb20-1150"><a href="#cb20-1150" tabindex="-1"></a><span class="co"># Accuracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-1151"><a href="#cb20-1151" tabindex="-1"></a>accuracy_percent_training <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_training <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_training, <span class="dv">2</span>)</span>
<span id="cb20-1152"><a href="#cb20-1152" tabindex="-1"></a></span>
<span id="cb20-1153"><a href="#cb20-1153" tabindex="-1"></a><span class="co"># Stores training accuracy percentage for each epoch</span></span>
<span id="cb20-1154"><a href="#cb20-1154" tabindex="-1"></a>Accuracy_Percent<span class="sc">$</span>Training_percent[epoch] <span class="ot">&lt;-</span> accuracy_percent_training</span>
<span id="cb20-1155"><a href="#cb20-1155" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1156"><a href="#cb20-1156" tabindex="-1"></a>  <span class="co">#Accuracy Testing</span></span>
<span id="cb20-1157"><a href="#cb20-1157" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb20-1158"><a href="#cb20-1158" tabindex="-1"></a><span class="co"># Find the highest probability for each observation in X_out_testing</span></span>
<span id="cb20-1159"><a href="#cb20-1159" tabindex="-1"></a>X_out_testing_highest <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-1160"><a href="#cb20-1160" tabindex="-1"></a></span>
<span id="cb20-1161"><a href="#cb20-1161" tabindex="-1"></a>X_out_testing_hadamard_Y_testing <span class="ot">&lt;-</span> X_out_testing <span class="sc">*</span> Y_One_Hot_Encoding_testing</span>
<span id="cb20-1162"><a href="#cb20-1162" tabindex="-1"></a></span>
<span id="cb20-1163"><a href="#cb20-1163" tabindex="-1"></a><span class="co"># Create a vector that contains the highest value for each column</span></span>
<span id="cb20-1164"><a href="#cb20-1164" tabindex="-1"></a>X_out_testing_hadamard_Y_v_testing <span class="ot">&lt;-</span> <span class="fu">apply</span>(X_out_testing_hadamard_Y_testing, <span class="dv">2</span>, max)</span>
<span id="cb20-1165"><a href="#cb20-1165" tabindex="-1"></a></span>
<span id="cb20-1166"><a href="#cb20-1166" tabindex="-1"></a><span class="co"># Subtract the two vectors</span></span>
<span id="cb20-1167"><a href="#cb20-1167" tabindex="-1"></a>difference_testing <span class="ot">&lt;-</span> X_out_testing_highest <span class="sc">-</span> X_out_testing_hadamard_Y_v_testing</span>
<span id="cb20-1168"><a href="#cb20-1168" tabindex="-1"></a></span>
<span id="cb20-1169"><a href="#cb20-1169" tabindex="-1"></a><span class="co"># Count the number of zeros</span></span>
<span id="cb20-1170"><a href="#cb20-1170" tabindex="-1"></a>count_zeros_testing <span class="ot">&lt;-</span> <span class="fu">sum</span>(difference_testing <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb20-1171"><a href="#cb20-1171" tabindex="-1"></a></span>
<span id="cb20-1172"><a href="#cb20-1172" tabindex="-1"></a><span class="co"># Accuracy = (number of zeros * 100) / number of observations, round to 2 decimal places</span></span>
<span id="cb20-1173"><a href="#cb20-1173" tabindex="-1"></a>accuracy_percent_testing <span class="ot">&lt;-</span> <span class="fu">round</span>((count_zeros_testing <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">/</span> N_testing, <span class="dv">2</span>)</span>
<span id="cb20-1174"><a href="#cb20-1174" tabindex="-1"></a></span>
<span id="cb20-1175"><a href="#cb20-1175" tabindex="-1"></a><span class="co"># Stores testing accuracy percentage for each epoch</span></span>
<span id="cb20-1176"><a href="#cb20-1176" tabindex="-1"></a>Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch] <span class="ot">&lt;-</span> accuracy_percent_testing</span>
<span id="cb20-1177"><a href="#cb20-1177" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb20-1178"><a href="#cb20-1178" tabindex="-1"></a>  <span class="co"># Comment out #cat() will make results not being printed out on console</span></span>
<span id="cb20-1179"><a href="#cb20-1179" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, epoch, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1180"><a href="#cb20-1180" tabindex="-1"></a>    <span class="st">&quot;Training Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1181"><a href="#cb20-1181" tabindex="-1"></a>    <span class="st">&quot;Testing Mean CCE Loss: &quot;</span>, CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out[epoch], <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1182"><a href="#cb20-1182" tabindex="-1"></a>    <span class="st">&quot;Training Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Training_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb20-1183"><a href="#cb20-1183" tabindex="-1"></a>    <span class="st">&quot;Testing Accuracy Percent: &quot;</span>, Accuracy_Percent<span class="sc">$</span>Testing_percent[epoch], <span class="st">&quot;%</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb20-1184"><a href="#cb20-1184" tabindex="-1"></a>}</span>
<span id="cb20-1185"><a href="#cb20-1185" tabindex="-1"></a></span>
<span id="cb20-1186"><a href="#cb20-1186" tabindex="-1"></a>}</span></code></pre></div>
<p>Minimum value for mean categorical cross entropy loss.<br>
Epoch/iteration with minimum mean categorical cross entropy
loss.<br></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="co"># SGD and Mini Batch Gradient Descent</span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>  <span class="co"># SGD and Mini Batch Gradient Descent</span></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>  Iteration_lowest_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">which.min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>  Epoch_lowest_CCEntropy_Loss <span class="ot">&lt;-</span> CCEntropy_Loss<span class="sc">$</span>epoch[Iteration_lowest_CCEntropy_Loss]</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>  <span class="co"># Min Testing CCE Loss</span></span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>  Min_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>  <span class="co"># Training CCE Loss based on the epoch with the lowest testing CCE Loss</span></span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>  Min_CCEntropy_Loss_training <span class="ot">&lt;-</span> CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[Iteration_lowest_CCEntropy_Loss]</span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb21-16"><a href="#cb21-16" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-17"><a href="#cb21-17" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb21-18"><a href="#cb21-18" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb21-19"><a href="#cb21-19" tabindex="-1"></a>  <span class="co"># Batch Gradient Descent</span></span>
<span id="cb21-20"><a href="#cb21-20" tabindex="-1"></a>  Epoch_lowest_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">which.min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-21"><a href="#cb21-21" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" tabindex="-1"></a>  <span class="co"># Min Testing CCE Loss</span></span>
<span id="cb21-23"><a href="#cb21-23" tabindex="-1"></a>  Min_CCEntropy_Loss <span class="ot">&lt;-</span> <span class="fu">min</span>(CCEntropy_Loss<span class="sc">$</span>CCEL_testing_x_out)</span>
<span id="cb21-24"><a href="#cb21-24" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" tabindex="-1"></a>  <span class="co"># Training CCE Loss based on the epoch with the lowest testing CCE Loss</span></span>
<span id="cb21-26"><a href="#cb21-26" tabindex="-1"></a>  Min_CCEntropy_Loss_training <span class="ot">&lt;-</span> CCEntropy_Loss<span class="sc">$</span>CCEL_x_out[Epoch_lowest_CCEntropy_Loss]</span>
<span id="cb21-27"><a href="#cb21-27" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="plot-mean-cce-loss-and-accuracy" class="section level2">
<h2>Plot Mean CCE Loss and Accuracy</h2>
<div id="plot-categorical-cross-entropy-testing-loss" class="section level3">
<h3>Plot Categorical Cross Entropy Testing Loss</h3>
<p>Plot of the mean categorical cross entropy TESTING loss for
CCEL_testing_x3<br> for each epoch in log scale. The red point
“CCEL_testing” is the location<br> of the testing loss where the testing
loss is the lowest.<br></p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="plot-categorical-cross-entropy-training-loss" class="section level3">
<h3>Plot Categorical Cross Entropy Training Loss</h3>
<p>Plot of the mean categorical cross entropy TRAINING loss for CCEL_x3
for<br> each epoch in log scale. The red point “CCEL_training” is the
location of<br> the training loss where the testing loss is the
lowest.<br></p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="plot-tracking-model-performance-training-vs-testing-accuracy" class="section level3">
<h3>Plot Tracking Model Performance: Training vs Testing Accuracy</h3>
<p>Percent Accuracy Derived from Lowest Loss in Testing Data.<br></p>
<p>Black dot is the accuracy that corresponds to the lowest loss in
the<br> testing data.<br></p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
</div>
<div id="neural-network-performance-summary" class="section level2">
<h2>Neural Network Performance Summary</h2>
<div id="no-mapping-vs-mapping" class="section level3">
<h3>No Mapping vs Mapping</h3>
<pre><code>## [1] &quot;No mapping&quot;</code></pre>
</div>
<div id="neural-network-parameters" class="section level3">
<h3>Neural Network Parameters</h3>
<pre><code>## [1] &quot;Number of training observations used in each batch: 350&quot;</code></pre>
<pre><code>## [1] &quot;Number of hidden layers: 1&quot;</code></pre>
<pre><code>## [1] &quot;Number of neurons in hidden layer: 100&quot;</code></pre>
</div>
<div id="epoch-with-min-testing-cce-loss" class="section level3">
<h3>Epoch with Min Testing CCE Loss</h3>
<pre><code>## [1] &quot;Epoch with Minimum Testing CCE Loss: 3.42&quot;</code></pre>
</div>
<div id="categorical-cross-entropy-loss" class="section level3">
<h3>Categorical Cross Entropy Loss</h3>
<pre><code>## [1] &quot;Minimum Testing CCE Loss: 0.16&quot;</code></pre>
<pre><code>## [1] &quot;Training CCE Loss based on lowest testing CCE Loss: 0.1&quot;</code></pre>
</div>
<div id="optimal-accuracy" class="section level3">
<h3>Optimal Accuracy</h3>
<pre><code>## [1] &quot;Optimal Testing Accuracy %: 95.21&quot;</code></pre>
<pre><code>## [1] &quot;Optimal Training Accuracy %: 0.9&quot;</code></pre>
</div>
</div>
<div id="feedforward-prediction" class="section level2">
<h2>Feedforward Prediction</h2>
<div id="optimal-m-weight-matrices" class="section level3">
<h3>Optimal M Weight Matrices</h3>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="do">###########################</span></span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="co"># Optimal M Weight Matrices</span></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a><span class="do">###########################</span></span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-13"><a href="#cb31-13" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb31-14"><a href="#cb31-14" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb31-15"><a href="#cb31-15" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-16"><a href="#cb31-16" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-18"><a href="#cb31-18" tabindex="-1"></a>}</span>
<span id="cb31-19"><a href="#cb31-19" tabindex="-1"></a></span>
<span id="cb31-20"><a href="#cb31-20" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb31-21"><a href="#cb31-21" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-22"><a href="#cb31-22" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb31-23"><a href="#cb31-23" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb31-24"><a href="#cb31-24" tabindex="-1"></a><span class="cf">if</span> (n <span class="sc">&lt;</span> N_training) {</span>
<span id="cb31-25"><a href="#cb31-25" tabindex="-1"></a><span class="co">#SGD and Mini Batch Gradient Descent</span></span>
<span id="cb31-26"><a href="#cb31-26" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-27"><a href="#cb31-27" tabindex="-1"></a></span>
<span id="cb31-28"><a href="#cb31-28" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-29"><a href="#cb31-29" tabindex="-1"></a></span>
<span id="cb31-30"><a href="#cb31-30" tabindex="-1"></a>M3_min <span class="ot">&lt;-</span>M3_list[[Iteration_lowest_CCEntropy_Loss]]</span>
<span id="cb31-31"><a href="#cb31-31" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb31-32"><a href="#cb31-32" tabindex="-1"></a><span class="co">#Batch Gradient Descent</span></span>
<span id="cb31-33"><a href="#cb31-33" tabindex="-1"></a>M1_min <span class="ot">&lt;-</span>M1_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-34"><a href="#cb31-34" tabindex="-1"></a></span>
<span id="cb31-35"><a href="#cb31-35" tabindex="-1"></a>M2_min <span class="ot">&lt;-</span>M2_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-36"><a href="#cb31-36" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" tabindex="-1"></a>M3_min <span class="ot">&lt;-</span>M3_list[[Epoch_lowest_CCEntropy_Loss]]</span>
<span id="cb31-38"><a href="#cb31-38" tabindex="-1"></a>}</span>
<span id="cb31-39"><a href="#cb31-39" tabindex="-1"></a></span>
<span id="cb31-40"><a href="#cb31-40" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="feedforward-code" class="section level3">
<h3>Feedforward Code</h3>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="co"># Randomly select 3 images from the testing data</span></span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>Randomly_selected_rows_pred <span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">nrow</span>(Data_testing_scaled), n_prediction, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>Data_pred <span class="ot">&lt;-</span>Data_testing_scaled[Randomly_selected_rows_pred, ] </span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a>Row_Image_Matrix_pred <span class="ot">&lt;-</span>Data_pred[, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(Data_pred)]  <span class="sc">%&gt;%</span>  <span class="fu">as.matrix</span>()</span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a>  <span class="co"># Code for Training forward forward pass</span></span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a>  <span class="do">###############################################</span></span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb32-11"><a href="#cb32-11" tabindex="-1"></a><span class="co"># No mapping vs Fourier Feature Mapping</span></span>
<span id="cb32-12"><a href="#cb32-12" tabindex="-1"></a><span class="cf">switch</span>(NoMapping_vs_FourierFeatureMapping,</span>
<span id="cb32-13"><a href="#cb32-13" tabindex="-1"></a>       <span class="st">&quot;No mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb32-14"><a href="#cb32-14" tabindex="-1"></a>X1_pred <span class="ot">&lt;-</span>Row_Image_Matrix_pred <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb32-15"><a href="#cb32-15" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" tabindex="-1"></a>X1_map_vs_no_map_pred <span class="ot">&lt;-</span>X1_pred</span>
<span id="cb32-17"><a href="#cb32-17" tabindex="-1"></a>       },</span>
<span id="cb32-18"><a href="#cb32-18" tabindex="-1"></a>       <span class="st">&quot;Feature Mapping&quot;</span> <span class="ot">=</span> {</span>
<span id="cb32-19"><a href="#cb32-19" tabindex="-1"></a><span class="co">#Xi = X1_pred</span></span>
<span id="cb32-20"><a href="#cb32-20" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" tabindex="-1"></a>X1_pred <span class="ot">&lt;-</span>Row_Image_Matrix_pred <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb32-22"><a href="#cb32-22" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" tabindex="-1"></a>Hf_pi_B_Xi_pred <span class="ot">&lt;-</span>Hf_pi_B_pred <span class="sc">*</span> X1_pred</span>
<span id="cb32-24"><a href="#cb32-24" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" tabindex="-1"></a><span class="co"># apply the cos to each element of the matrix</span></span>
<span id="cb32-26"><a href="#cb32-26" tabindex="-1"></a>cos_element_wise_pred <span class="ot">&lt;-</span><span class="fu">cos</span>(Hf_pi_B_Xi_pred)</span>
<span id="cb32-27"><a href="#cb32-27" tabindex="-1"></a></span>
<span id="cb32-28"><a href="#cb32-28" tabindex="-1"></a><span class="co"># apply the sin to each element of the matrix</span></span>
<span id="cb32-29"><a href="#cb32-29" tabindex="-1"></a>sin_element_wise_pred <span class="ot">&lt;-</span><span class="fu">sin</span>(Hf_pi_B_Xi_pred)</span>
<span id="cb32-30"><a href="#cb32-30" tabindex="-1"></a></span>
<span id="cb32-31"><a href="#cb32-31" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb32-32"><a href="#cb32-32" tabindex="-1"></a><span class="co"># Create a vector of alternating ones and zeros</span></span>
<span id="cb32-33"><a href="#cb32-33" tabindex="-1"></a>one_zero_V_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb32-34"><a href="#cb32-34" tabindex="-1"></a></span>
<span id="cb32-35"><a href="#cb32-35" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb32-36"><a href="#cb32-36" tabindex="-1"></a>one_zero_M_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(one_zero_V_pred, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)  </span>
<span id="cb32-37"><a href="#cb32-37" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" tabindex="-1"></a>cos_matrix_pred <span class="ot">&lt;-</span>one_zero_M_pred <span class="sc">*</span> cos_element_wise_pred</span>
<span id="cb32-39"><a href="#cb32-39" tabindex="-1"></a></span>
<span id="cb32-40"><a href="#cb32-40" tabindex="-1"></a><span class="co"># Create a vector of alternating zeros and ones</span></span>
<span id="cb32-41"><a href="#cb32-41" tabindex="-1"></a>zero_one_V_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">length.out =</span> Xd_m_rows))  <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb32-42"><a href="#cb32-42" tabindex="-1"></a></span>
<span id="cb32-43"><a href="#cb32-43" tabindex="-1"></a><span class="co"># Repeat and bind the vector into a matrix</span></span>
<span id="cb32-44"><a href="#cb32-44" tabindex="-1"></a>zero_one_M_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(zero_one_V_pred, n_prediction,), <span class="at">nrow =</span> Xd_m_rows, <span class="at">ncol =</span> n_prediction, <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb32-45"><a href="#cb32-45" tabindex="-1"></a></span>
<span id="cb32-46"><a href="#cb32-46" tabindex="-1"></a>sin_matrix_pred <span class="ot">&lt;-</span>zero_one_M_pred <span class="sc">*</span> sin_element_wise_pred</span>
<span id="cb32-47"><a href="#cb32-47" tabindex="-1"></a></span>
<span id="cb32-48"><a href="#cb32-48" tabindex="-1"></a><span class="co"># Fourier Feature Mapping</span></span>
<span id="cb32-49"><a href="#cb32-49" tabindex="-1"></a>gamma_Xi_pred <span class="ot">&lt;-</span> cos_matrix_pred <span class="sc">+</span> sin_matrix_pred</span>
<span id="cb32-50"><a href="#cb32-50" tabindex="-1"></a></span>
<span id="cb32-51"><a href="#cb32-51" tabindex="-1"></a>X1_map_vs_no_map_pred <span class="ot">&lt;-</span>gamma_Xi_pred</span>
<span id="cb32-52"><a href="#cb32-52" tabindex="-1"></a>       })</span>
<span id="cb32-53"><a href="#cb32-53" tabindex="-1"></a><span class="do">#########################</span></span>
<span id="cb32-54"><a href="#cb32-54" tabindex="-1"></a>X1_pred <span class="ot">&lt;-</span><span class="fu">rbind</span>(X1_map_vs_no_map_pred, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">ncol</span>(X1_map_vs_no_map_pred)))</span>
<span id="cb32-55"><a href="#cb32-55" tabindex="-1"></a></span>
<span id="cb32-56"><a href="#cb32-56" tabindex="-1"></a>Z2_pred <span class="ot">&lt;-</span> M1_min <span class="sc">%*%</span> X1_pred</span>
<span id="cb32-57"><a href="#cb32-57" tabindex="-1"></a></span>
<span id="cb32-58"><a href="#cb32-58" tabindex="-1"></a>LeakyReLU_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z2_pred, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb32-59"><a href="#cb32-59" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb32-60"><a href="#cb32-60" tabindex="-1"></a></span>
<span id="cb32-61"><a href="#cb32-61" tabindex="-1"></a>X2_pred <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU_pred, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb32-62"><a href="#cb32-62" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb32-63"><a href="#cb32-63" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb32-64"><a href="#cb32-64" tabindex="-1"></a><span class="co"># For one and two hidden layers</span></span>
<span id="cb32-65"><a href="#cb32-65" tabindex="-1"></a><span class="cf">if</span> (num_hidden_layers <span class="sc">&lt;</span> <span class="dv">2</span>) {</span>
<span id="cb32-66"><a href="#cb32-66" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-67"><a href="#cb32-67" tabindex="-1"></a><span class="co"># When using One Hidden Layer</span></span>
<span id="cb32-68"><a href="#cb32-68" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-69"><a href="#cb32-69" tabindex="-1"></a>Z_out_pred <span class="ot">&lt;-</span> M2_min <span class="sc">%*%</span> X2_pred</span>
<span id="cb32-70"><a href="#cb32-70" tabindex="-1"></a></span>
<span id="cb32-71"><a href="#cb32-71" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb32-72"><a href="#cb32-72" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-73"><a href="#cb32-73" tabindex="-1"></a><span class="co"># When using Two Hidden Layers</span></span>
<span id="cb32-74"><a href="#cb32-74" tabindex="-1"></a><span class="do">###############</span></span>
<span id="cb32-75"><a href="#cb32-75" tabindex="-1"></a>Z3_pred <span class="ot">&lt;-</span> M2_min <span class="sc">%*%</span> X2_pred</span>
<span id="cb32-76"><a href="#cb32-76" tabindex="-1"></a></span>
<span id="cb32-77"><a href="#cb32-77" tabindex="-1"></a>LeakyReLU2_pred <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(Z3_pred, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, x, LeakyReLU_alpha <span class="sc">*</span> x))) <span class="sc">%&gt;%</span> </span>
<span id="cb32-78"><a href="#cb32-78" tabindex="-1"></a>    <span class="fu">as.matrix</span>()</span>
<span id="cb32-79"><a href="#cb32-79" tabindex="-1"></a></span>
<span id="cb32-80"><a href="#cb32-80" tabindex="-1"></a>X3_pred <span class="ot">&lt;-</span> <span class="fu">rbind</span>(LeakyReLU2_pred, <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb32-81"><a href="#cb32-81" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb32-82"><a href="#cb32-82" tabindex="-1"></a></span>
<span id="cb32-83"><a href="#cb32-83" tabindex="-1"></a>Z_out_pred <span class="ot">&lt;-</span> M3_min <span class="sc">%*%</span> X3_pred</span>
<span id="cb32-84"><a href="#cb32-84" tabindex="-1"></a>}</span>
<span id="cb32-85"><a href="#cb32-85" tabindex="-1"></a></span>
<span id="cb32-86"><a href="#cb32-86" tabindex="-1"></a>X_out_pred <span class="ot">&lt;-</span><span class="fu">apply</span>(Z_out_pred, <span class="dv">2</span>, softmax_MAX)</span>
<span id="cb32-87"><a href="#cb32-87" tabindex="-1"></a></span>
<span id="cb32-88"><a href="#cb32-88" tabindex="-1"></a><span class="co"># Predicted Minist Digit</span></span>
<span id="cb32-89"><a href="#cb32-89" tabindex="-1"></a>Mnist_digit_pred <span class="ot">&lt;-</span><span class="fu">apply</span>(X_out_pred, <span class="dv">2</span>, which.max)<span class="sc">-</span><span class="dv">1</span></span></code></pre></div>
</div>
<div id="pick-3-random-images-from-testing-data-to-predict." class="section level3">
<h3>Pick 3 Random Images From Testing Data to Predict.</h3>
<pre><code>## [1] &quot;Predicted Number: 4&quot;</code></pre>
<pre><code>## [1] &quot;Image:&quot;</code></pre>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAATlBMVEUAAAAAADoAAGYAOjoAOpAAZrY6AAA6ADo6kNtNTU1mAABmtv+QOgCQZgCQ2/+urq62ZgC2///bkDrb///m5ub/tmb/25D//7b//9v///+L2pFaAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAE/UlEQVR4nO3c4XaaSABAYdJt0jabJpaNSXz/F10FTUWEywxMGPDeHz3l1E70OzAygil21lsx9xPIPYEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIGgiYF+LaXBr1ugLwEqTs39ugfnHgQJBAkECQQJBAkECQQJBAkECQQJBCUC2hTF/cfTfs31r0DXKr//tze63+3eH38K1O7jac+yvXs+UmUL9FrV/5gkQO+P+yNr++3P7vSnQM3cg6jPOaiiEqhd7u9iL1UzAmFf49CZQFA+QGWe72L5AF2OMvtVjXOa+u/9j7+5Q0wgKBOg6i1+X8cMdPNAZXE8P9wWqU8UX89K8fh0S40jVeqlxiKBqsVqXfLF6iKB3IOo8rQGSzkHjaEZMj3XJXoXe3+s38U69h+BsHE2rRcc+niB4PErBxq+1DwvFLROIEggSCBokUChE20caJ1AkECQQNDCgOIm2lDQ8xb2of3qgT4LfZ6NlyoQvFSBul7ja+hLjQM9TyBIIEggKH+g87O8yJPDuOlZIIEEEujXxfQc99/CWf6WFOjtoesezhsHOl027LkBJu6VrgXodMXZPaiz98fDRecJgV6GTLSN+Xz09JwUaLfb3D0L1FtZ/BSot7eHfwTq7eOp86sag4Da53ov5//Q5mg3hiY9UF8CHRvzVYSbALocJeSqxi0CfTb8GfYLvDb8WmQCLRVowq8irBJoyq8irBEowY3k5wKdJ36LAUrwVYR1AbkHUdN/FWFlQF/yVYR27Wl7/Jj5nwcFJBAkECQQNO30XCcQJBAkECQQJBAkECQQlPOH9gGlWKbWrWQPEggSCBJoUNMuU+sEggSCBIJWBZQigSCBIIGg/O/umLn87+6YuaVcm5+tpdzdMVvuQdBS7u6YrVXd3ZEiz4MggaCcv4qQRSu5qpEuDzFoLqBiMaUBwsXqkBHpAaMHGP8TIh87YLE6ZMT1Ag1YagwZcb1AAxarQ0ZcL5B7EMWL1SEjrhiIF6tDRlwz0CQjCjTyAWsHWlkCQQJBAkECQQJBAkECQQJBAkECQQJBAkGTAW2L4u756kbMAPs2fR868QhlUXT/NsOApgLa7p/a9vT0GhsxAxy2ez+VwxHKw8YUQhMB1R9Xb+7bGzED7KrPLgOBLp7CffBTuN5EQPWvdz3e0tDYiBng8LfvvwOBGiPkB/TjsGsfLwY1NmIGqDZD56DmCLkdYvWxf5wBGhsxA1SHSyjQxU+NeZ+4Vp5Ah8tu44A2+z3p7aH74tTgsjzEqo1Rh1jMNHi9LCfp8niLStAU0hghZie+Xq5v8+F7UGOEWitsJ75etieK4WfSjRFym4Oqw+Lw5OoTkDLiLaQxwC5mqdEYYbM/RCfwcbFKCQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBP0PjgcDi2t/wI8AAAAASUVORK5CYII=" /><!-- --></p>
<pre><code>## [1] &quot;Predicted Number: 0&quot;</code></pre>
<pre><code>## [1] &quot;Image:&quot;</code></pre>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAATlBMVEUAAAAAADoAAGYAOjoAOpAAZrY6AAA6ADo6kNtNTU1mAABmtv+QOgCQZgCQ2/+urq62ZgC2///bkDrb///m5ub/tmb/25D//7b//9v///+L2pFaAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEzElEQVR4nO3c63KaQABAYdImaZqm0dKaxPd/0SJqJsjlLAvICuf86JSOXfUbQFbAbG+dZXO/gNQTCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBoZ6OetFPy+BboKUHZu7vcdnGsQJBAkECQQJBAkECQQJBAkECQQJBA0EdA2yx4+Xoo51y+Bmsrv/xZGD/v9+/OTQPU+XgqW3d3riUqgy96fiy1r9+3P/vynQNVcg6jPfVBJJVC9aT/FNmX/vlT/l3rHxyQChAkkUFj5FJ9iSwK6HGXgWY1ugvDCsW5sExNoLUDlR3xRyx5o9UB5djo+3GUjHSiOJdObabqpxolqpKnGsoDKyeqxsSarywJyDaLy8xxsyn1QyH/bfKmNaQ6gYiM7foq1rD8CYQIJdC2gtvcVBBQ+nEACCSTQRa371jiabqa2g0aBBBJIIH4bw5kSAer1pf0agT4TSKDpgSbZPQskkEACIVBdZjPu7lkggQQSqBfQuDKpAL09tl3DuXKg82nDjgtg1g10PuPsGtTa+/PhpHO6QPV5cNsjJ9sHbe9eBeosz54E6uzt8btAnX28tN6qMTdQ+BOs9EAxHaC4WxFWBHQ5SthZjfUCfTYbUN+hBZoHaOCtCIsHGnorwtKBBl9IvnSgwbciLB3INYgaeivC4oHGuRWh/l4GOdVHCzkRkOZxkEACCTQPUBzTkHEEEkig1QJFMjVc+zmAWCCBBFoSUPTvB3W/3x71feL01yCBBBLoCkAjOMU9o0ACCSTQZd2HgpsvDXoagQQS6GaBpvqhyeuX5tUdCZXmufmESvPqjoRyDYLSvLojoVK+uiOJbvI46JoJBKV5K0JC3cpZjdlyE4PmAspupmmAcLIaMiI9YPAAw58h8rEBk9WQEZcLFDDVCBlxuUABk9WQEZcL5BpE8WQ1ZMQFA/FkNWTEJQONMqJAAx+wdKCFJRAkECQQJBAkECQQJBAkECQQJBAkECQQNBrQLsvuXhsXYgYo2nZ96cQj5FnW/muGPRoLaFe8tN355VUWYgY4LHd+K4cj5IeFMYRGAjp+Xb19qC/EDLAvv7vsCXTxEh56v4TmRgI6/rzr6ZKGykLMAIe/3f/uCVQZIT2gH4dV+3QyqLIQM0C52HcfVB0htU3suO2f9gCVhZgBys2lL9DFs8Z8TjSVJtDhtNswoG2xJr09tp+cCi7JTaxcGLSJxewGm0tyJ52fLlHptQupjBCzEjeX6sd8/zWoMsJRq99K3FyyB4r9j6QrI6S2Dyo3i8OLOx6A5BEfIZUB9jFTjcoI22ITHcHHySolECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQ9B/dwlvDeW26dAAAAABJRU5ErkJggg==" /><!-- --></p>
<pre><code>## [1] &quot;Predicted Number: 3&quot;</code></pre>
<pre><code>## [1] &quot;Image:&quot;</code></pre>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAATlBMVEUAAAAAADoAAGYAOjoAOpAAZrY6AAA6ADo6kNtNTU1mAABmtv+QOgCQZgCQ2/+urq62ZgC2///bkDrb///m5ub/tmb/25D//7b//9v///+L2pFaAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAE2klEQVR4nO3c63aaSgBAYdI2aZqmiaU1ie//osVblshlD8NQB9j7x1mHLp3ot2AEAYud9Vbc+gXknkCQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkECQQJBAkEBQYqAfcyn4fQv0X4CKc7d+38G5BkECQQJBAkECQQJBAkECQQJBAkECQRMBbYri/uO5Oub6KVBb5bc/ldH9bvf+9ChQs4/nimV793KiEui696dqy9p++b07/1egeq5B1OccdKASqNlUn2KvF/1tdPz3pD5z2w8SCFoeUJn2U2x5QNejxJ7VaGp0lpZpLpuYQCsDOnzEV3XMQKsHKovT/uG2GL2jGETQnLazBrrYfx5/qLFEoMPB6rHxB6tLBHINosrzMdioOaiFpv8Jl0yppuqJPsXen46fYh3rj0CYQAL9X6DXkDc8xVQtkEACCRT6TgWCBIIEgtYGNKAmzUx3FKe6FWExQJ+Nf+W1BIIEaqmJ0mw8k0ACCSTQdf0o/QkkkEACxQOFTMxpmSYFenvouoZz5UDn04Y9F8CsG+h8xtk1qLP3p/1J5/RAcY05lJ1sDtrcvQjUW1k8CtTb28NXgXr7eO68VeMWN7PkB9TX1Bot5QmU+FaEMeUJdD3KDX9gaRZAn02t0ZJAUFZAyW9FSFBOQClvRUhWRkBJLyRPVkZASW9FSFZGQK5BVJpbERKXE1CaWxGS1TxYDX/uKvaDBIIEggSC/jYKf65AkECQQNBcgF4vJ9rwJ4yhEUgggZYHNNXvB71e1PWYoX/yJkCfCSSQQFkBwZvvh4ujEUgggQTimtPzeJTLBIIEggSCVnR1R1wrurojrhWdm49rRVd3xOUaBK3o6o64VnF1x5hmvx80dQJBK7oVIa65nNW4WW5i0K2Aitk0DRAerIaMSA8YPcD4vxD52ICD1ZARlwsUcKgRMuJygQIOVkNGXC6QaxDFB6shIy4YiA9WQ0ZcMlCSEQUa+YClAy0sgSCBIIEggSCBIIEggSCBIIEggSCBIIGgZEDborh7aV2IGaBq0/elE49QFkX3rxkOKBXQtnpp2/PLqy3EDLBf7v1WDkco9wsphBIBHb+u3tw3F2IG2B2+uxwIdPUS7ge/hPYSAR1/3vV0SUNtIWaA/f99+zUQqDZCfkDf96v26WRQbSFmgMPi0DmoPkJum9hx2z/NALWFmAEOm8tQoKu/GvM50VaeQPvTbuOANtWa9PbQfXIquCw3scPCqE0sZhpsL8tJujxdojJoCqmNELMSt5frx/zwNag2wlFr2ErcXrY7isP3pGsj5DYHHTaL/Ys77oCUER8htQF2MYcatRE21SaawMeDVUogSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCBIIEggSCDoH/FZJ0NiocTzAAAAAElFTkSuQmCC" /><!-- --></p>
<p>FIN.<br></p>
<p>References<br></p>
<p>B, U. (2021, November 15). Cost Function in Machine Learning.
Medium.<br> <a href="https://medium.com/@uma.bollikonda/cost-function-in-machine-learning-129de85120d5" class="uri">https://medium.com/@uma.bollikonda/cost-function-in-machine-learning-129de85120d5</a><br></p>
<p><span class="citation">@article</span>{Chadha2020DistilledNeuralNetworks,<br>
title = {Neural Networks},<br> author = {Chadha, Aman},<br> journal =
{Distilled Notes for Stanford CS229: Machine Learning},<br> year =
{2020},<br> note = {}<br> }<br></p>
<p>charleshsliao. (2017, February 25). Two Ways of Visualization of
MNIST with R. Charles’ Hodgepodge.<br> <a href="https://charleshsliao.wordpress.com/2017/02/25/two-ways-of-visualization-of-mnist-with-r/" class="uri">https://charleshsliao.wordpress.com/2017/02/25/two-ways-of-visualization-of-mnist-with-r/</a><br></p>
<p>Kingma, D., &amp; Lei Ba, J. (2017). ADAM: A METHOD FOR STOCHASTIC
OPTIMIZATION.<br> <a href="https://arxiv.org/pdf/1412.6980.pdf" class="uri">https://arxiv.org/pdf/1412.6980.pdf</a><br></p>
<p>LeCun, Y. (2009). MNIST handwritten digit database, Yann LeCun,
Corinna Cortes and Chris Burges. Lecun.com.<br> <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a><br></p>
<p>Mildenhall, B. (n.d.). Fourier Features Let Networks Learn High
Frequency Functions in Low Dimensional Domains (10min talk)<br> [Review
of Fourier Features Let Networks Learn High Frequency Functions in Low
Dimensional Domains (10min talk)].<br> <a href="https://www.youtube.com/watch?v=iKyIJ_EtSkw&amp;ab_channel=BENMILDENHALL" class="uri">https://www.youtube.com/watch?v=iKyIJ_EtSkw&amp;ab_channel=BENMILDENHALL</a><br></p>
<p>neuralthreads. (2021, December 6). Softmax function — It is
frustrating that everyone talks about it but very few talk about
its….<br> Medium. <a href="https://neuralthreads.medium.com/softmax-function-it-is-frustrating-that-everyone-talks-about-it-but-very-few-talk-about-its-54c90b9d0acd" class="uri">https://neuralthreads.medium.com/softmax-function-it-is-frustrating-that-everyone-talks-about-it-but-very-few-talk-about-its-54c90b9d0acd</a><br></p>
<p>Sagar, A. (n.d.). 5 Techniques to Prevent Overfitting in Neural
Networks<br> [Review of 5 Techniques to Prevent Overfitting in Neural
Networks]. KDnuggets.<br> <a href="https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html" class="uri">https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html</a><br></p>
<p>Taboga, M. (n.d.). Vec operator [Review of Vec operator].
StatLect.<br> <a href="https://www.statlect.com/matrix-algebra/vec-operator" class="uri">https://www.statlect.com/matrix-algebra/vec-operator</a><br></p>
<p>Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S.,
Raghavan, N., Singhal, U.,<br> Ramamoorthi, R., Barron, J. T., &amp; Ng,
R. (2020, June 18).<br> Fourier features let networks learn high
frequency functions in low dimensional domains.<br> arXiv.org. <a href="https://arxiv.org/abs/2006.10739" class="uri">https://arxiv.org/abs/2006.10739</a><br></p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
